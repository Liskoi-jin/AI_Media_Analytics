"""
åª’ä»‹è‡ªåŠ¨åŒ–å®¡è®¡åˆ†æç³»ç»Ÿ - Flaskä¸»åº”ç”¨
æ•´åˆå·¥ä½œé‡ã€å·¥ä½œè´¨é‡ã€æˆæœ¬ä¸‰å¤§çœŸå®åˆ†ææ¨¡å—
æä¾›Webäº¤äº’ç•Œé¢ï¼ˆå…¼å®¹ä¸­æ–‡æ–‡ä»¶å+ç¼–ç è‡ªåŠ¨é€‚é…+å»é‡ä¸Šä¼ +å®Œæ•´å¼‚å¸¸å…œåº•ï¼‰
"""

# app_auto.py æœ€é¡¶éƒ¨
import logging
import os


# ========== ã€ç»ˆæä¿®å¤ã€‘ä¸€æ¬¡æ€§çš„æ—¥å¿—é…ç½® ==========
def setup_logging():
    """
    ç»Ÿä¸€æ—¥å¿—é…ç½®ï¼Œç¡®ä¿åªé…ç½®ä¸€æ¬¡ï¼Œé¿å…é‡å¤æ‰“å°
    """
    # 1. è·å–æ ¹æ—¥å¿—å™¨å’Œå½“å‰æ¨¡å—æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    app_logger = logging.getLogger(__name__)

    # 2. æ¸…é™¤æ‰€æœ‰ç°æœ‰å¤„ç†å™¨ï¼ˆé¿å…é‡å¤ï¼‰
    if root_logger.handlers:
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)

    if app_logger.handlers:
        for handler in app_logger.handlers[:]:
            app_logger.removeHandler(handler)

    # 3. è®¾ç½®æ—¥å¿—çº§åˆ«
    root_logger.setLevel(logging.INFO)
    app_logger.setLevel(logging.INFO)

    # 4. åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # 5. åªåˆ›å»ºä¸€ä¸ªæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)

    # 6. é˜²æ­¢é‡å¤æ·»åŠ å¤„ç†å™¨
    if not root_logger.handlers:
        root_logger.addHandler(console_handler)

    # 7. é˜²æ­¢å‘ä¸Šä¼ æ’­åˆ°æ ¹æ—¥å¿—å™¨ï¼ˆé¿å…é‡å¤æ‰“å°ï¼‰
    app_logger.propagate = False

    # 8. ç¡®ä¿ app_logger ä¹Ÿæœ‰å¤„ç†å™¨ï¼ˆå¦‚æœä¸ä¼ æ’­åˆ°æ ¹æ—¥å¿—å™¨ï¼‰
    if not app_logger.handlers:
        app_logger.addHandler(console_handler)

    return app_logger


# ç«‹å³é…ç½®æ—¥å¿—
logger = setup_logging()
logger.info("âœ… æ—¥å¿—ç³»ç»Ÿé…ç½®å®Œæˆï¼ˆå•æ¬¡é…ç½®ï¼Œæ— é‡å¤ï¼‰")
import os
import re
import json
import unicodedata
import traceback
import logging  # å…ˆå¯¼å…¥loggingæ¨¡å—
from datetime import datetime
from io import BytesIO
from typing import List, Dict, Any

import pandas as pd
import numpy as np
from flask import (
    Flask, render_template, request, redirect,
    url_for, flash, send_file, jsonify, g, make_response, send_from_directory
)
from werkzeug.utils import secure_filename

from auth import auth_bp, init_db as init_auth_db
from flask import session, redirect, url_for
from auth.utils import get_current_user, login_required

from src.db_utils import query_workload_data, query_quality_data, query_cost_data
from datetime import datetime



# ------------------------------ åˆå§‹åŒ–åŸºç¡€logger ------------------------------
# å…ˆåˆ›å»ºåŸºç¡€loggerï¼Œç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½æœ‰loggerå¯ç”¨
def setup_default_logger():
    """åˆ›å»ºå’Œé…ç½®é»˜è®¤logger"""
    logger = logging.getLogger(__name__)
    # é¿å…é‡å¤æ·»åŠ handler
    if not logger.handlers:
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
            ]
        )
    return logger

# ç«‹å³åˆ›å»ºåŸºç¡€logger
logger = setup_default_logger()

# ------------------------------ å¼•å…¥çœŸå®åˆ†ææ¨¡å— & å·¥å…·ç±» ------------------------------
# å…¼å®¹æ¨¡å—å¯¼å…¥å¤±è´¥çš„å…œåº•å¤„ç†ï¼ˆé¿å…åº”ç”¨å¯åŠ¨æŠ¥é”™ï¼‰
ID_TO_NAME_MAPPING = {}
NAME_TO_GROUP_MAPPING = {}

try:
    from src.data_processor import DataProcessor
    from src.workload_analyzer import WorkloadAnalyzer
    from src.quality_analyzer import QualityAnalyzer  # å·¥ä½œè´¨é‡åˆ†æå™¨
    from src.cost_analyzer import CostAnalyzer        # æˆæœ¬åˆ†æå™¨
    from src.report_generator import ReportGenerator  # æŠ¥å‘Šç”Ÿæˆå™¨
    from src.utils import ID_TO_NAME_MAPPING as SRC_ID_MAPPING, NAME_TO_GROUP_MAPPING as SRC_NAME_MAPPING

    # å¦‚æœæˆåŠŸå¯¼å…¥ï¼Œä½¿ç”¨srcæ¨¡å—ä¸­çš„æ˜ å°„
    ID_TO_NAME_MAPPING = SRC_ID_MAPPING
    NAME_TO_GROUP_MAPPING = SRC_NAME_MAPPING
    logger.info("âœ… æˆåŠŸå¯¼å…¥srcæ‰€æœ‰åˆ†ææ¨¡å—")

except ImportError as e:
    logger.warning(f"âš ï¸ éƒ¨åˆ†æ¨¡å—å¯¼å…¥å¤±è´¥ï¼š{e}ï¼Œå·²å¯ç”¨å…œåº•æ¨¡æ‹Ÿç±»ï¼Œä¸å½±å“åŸºç¡€è¿è¡Œ")

    # æ¨¡æ‹ŸDataProcessorç±»
    class DataProcessor:
        def process_for_media_analysis(self, file_paths, category):
            return {"processed_data": pd.DataFrame(), "filtered_data": pd.DataFrame(), "stats": {}}

        def process_for_cost_analysis(self, file_paths, category):
            return {"processed_data": pd.DataFrame(), "filtered_data": pd.DataFrame(), "stats": {}}

    # æ¨¡æ‹ŸWorkloadAnalyzerç±»
    class WorkloadAnalyzer:
        def __init__(self, df, known_id_name_mapping=None, config=None):
            self.df = df
            self.config = config or {}
        def analyze(self, top_n=10):
            detail_df = self.df.reset_index(drop=False).fillna("") if not self.df.empty else pd.DataFrame()
            group_df = self.df.groupby("å°ç»„åç§°").sum().reset_index(drop=False).fillna("") if not self.df.empty else pd.DataFrame()
            return {"detail": detail_df, "summary": {}, "group_summary": group_df, "top_media_ranking": detail_df}

    # æ¨¡æ‹ŸQualityAnalyzerç±»
    class QualityAnalyzer:
        def __init__(self, df, known_id_name_mapping=None, config=None):
            self.df = df
            self.config = config or {}
        def analyze(self, use_original_state=False):
            detail_df = self.df.reset_index(drop=False).fillna("") if not self.df.empty else pd.DataFrame()
            group_df = self.df.groupby("å°ç»„åç§°").sum().reset_index(drop=False).fillna("") if not self.df.empty else pd.DataFrame()
            return {"detail": detail_df, "summary": {}, "group_summary": group_df, "quality_distribution": detail_df}

    # æ¨¡æ‹ŸCostAnalyzerç±»
    class CostAnalyzer:
        def __init__(self, processed_df, filtered_df):
            self.processed_df = processed_df
            self.filtered_df = filtered_df
        def analyze(self, top_n=10):
            media_detail = self.processed_df.reset_index(drop=False).fillna("") if not self.processed_df.empty else pd.DataFrame()
            group_summary = self.processed_df.groupby("å°ç»„åç§°").sum().reset_index(drop=False).fillna("") if not self.processed_df.empty else pd.DataFrame()
            return {
                "overall_summary": {'æ•´ä½“å¹³å‡æˆæœ¬':0.0,'æ•´ä½“è¿”ç‚¹å æŠ¥ä»·æ¯”ä¾‹(%)':'0%','æ€»æˆæœ¬':0.0},
                "media_detail": media_detail,
                "group_summary": group_summary,
                "filtered_summary": {'ç­›é™¤æ€»æˆæœ¬':0,'ç­›é™¤æˆæœ¬å æ¯”':0,'ç­›é™¤è¾¾äººæ•°é‡':0,'ç­›é™¤å‘å¸ƒæ•°é‡':0},
                "cost_efficiency_ranking": media_detail
            }

    # ========== æ ¸å¿ƒä¿®å¤ï¼šé‡å†™çœŸå®çš„ReportGenerator ä¸å†æ˜¯æ¨¡æ‹Ÿç©ºè¡¨ ==========
    class ReportGenerator:
        def __init__(self, analysis_results=None, output_dir="./outputs"):
            self.analysis_results = analysis_results if analysis_results is not None else {}
            self.output_dir = output_dir
            # ç¡®ä¿æŠ¥å‘Šè¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.join(output_dir, 'reports'), exist_ok=True)
            os.makedirs(os.path.join(output_dir, 'excel'), exist_ok=True)

        # ========== âœ… ä¿®å¤ï¼šä¿®æ”¹generate_excel_reportæ–¹æ³•ï¼Œç§»é™¤analysis_idå‚æ•° ==========
        def generate_excel_report(self, analysis_mode='full'):
            """ç”ŸæˆExcelæŠ¥å‘Š - çœŸå®å†™å…¥å¤šsheetæ•°æ®+ä¿®å¤ç´¢å¼•é”™ä¹±+ç©ºæ•°æ®å†™å…¥è¡¨å¤´"""
            try:
                # âœ… ä¿®å¤ï¼šç§»é™¤analysis_idå‚æ•°ï¼Œä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆæ–‡ä»¶å
                time_str = datetime.now().strftime('%Y%m%d_%H%M%S')
                excel_filename = f"Media_Analysis_Report_{time_str}.xlsx"
                excel_file_path = os.path.join(self.output_dir, 'excel', excel_filename)

                # è·å–åˆ†æç»“æœæ•°æ®
                workload_detail = self.analysis_results.get('workload', {}).get('result', pd.DataFrame())
                quality_detail = self.analysis_results.get('quality', {}).get('result', pd.DataFrame())
                cost_detail = self.analysis_results.get('cost', {}).get('media_detail', pd.DataFrame())
                cost_ranking = self.analysis_results.get('cost', {}).get('cost_efficiency_ranking', pd.DataFrame())

                # âœ… æ ¸å¿ƒä¿®å¤ï¼šreset_index(drop=True) ä¸¢å¼ƒç´¢å¼•ï¼Œåªä¿ç•™ä¸šåŠ¡æ•°æ®ï¼Œè§£å†³ç´¢å¼•é”™ä¹±
                # âœ… ç©ºæ•°æ®ä¹Ÿå†™å…¥è¡¨å¤´ï¼Œfillna(0) æ•°å­—åˆ—å¡«å……0ï¼Œå­—ç¬¦ä¸²åˆ—å¡«å……ç©ºå­—ç¬¦ä¸²
                with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:
                    # å·¥ä½œé‡åˆ†æsheet - å¿…å†™ï¼Œç©ºæ•°æ®ä¹Ÿå†™å…¥è¡¨å¤´
                    if not workload_detail.empty:
                        workload_df = workload_detail.reset_index(drop=True).fillna(
                            {"å°ç»„åç§°": "", "åª’ä»‹åç§°": ""}).fillna(0)
                        workload_df.to_excel(writer, sheet_name="åª’ä»‹å·¥ä½œé‡åˆ†æ", index=False)
                    else:
                        pd.DataFrame({"æç¤º": ["æ— å·¥ä½œé‡åˆ†ææ•°æ®"]}).to_excel(writer, sheet_name="åª’ä»‹å·¥ä½œé‡åˆ†æ", index=False)

                    # è´¨é‡åˆ†æsheet - å¿…å†™
                    if not quality_detail.empty:
                        quality_df = quality_detail.reset_index(drop=True).fillna({"å°ç»„åç§°": "", "åª’ä»‹åç§°": ""}).fillna(0)
                        quality_df.to_excel(writer, sheet_name="åª’ä»‹è´¨é‡åˆ†æ", index=False)
                    else:
                        pd.DataFrame({"æç¤º": ["æ— è´¨é‡åˆ†ææ•°æ®"]}).to_excel(writer, sheet_name="åª’ä»‹è´¨é‡åˆ†æ", index=False)

                    # æˆæœ¬æ˜ç»†sheet - å¿…å†™
                    if not cost_detail.empty:
                        cost_df = cost_detail.reset_index(drop=True).fillna({"å°ç»„åç§°": "", "åª’ä»‹åç§°": ""}).fillna(0)
                        cost_df.to_excel(writer, sheet_name="åª’ä»‹æˆæœ¬æ˜ç»†", index=False)
                    else:
                        pd.DataFrame({"æç¤º": ["æ— æˆæœ¬åˆ†ææ•°æ®"]}).to_excel(writer, sheet_name="åª’ä»‹æˆæœ¬æ˜ç»†", index=False)

                    # æˆæœ¬æ•ˆç‡æ’åsheet - å¿…å†™
                    if not cost_ranking.empty:
                        cost_rank_df = cost_ranking.reset_index(drop=True).fillna({"å°ç»„åç§°": "", "åª’ä»‹åç§°": ""}).fillna(0)
                        cost_rank_df.to_excel(writer, sheet_name="æˆæœ¬æ•ˆç‡æ’å", index=False)
                    else:
                        pd.DataFrame({"æç¤º": ["æ— æˆæœ¬æ•ˆç‡æ’åæ•°æ®"]}).to_excel(writer, sheet_name="æˆæœ¬æ•ˆç‡æ’å", index=False)

                    # æ±‡æ€»sheet - å¿…å†™ï¼Œå±•ç¤ºå„æ¨¡å—æ•°æ®é‡
                    summary_df = pd.DataFrame({
                        'åˆ†æç±»å‹': ['å·¥ä½œé‡åˆ†æ', 'è´¨é‡åˆ†æ', 'æˆæœ¬åˆ†æ'],
                        'æœ‰æ•ˆæ•°æ®é‡': [len(workload_detail), len(quality_detail), len(cost_detail)],
                        'ç”Ÿæˆæ—¶é—´': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * 3
                    })
                    summary_df.to_excel(writer, sheet_name="åˆ†ææ±‡æ€»", index=False)

                logger.info(f"âœ… ExcelæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼š{excel_file_path}")
                return excel_file_path
            except Exception as e:
                logger.error(f"ç”ŸæˆExcelæŠ¥å‘Šå¤±è´¥ï¼š{e}")
                return ""

        def generate_all_reports(self, analysis_mode='full'):
            """ç”Ÿæˆæ‰€æœ‰æ ¼å¼æŠ¥å‘Š"""
            return {"excel_report": self.generate_excel_report(analysis_mode), "html_report": None}

    DataProcessor = DataProcessor
    WorkloadAnalyzer = WorkloadAnalyzer
    QualityAnalyzer = QualityAnalyzer
    CostAnalyzer = CostAnalyzer
    ReportGenerator = ReportGenerator

# ------------------------------ åˆå§‹åŒ–é…ç½® ------------------------------
app = Flask(__name__)

# ========== æ–°å¢ï¼šæ•°æ®åº“é…ç½® ==========
# ç›´æ¥è®¾ç½®æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': 'rm-cn-2104msjne000170o.rwlb.rds.aliyuncs.com',
    'port': 3306,
    'user': 'root',  # æ”¹ä¸ºä½ çš„æ•°æ®åº“ç”¨æˆ·å
    'password': 'Lj041213',  # æ”¹ä¸ºä½ çš„æ•°æ®åº“å¯†ç 
    'database': 'ai_media_db',  # æ”¹ä¸ºä½ çš„æ•°æ®åº“å
    'charset': 'utf8mb4'
}

# æ„å»ºæ•°æ®åº“è¿æ¥URI
app.config['SQLALCHEMY_DATABASE_URI'] = f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}?charset={DB_CONFIG['charset']}"
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
app.config['SQLALCHEMY_ECHO'] = False

# åˆå§‹åŒ–æƒé™æ¨¡å—æ•°æ®åº“ï¼ˆç‹¬ç«‹åˆå§‹åŒ–ï¼Œä¸å½±å“åŸæœ‰ç³»ç»Ÿï¼‰
app.config['SECRET_KEY'] = app.config.get('SECRET_KEY', 'ai_media_auth_2025_secure')  # ç”¨äºä¼šè¯åŠ å¯†
init_auth_db(app)  # åˆå§‹åŒ–ç”¨æˆ·è¡¨ï¼ˆè‹¥å·²å­˜åœ¨åˆ™ä¸åˆ›å»ºï¼‰

# æ³¨å†Œæƒé™æ¨¡å—è“å›¾ï¼ˆè·¯ç”±å‰ç¼€/authï¼Œä¸åŸæœ‰ç³»ç»Ÿéš”ç¦»ï¼‰
app.register_blueprint(auth_bp)

# ç”Ÿäº§ç¯å¢ƒæ¨èï¼šä»ç¯å¢ƒå˜é‡è¯»å–ç§˜é’¥ï¼Œæœ¬åœ°å¼€å‘ç”¨é»˜è®¤å€¼
app.secret_key = os.getenv('SECRET_KEY', 'media-audit-2025-secure-key-@#$%^&*')

# é…ç½®ä¸Šä¼ æ–‡ä»¶å¤¹ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…æƒé™é—®é¢˜ï¼‰
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
app.config['UPLOAD_FOLDER'] = os.path.join(BASE_DIR, 'uploads')
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100MBæ–‡ä»¶ä¸Šä¼ é™åˆ¶

# åŸºç¡€é…ç½®
app.config['DEBUG'] = True
app.config['OUTPUT_DIR'] = os.path.join(BASE_DIR, 'outputs')
app.config['LOG_FILE'] = os.path.join(app.config['OUTPUT_DIR'], 'logs', 'media_audit.log')
app.config['LOG_LEVEL'] = 'INFO'

# ========================== æ ¸å¿ƒä¿®å¤ 1/5ï¼šæ³¨å†Œå…¨å±€ format_number è¿‡æ»¤å™¨ + æ–°å¢ safe_min è¿‡æ»¤å™¨ ==========================
@app.template_filter('format_number')
def format_number_filter(value, decimal_places=2):
    """
    Jinja2è¿‡æ»¤å™¨ï¼šæ ¼å¼åŒ–æ•°å­—ï¼Œä¿ç•™æŒ‡å®šå°æ•°ä½ï¼Œå…¼å®¹ç©ºå€¼/éæ•°å­—/NaN/Noneï¼Œé€‚é…æˆæœ¬åˆ†æé‡‘é¢å±•ç¤º
    :param value: è¦æ ¼å¼åŒ–çš„å€¼ï¼ˆæ”¯æŒæ•°å­—/å­—ç¬¦ä¸²/ç©ºå€¼ï¼‰
    :param decimal_places: ä¿ç•™å°æ•°ä½æ•°ï¼Œé»˜è®¤2ä½
    :return: æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²ï¼Œç©ºå€¼è¿”å› 0.00
    """
    try:
        # ã€ä¿®å¤ã€‘å‰ç½®åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹ï¼Œé¿å…pd.isnaå¤„ç†épdå¯¹è±¡æŠ¥é”™
        if isinstance(value, (str, int, float)) is False or value is None or value == '' or str(value).strip() == '-':
            return f"0.{0 * decimal_places}"
        # å¤„ç†ç©ºå€¼ã€Noneã€NaNç­‰å¼‚å¸¸æƒ…å†µ
        if pd.isna(value):
            return f"0.{0 * decimal_places}"
        # è½¬ä¸ºæµ®ç‚¹æ•°åæ ¼å¼åŒ–
        num = float(value)
        return f"{num:.{decimal_places}f}"
    except (ValueError, TypeError, Exception):
        # éæ•°å­—ç±»å‹ç›´æ¥è¿”å›åŸå€¼ï¼Œé¿å…æŠ¥é”™
        return f"0.{0 * decimal_places}"

@app.template_filter('safe_min')
def safe_min_filter(value, min_val):
    """
    ã€æ ¸å¿ƒä¿®å¤ã€‘è§£å†³ Jinja2 åŸç”Ÿ min è¿‡æ»¤å™¨æŠ¥é”™ï¼šTypeError: 'float' object is not iterable
    ä¸“ä¸º å•ä¸ªæ•°å€¼ å’Œ å¯¹æ¯”å€¼ è®¾è®¡çš„å®‰å…¨æœ€å°å€¼è¿‡æ»¤å™¨ï¼Œç”¨äºç™¾åˆ†æ¯”/å®½åº¦é™åˆ¶åœºæ™¯
    """
    try:
        val = float(value) if value else 0.0
        minv = float(min_val) if min_val else 0.0
        return min(val, minv)
    except:
        return min_val

# æ–°å¢format_percentageè¿‡æ»¤å™¨ï¼Œç”¨äºquality_analysis.htmlä¸­çš„ç™¾åˆ†æ¯”æ ¼å¼åŒ–
@app.template_filter('format_percentage')
def format_percentage_filter(value, default='0.00%'):
    """
    æ ¼å¼åŒ–ç™¾åˆ†æ¯”ï¼Œå…¼å®¹å­—ç¬¦ä¸²å’Œæ•°å€¼ç±»å‹
    """
    try:
        if value is None or pd.isna(value):
            return default
        if isinstance(value, (int, float)):
            return f"{value:.2f}%"
        if isinstance(value, str):
            # å¦‚æœå·²ç»æ˜¯ç™¾åˆ†æ¯”æ ¼å¼ï¼Œç›´æ¥è¿”å›
            if '%' in value:
                return value
            # å¦åˆ™å°è¯•è½¬æ¢
            try:
                num = float(value)
                return f"{num:.2f}%"
            except:
                return default
        return default
    except:
        return default

# ------------------------------ ç¡®ä¿ç›®å½•å­˜åœ¨ ------------------------------
def create_dir_with_permission(dir_path):
    """åˆ›å»ºç›®å½•å¹¶å¤„ç†æƒé™é—®é¢˜ï¼Œå¸¦å…œåº•æ–¹æ¡ˆ"""
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path, mode=0o755, exist_ok=True)
            logger.info(f"ğŸ“‚ ç›®å½•åˆ›å»ºæˆåŠŸï¼š{dir_path}")
        except PermissionError as e:
            logger.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥ï¼Œæƒé™ä¸è¶³ï¼š{dir_path}ï¼Œé”™è¯¯ï¼š{e}")
            fallback_dir = os.path.join(os.path.expanduser("~"), "media_audit", os.path.basename(dir_path))
            os.makedirs(fallback_dir, mode=0o755, exist_ok=True)
            logger.warning(f"âš ï¸ è‡ªåŠ¨åˆ›å»ºå…œåº•ç›®å½•ï¼š{fallback_dir}")
            return fallback_dir
    return dir_path

# åˆ›å»ºæ‰€æœ‰å¿…è¦ç›®å½•
app.config['UPLOAD_FOLDER'] = create_dir_with_permission(app.config['UPLOAD_FOLDER'])
app.config['OUTPUT_DIR'] = create_dir_with_permission(app.config['OUTPUT_DIR'])
create_dir_with_permission(os.path.join(app.config['OUTPUT_DIR'], 'analysis_results'))
create_dir_with_permission(os.path.join(app.config['OUTPUT_DIR'], 'logs'))
create_dir_with_permission(os.path.join(app.config['OUTPUT_DIR'], 'reports'))
create_dir_with_permission(os.path.join(app.config['OUTPUT_DIR'], 'excel'))

# é‡æ–°é…ç½®æ—¥å¿—ï¼ˆä½¿ç”¨å·²åˆ›å»ºçš„ç›®å½•ï¼‰
try:
    file_handler = logging.FileHandler(app.config['LOG_FILE'], encoding='utf-8')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logger.addHandler(file_handler)
    logger.setLevel(getattr(logging, app.config['LOG_LEVEL'].upper()))
    logger.info("ğŸ“ æ—¥å¿—ç³»ç»Ÿé…ç½®å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶å·²ç”Ÿæ•ˆ")
except Exception as e:
    logger.error(f"âŒ é…ç½®æ—¥å¿—æ–‡ä»¶å¤±è´¥ï¼š{e}ï¼Œå°†ç»§ç»­ä½¿ç”¨æ§åˆ¶å°æ—¥å¿—")

# ------------------------------ æ ¸å¿ƒå·¥å…·å‡½æ•° ------------------------------
def secure_filename_cn(filename):
    """å…¼å®¹ä¸­æ–‡çš„å®‰å…¨æ–‡ä»¶åå¤„ç†ï¼Œå½»åº•è§£å†³ä¸­æ–‡ä¹±ç /éæ³•å­—ç¬¦é—®é¢˜"""
    if not filename:
        return 'unnamed_file'
    filename = unicodedata.normalize('NFKC', filename)
    illegal_chars = r'[\\/:*?"<>|]'
    filename = re.sub(illegal_chars, '_', filename)
    filename = filename.strip()
    if len(filename) > 255:
        name, ext = os.path.splitext(filename)
        filename = name[:200] + ext
    return filename if filename else 'unnamed_file'

def save_file_with_duplicate_check(file, force_cover=False):
    """ä¿å­˜æ–‡ä»¶+åŒå±‚å»é‡ï¼šâ‘ åŒè¯·æ±‚å†…å»é‡ â‘¡æ–‡ä»¶å·²å­˜åœ¨å»é‡ï¼Œå½»åº•è§£å†³é‡å¤ä¸Šä¼ """
    if not file or file.filename.strip() == '':
        return ""
    original_filename = secure_filename_cn(file.filename)
    save_path = os.path.join(app.config['UPLOAD_FOLDER'], original_filename)

    # åŒè¯·æ±‚å†…å»é‡ï¼šé¿å…ä¸€æ¬¡ä¸Šä¼ å¤šä¸ªç›¸åŒæ–‡ä»¶
    if not hasattr(g, 'uploaded_files'):
        g.uploaded_files = set()
    if original_filename in g.uploaded_files:
        logger.info(f"â­ï¸ åŒè¯·æ±‚é‡å¤æ–‡ä»¶ï¼Œè·³è¿‡ï¼š{original_filename}")
        return ""
    g.uploaded_files.add(original_filename)

    # æ–‡ä»¶å·²å­˜åœ¨å»é‡
    if not os.path.exists(save_path) or force_cover:
        try:
            file.save(save_path)
            logger.info(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸï¼š{original_filename}")
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{e}")
            save_path = ""
    else:
        logger.info(f"â­ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ ï¼š{original_filename}")
    return save_path

def read_file_with_auto_encoding(file_path):
    """è‡ªåŠ¨è¯†åˆ«ç¼–ç è¯»å–Excel/CSVï¼Œå…¼å®¹æ‰€æœ‰å¸¸è§ç¼–ç ï¼Œå…œåº•ç©ºDataFrame"""
    if not os.path.exists(file_path):
        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        return pd.DataFrame()
    file_ext = os.path.splitext(file_path)[1].lower()
    try:
        if file_ext in ['.xlsx', '.xls']:
            return pd.read_excel(file_path, engine='openpyxl' if file_ext == '.xlsx' else 'xlrd')
        elif file_ext == '.csv':
            encoding_list = ['utf-8-sig', 'gbk', 'gb2312', 'latin-1', 'utf-8']
            for encoding in encoding_list:
                try:
                    return pd.read_csv(file_path, encoding=encoding)
                except (UnicodeDecodeError, Exception):
                    continue
            raise Exception(f"ç¼–ç ä¸å…¼å®¹ï¼š{os.path.basename(file_path)}")
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{file_ext}ï¼Œä»…æ”¯æŒxlsx/xls/csv")
            return pd.DataFrame()
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{file_path}ï¼Œé”™è¯¯ï¼š{e}")
        return pd.DataFrame()


# ========== æ–°å¢ï¼šæ•°æ®åº“å­—æ®µæ˜ å°„å‡½æ•° ==========
def map_database_fields(df):
    """
    ç®€åŒ–æ•°æ®åº“å­—æ®µæ˜ å°„ - å› ä¸ºdb_utils.pyå·²ç»è¿”å›æ­£ç¡®çš„å­—æ®µå
    """
    if df.empty:
        return df

    df_copy = df.copy()

    logger.info(f"æ•°æ®åº“å­—æ®µæ˜ å°„ï¼ŒåŸå§‹åˆ—å: {list(df_copy.columns)}")
    logger.info(f"æ•°æ®ç¤ºä¾‹: {df_copy.iloc[0].to_dict() if len(df_copy) > 0 else 'ç©ºæ•°æ®'}")

    # ç¡®ä¿æœ‰å¿…è¦çš„å­—æ®µ
    required_fields = ['åª’ä»‹å§“å', 'å¯¹åº”çœŸå', 'æ‰€å±å°ç»„', 'æ•°æ®ç±»å‹', 'å®šæ¡£åª’ä»‹', 'æäº¤åª’ä»‹']
    for field in required_fields:
        if field not in df_copy.columns:
            logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
            if field == 'æ‰€å±å°ç»„':
                df_copy[field] = 'é»˜è®¤ç»„'
            elif field == 'æ•°æ®ç±»å‹':
                df_copy[field] = 'ææŠ¥'  # é»˜è®¤å€¼
            elif field in ['åª’ä»‹å§“å', 'å®šæ¡£åª’ä»‹'] and 'schedule_user_name' in df_copy.columns:
                df_copy[field] = df_copy['schedule_user_name']
            elif field in ['å¯¹åº”çœŸå', 'æäº¤åª’ä»‹'] and 'submit_media_user_name' in df_copy.columns:
                df_copy[field] = df_copy['submit_media_user_name']
            else:
                df_copy[field] = 'æœªçŸ¥'

    return df_copy

# ========== æ›¿æ¢ä½ å½“å‰çš„ convert_pandas_types_to_python å‡½æ•° ==========
def convert_pandas_types_to_python(data):
    """
    æ ¸å¿ƒä¿®å¤ï¼šé€’å½’è½¬æ¢Pandas/numpyç‰¹æ®Šç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    å½»åº•è§£å†³ã€ŒObject of type int64/float64 is not JSON serializableã€æŠ¥é”™
    âœ… æ–°å¢ï¼šç©ºDataFrameè¿”å›å¸¦è¡¨å¤´çš„å­—å…¸åˆ—è¡¨ã€NaNæ•°å­—å¡«å……0ã€å­—ç¬¦ä¸²å¡«å……ç©ºå€¼
    """
    if isinstance(data, pd.DataFrame):
        if data.empty:
            # âœ… ç©ºDataFrameè¿”å›ç©ºåˆ—è¡¨ï¼Œæ¨¡æ¿éå†æ— æ•°æ®æ—¶æ˜¾ç¤º"æš‚æ— æ•°æ®"
            return []
        # âœ… å…³é”®ä¿®å¤ï¼šfillna(0) å¤„ç†æ‰€æœ‰æ•°å€¼åˆ—ï¼Œç¡®ä¿ä¸ä¼šå‡ºç°NaN
        try:
            # å…ˆå¤åˆ¶æ•°æ®
            df_copy = data.copy()
            # è·å–æ•°å€¼åˆ—
            numeric_cols = df_copy.select_dtypes(include=[np.number]).columns
            # æ•°å€¼åˆ—å¡«å……0
            if len(numeric_cols) > 0:
                df_copy[numeric_cols] = df_copy[numeric_cols].fillna(0)
            # å¯¹è±¡åˆ—å¡«å……ç©ºå­—ç¬¦ä¸²
            object_cols = df_copy.select_dtypes(include=['object']).columns
            if len(object_cols) > 0:
                df_copy[object_cols] = df_copy[object_cols].fillna('')

            return df_copy.reset_index(drop=True).to_dict('records')
        except Exception as e:
            logger.error(f"è½¬æ¢DataFrameå¤±è´¥: {e}")
            return []
    elif isinstance(data, pd.Series):
        try:
            return data.reset_index(drop=True).fillna(0).to_dict()
        except:
            return {}
    elif isinstance(data, dict):
        return {key: convert_pandas_types_to_python(value) for key, value in data.items()}
    elif isinstance(data, (list, tuple)):
        return [convert_pandas_types_to_python(item) for item in data]
    elif isinstance(data, (np.integer, np.int8, np.int16, np.int32, np.int64)):
        return int(data)
    elif pd.api.types.is_integer_dtype(type(data)):
        return int(data) if pd.notna(data) else 0
    elif isinstance(data, (np.floating, np.float16, np.float32, np.float64)):
        return float(data)
    elif pd.api.types.is_float_dtype(type(data)):
        return float(data) if pd.notna(data) else 0.0
    elif isinstance(data, np.bool_):
        return bool(data)
    elif isinstance(data, np.ndarray):
        return data.tolist()
    elif isinstance(data, (pd.Timestamp, datetime)):
        return data.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(data) else ""
    elif pd.isna(data):
        return 0 if isinstance(data, (int, float)) else ""
    return data

def preprocess_percent_str_to_float(percent_str):
    """é¢„å¤„ç†ç™¾åˆ†æ•°å­—ç¬¦ä¸²è½¬æµ®ç‚¹å‹ï¼Œæ¨¡æ¿æ¸²æŸ“ä¸“ç”¨ï¼Œå¼‚å¸¸å€¼å…œåº•0.0"""
    if not percent_str:
        return 0.0
    if not isinstance(percent_str, str):
        try:
            return float(percent_str)
        except (ValueError, TypeError):
            return 0.0
    try:
        num_str = percent_str.replace('%', '').strip()
        return float(num_str) if num_str else 0.0
    except (ValueError, AttributeError):
        return 0.0

# ------------------------------ æ–°å¢æ ¸å¿ƒä¿®å¤å‡½æ•° - è¡¥å…¨å°ç»„æ•°æ®å­—æ®µ ------------------------------
def fill_group_data_fields(group_list):
    """
    ä¿®å¤æ ¸å¿ƒé—®é¢˜ï¼šä¸ºå°ç»„æ•°æ®è¡¥å…¨ã€æ€»å®šæ¡£æ•°ã€æ€»ææŠ¥æ•°ã€‘å­—æ®µï¼Œè§£å†³æ¨¡æ¿UndefinedæŠ¥é”™
    æ‰€æœ‰ç¼ºå¤±å­—æ®µé»˜è®¤èµ‹å€¼0ï¼Œå®Œç¾é€‚é…workload_analysis.htmlçš„{{ group_data|map(attribute='æ€»å®šæ¡£æ•°')|max }}è¯­æ³•
    """
    filled_group = []
    for group in group_list:
        if isinstance(group, dict):
            group['æ€»å®šæ¡£æ•°'] = group.get('æ€»å®šæ¡£æ•°', 0) or 0
            group['æ€»ææŠ¥æ•°'] = group.get('æ€»ææŠ¥æ•°', 0) or 0
            group['å®šæ¡£æ•°'] = group.get('å®šæ¡£æ•°', 0) or 0
            group['ææŠ¥æ•°'] = group.get('ææŠ¥æ•°', 0) or 0
            group['å°ç»„åç§°'] = group.get('å°ç»„åç§°', 'æœªçŸ¥å°ç»„') or 'æœªçŸ¥å°ç»„'
        filled_group.append(group)
    return filled_group

# ------------------------------ ã€æ–°å¢æ ¸å¿ƒä¿®å¤ã€‘è¡¥å…¨æˆæœ¬æ•°æ®æ‰€æœ‰ç¼ºå¤±å­—æ®µ ------------------------------
def fill_cost_data_fields(cost_data_list):
    """
    ä¿®å¤æ ¸å¿ƒæŠ¥é”™ï¼šdict object has no attribute 'ç­›é™¤æ€»æˆæœ¬'
    ä¸ºæˆæœ¬åˆ†æçš„æ¯æ¡æ•°æ®è¡¥å…¨æ‰€æœ‰å‰ç«¯æ¨¡æ¿ç”¨åˆ°çš„ä¸­æ–‡keyå­—æ®µï¼Œé»˜è®¤å€¼0ï¼Œå½»åº•è§£å†³å­—æ®µä¸å­˜åœ¨æŠ¥é”™
    """
    filled_cost = []
    # å‰ç«¯æ¨¡æ¿ç”¨åˆ°çš„æ‰€æœ‰æˆæœ¬ç›¸å…³å­—æ®µï¼Œå…¨éƒ¨å…œåº•
    cost_fields = [
        'ç­›é™¤æ€»æˆæœ¬', 'ç­›é™¤æˆæœ¬å æ¯”', 'ç­›é™¤è¾¾äººæ•°é‡', 'ç­›é™¤å‘å¸ƒæ•°é‡',
        'æ€»æˆæœ¬', 'å¹³å‡æˆæœ¬', 'æ€»è¿”ç‚¹é‡‘é¢', 'è¿”ç‚¹å æ¯”',
        'åª’ä»‹åç§°', 'å°ç»„åç§°', 'æ€»å‘å¸ƒæ•°', 'æ€»è¾¾äººæ•°',
        'æœ‰æ•ˆå‘å¸ƒæ•°', 'æœ‰æ•ˆè¾¾äººæ•°', 'æˆæœ¬å‘æŒ¥ç‡'
    ]
    for row in cost_data_list:
        if isinstance(row, dict):
            for field in cost_fields:
                if field not in row or row[field] is None or pd.isna(row[field]):
                    row[field] = 0
        filled_cost.append(row)
    return filled_cost

# ------------------------------ å…¨å±€å˜é‡ ------------------------------
analysis_results = {}  # å†…å­˜å­˜å‚¨åˆ†æç»“æœ
# åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—ï¼ˆå…¨å±€å•ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
data_processor = DataProcessor()

# ------------------------------ ä¸Šä¸‹æ–‡å¤„ç†å™¨ ------------------------------
def has_endpoint(endpoint_name):
    return endpoint_name in app.view_functions


# ------------------------------ æ–°å¢ï¼šç®€åŒ–åˆ†æå‡½æ•° ------------------------------
def create_simple_workload_analysis(df):
    """ç®€åŒ–å·¥ä½œé‡åˆ†æ - ä½¿ç”¨å‰ç«¯æ¨¡æ¿æœŸæœ›çš„å­—æ®µå"""
    result = {
        "result": [],
        "summary": {},
        "group_summary": [],
        "top_media_ranking": []
    }

    try:
        if df.empty:
            return result

        logger.info(f"æ‰§è¡Œç®€åŒ–å·¥ä½œé‡åˆ†æï¼Œæ•°æ®è¡Œæ•°: {len(df)}")
        logger.info(f"æ•°æ®åˆ—å: {list(df.columns)}")

        # ç¡®ä¿æœ‰å¿…è¦çš„å­—æ®µ
        required_fields = ['åª’ä»‹å§“å', 'å¯¹åº”çœŸå', 'æ‰€å±å°ç»„']
        missing_fields = [f for f in required_fields if f not in df.columns]
        if missing_fields:
            logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
            # å°è¯•åˆ›å»ºç¼ºå¤±å­—æ®µ
            if 'åª’ä»‹å§“å' not in df.columns:
                if 'å®šæ¡£åª’ä»‹' in df.columns:
                    df['åª’ä»‹å§“å'] = df['å®šæ¡£åª’ä»‹']
                else:
                    df['åª’ä»‹å§“å'] = 'æœªçŸ¥åª’ä»‹'

            if 'å¯¹åº”çœŸå' not in df.columns:
                df['å¯¹åº”çœŸå'] = df['åª’ä»‹å§“å']  # ä½¿ç”¨åª’ä»‹å§“åä½œä¸ºå¯¹åº”çœŸå

            if 'æ‰€å±å°ç»„' not in df.columns:
                df['æ‰€å±å°ç»„'] = 'é»˜è®¤ç»„'

        # ç»Ÿè®¡æ¯ä¸ªåª’ä»‹çš„å·¥ä½œé‡
        media_summary = df.groupby('åª’ä»‹å§“å').agg({
            'è¾¾äººæ˜µç§°': 'count'  # ç»Ÿè®¡å¤„ç†è¾¾äººæ•°é‡
        }).reset_index()

        media_summary.columns = ['åª’ä»‹å§“å', 'æ€»å¤„ç†é‡']

        # è·å–æ¯ä¸ªåª’ä»‹çš„å…¶ä»–ä¿¡æ¯
        # 1. å¯¹åº”çœŸå
        if 'å¯¹åº”çœŸå' in df.columns:
            media_realname = df.groupby('åª’ä»‹å§“å')['å¯¹åº”çœŸå'].first().reset_index()
            media_summary = pd.merge(media_summary, media_realname, on='åª’ä»‹å§“å', how='left')
        else:
            media_summary['å¯¹åº”çœŸå'] = media_summary['åª’ä»‹å§“å']

        # 2. æ‰€å±å°ç»„
        if 'æ‰€å±å°ç»„' in df.columns:
            media_group = df.groupby('åª’ä»‹å§“å')['æ‰€å±å°ç»„'].first().reset_index()
            media_summary = pd.merge(media_summary, media_group, on='åª’ä»‹å§“å', how='left')
        else:
            media_summary['æ‰€å±å°ç»„'] = 'é»˜è®¤ç»„'

        # 3. è®¡ç®—å®šæ¡£é‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾æœ‰æˆæœ¬æˆ–ä¸‹å•ä»·çš„æ•°æ®å°±æ˜¯å®šæ¡£æ•°æ®ï¼‰
        if 'æˆæœ¬' in df.columns:
            media_cost = df[df['æˆæœ¬'] > 0].groupby('åª’ä»‹å§“å').size().reset_index()
            media_cost.columns = ['åª’ä»‹å§“å', 'å®šæ¡£é‡']
            media_summary = pd.merge(media_summary, media_cost, on='åª’ä»‹å§“å', how='left')
            media_summary['å®šæ¡£é‡'] = media_summary['å®šæ¡£é‡'].fillna(0)
        else:
            media_summary['å®šæ¡£é‡'] = media_summary['æ€»å¤„ç†é‡']  # å¦‚æœæ²¡æœ‰æˆæœ¬æ•°æ®ï¼Œå‡è®¾æ‰€æœ‰éƒ½æ˜¯å®šæ¡£

        # 4. è®¡ç®—å®šæ¡£ç‡
        media_summary['å®šæ¡£ç‡(%)'] = media_summary.apply(
            lambda row: f"{(row['å®šæ¡£é‡'] / row['æ€»å¤„ç†é‡'] * 100):.2f}%" if row['æ€»å¤„ç†é‡'] > 0 else "0.00%",
            axis=1
        )

        # 5. æ·»åŠ è¯„ä¼°å­—æ®µï¼ˆå‰ç«¯æ¨¡æ¿éœ€è¦ï¼‰
        media_summary['å®šæ¡£ç‡è¯„ä¼°'] = media_summary['å®šæ¡£ç‡(%)'].apply(
            lambda x: 'ä¼˜ç§€' if float(x.replace('%', '')) > 80 else 'è‰¯å¥½' if float(x.replace('%', '')) > 60 else 'ä¸€èˆ¬'
        )

        media_summary['äº§é‡è¯„ä¼°'] = media_summary['æ€»å¤„ç†é‡'].apply(
            lambda x: 'é«˜äº§' if x > 50 else 'ä¸­äº§' if x > 20 else 'ä½äº§'
        )

        media_summary['ç»¼åˆè¯„ä¼°'] = media_summary.apply(
            lambda row: 'Sçº§' if row['æ€»å¤„ç†é‡'] > 100 and float(row['å®šæ¡£ç‡(%)'].replace('%', '')) > 90
            else 'Açº§' if row['æ€»å¤„ç†é‡'] > 50 and float(row['å®šæ¡£ç‡(%)'].replace('%', '')) > 80
            else 'Bçº§' if row['æ€»å¤„ç†é‡'] > 20 and float(row['å®šæ¡£ç‡(%)'].replace('%', '')) > 70
            else 'Cçº§' if row['æ€»å¤„ç†é‡'] > 10 else 'Dçº§',
            axis=1
        )

        # 6. æ·»åŠ å…¶ä»–å­—æ®µï¼ˆå‰ç«¯æ¨¡æ¿éœ€è¦ï¼‰
        media_summary['å·²å‘å¸ƒæ•°'] = 0
        media_summary['æœªå‘å¸ƒæ•°'] = 0
        media_summary['å…¶ä»–çŠ¶æ€æ•°'] = 0

        # 7. æ’åºå¹¶æ·»åŠ æ’å
        media_summary = media_summary.sort_values('æ€»å¤„ç†é‡', ascending=False)
        media_summary['æ’å'] = range(1, len(media_summary) + 1)

        # æ„å»ºç»“æœ
        result = {
            "result": media_summary.to_dict('records'),
            "summary": {
                'æ€»å¤„ç†é‡': len(df),
                'æ€»å®šæ¡£é‡': int(media_summary['å®šæ¡£é‡'].sum()),
                'åª’ä»‹æ€»æ•°': len(media_summary),
                'æ•´ä½“å®šæ¡£ç‡': f"{(media_summary['å®šæ¡£é‡'].sum() / len(df) * 100):.2f}%" if len(df) > 0 else "0.00%"
            },
            "group_summary": [],
            "top_media_ranking": media_summary.head(10).to_dict('records')
        }

        logger.info(f"ç®€åŒ–å·¥ä½œé‡åˆ†æå®Œæˆï¼Œæ¶‰åŠåª’ä»‹æ•°: {len(media_summary)}")
        logger.info(f"æ€»å¤„ç†é‡: {len(df)}, æ€»å®šæ¡£é‡: {int(media_summary['å®šæ¡£é‡'].sum())}")

    except Exception as e:
        logger.error(f"ç®€åŒ–å·¥ä½œé‡åˆ†æå¤±è´¥: {e}", exc_info=True)

    return result


def create_simple_quality_analysis(df):
    """ç®€åŒ–å·¥ä½œè´¨é‡åˆ†æ"""
    result = {
        "result": [],
        "summary": {},
        "group_summary": [],
        "quality_distribution": [],
        "premium_detail": [],
        "high_read_detail": []
    }

    try:
        if df.empty:
            return result

        logger.info(f"æ‰§è¡Œç®€åŒ–å·¥ä½œè´¨é‡åˆ†æï¼Œæ•°æ®è¡Œæ•°: {len(df)}")

        # ç¡®ä¿æœ‰å…³é”®å­—æ®µ
        if 'è¾¾äººç”¨é€”' not in df.columns:
            logger.warning("æ•°æ®ä¸­æ— 'è¾¾äººç”¨é€”'å­—æ®µï¼Œåˆ›å»ºé»˜è®¤å€¼")
            df['è¾¾äººç”¨é€”'] = 'æ™®é€šè¾¾äºº'

        # æŒ‰è¾¾äººç”¨é€”ç»Ÿè®¡
        purpose_distribution = []
        if 'è¾¾äººç”¨é€”' in df.columns and len(df) > 0:
            purpose_counts = df['è¾¾äººç”¨é€”'].value_counts().reset_index()
            purpose_counts.columns = ['è¾¾äººç”¨é€”', 'æ•°é‡']
            purpose_distribution = purpose_counts.to_dict('records')

        # æå–ä¼˜è´¨è¾¾äººæ•°æ®
        premium_detail = []
        if 'è¾¾äººç”¨é€”' in df.columns:
            premium_df = df[df['è¾¾äººç”¨é€”'].str.contains('ä¼˜è´¨è¾¾äºº', na=False)]
            premium_detail = premium_df.head(100).to_dict('records') if not premium_df.empty else []

        # æå–é«˜é˜…è¯»è¾¾äººæ•°æ®
        high_read_detail = []
        if 'è¾¾äººç”¨é€”' in df.columns:
            high_read_df = df[df['è¾¾äººç”¨é€”'].str.contains('é«˜é˜…è¯»è¾¾äºº', na=False)]
            high_read_detail = high_read_df.head(100).to_dict('records') if not high_read_df.empty else []

        # æ±‡æ€»ç»Ÿè®¡
        summary = {
            'æ€»ææŠ¥æ•°': len(df),
            'è¾¾äººç”¨é€”åˆ†å¸ƒ': purpose_distribution,
            'æ¶‰åŠé¡¹ç›®æ•°': df['é¡¹ç›®åç§°'].nunique() if 'é¡¹ç›®åç§°' in df.columns else 0,
            'ä¼˜è´¨è¾¾äººæ•°é‡': len(premium_detail),
            'é«˜é˜…è¯»è¾¾äººæ•°é‡': len(high_read_detail),
            'å¤‡æ³¨': 'ç®€åŒ–å·¥ä½œè´¨é‡åˆ†æï¼ˆä¸ä¾èµ–æ•°æ®ç±»å‹ï¼‰'
        }

        # å–å‰100æ¡ä½œä¸ºæ˜ç»†
        detail_data = df.head(100).to_dict('records') if len(df) > 0 else []

        # å°ç»„æ±‡æ€»ï¼ˆå¦‚æœæœ‰å°ç»„ä¿¡æ¯ï¼‰
        group_summary = []
        if 'å°ç»„åç§°' in df.columns and len(df) > 0:
            group_df = df.groupby('å°ç»„åç§°').agg({
                'è¾¾äººæ˜µç§°': 'count'
            }).reset_index()
            group_df.columns = ['å°ç»„åç§°', 'ææŠ¥æ•°é‡']
            group_summary = group_df.to_dict('records')

        result = {
            "result": detail_data,
            "summary": summary,
            "group_summary": group_summary,
            "quality_distribution": purpose_distribution,
            "premium_detail": premium_detail,
            "high_read_detail": high_read_detail
        }

        logger.info(f"ç®€åŒ–è´¨é‡åˆ†æå®Œæˆï¼Œæ€»è®°å½•æ•°: {len(df)}")

    except Exception as e:
        logger.error(f"ç®€åŒ–è´¨é‡åˆ†æå¤±è´¥: {e}", exc_info=True)

    return result


def create_simple_cost_analysis(df):
    """ç®€åŒ–æˆæœ¬åˆ†æ - ä¿®å¤ç‰ˆ"""
    result = {
        "result": [],
        "summary": {},
        "overall_summary": {},
        "invalid_data_stats": {},
        "media_detail": [],
        "group_summary": [],
        "filtered_summary": {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0},
        "cost_efficiency_ranking": [],
        "detailed_data": []
    }

    try:
        if df.empty:
            return result

        logger.info(f"æ‰§è¡Œç®€åŒ–æˆæœ¬åˆ†æï¼Œæ•°æ®è¡Œæ•°: {len(df)}")

        # ========== ä¿®å¤æˆæœ¬å­—æ®µå¤„ç† ==========
        df_copy = df.copy()

        # æ£€æŸ¥æˆæœ¬å­—æ®µæ˜¯å¦å­˜åœ¨
        cost_field_name = None
        for field in ['æˆæœ¬', 'ä¸‹å•ä»·', 'æŠ¥ä»·', 'cost_amount', 'order_amount', 'cooperation_quote']:
            if field in df_copy.columns:
                cost_field_name = field
                break

        if cost_field_name:
            logger.info(f"ä½¿ç”¨æˆæœ¬å­—æ®µ: {cost_field_name}")

            # ç¡®ä¿æˆæœ¬å­—æ®µæ˜¯æ•°å€¼ç±»å‹
            try:
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åæ¸…ç†
                df_copy['æˆæœ¬_æ•°å€¼'] = df_copy[cost_field_name].astype(str)
                df_copy['æˆæœ¬_æ•°å€¼'] = df_copy['æˆæœ¬_æ•°å€¼'].str.replace(',', '').str.strip()
                # è½¬æ¢ä¸ºæ•°å€¼
                df_copy['æˆæœ¬_æ•°å€¼'] = pd.to_numeric(df_copy['æˆæœ¬_æ•°å€¼'], errors='coerce')
                # å¡«å……NaN
                df_copy['æˆæœ¬_æ•°å€¼'] = df_copy['æˆæœ¬_æ•°å€¼'].fillna(0.0)

                # ç»Ÿè®¡æœ‰æ•ˆæˆæœ¬æ•°æ®
                valid_cost = df_copy['æˆæœ¬_æ•°å€¼'] > 0
                logger.info(f"æœ‰æ•ˆæˆæœ¬æ•°æ®: {valid_cost.sum()}/{len(df_copy)}")

            except Exception as e:
                logger.error(f"è½¬æ¢æˆæœ¬å­—æ®µå¤±è´¥: {e}")
                df_copy['æˆæœ¬_æ•°å€¼'] = 0.0
        else:
            logger.warning("æœªæ‰¾åˆ°æˆæœ¬å­—æ®µï¼Œå…¨éƒ¨è®¾ä¸º0")
            df_copy['æˆæœ¬_æ•°å€¼'] = 0.0

        # ========== è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ ==========
        total_cost = df_copy['æˆæœ¬_æ•°å€¼'].sum()
        avg_cost = df_copy['æˆæœ¬_æ•°å€¼'].mean() if len(df_copy) > 0 else 0

        # è¿”ç‚¹æ¯”ä¾‹è®¡ç®—
        rebate_ratio = 0
        rebate_total = 0
        quote_total = 0

        # æ£€æŸ¥è¿”ç‚¹å­—æ®µ
        rebate_field = None
        for field in ['è¿”ç‚¹', 'rebate_amount']:
            if field in df_copy.columns:
                rebate_field = field
                break

        if rebate_field:
            try:
                # è½¬æ¢è¿”ç‚¹å­—æ®µ
                df_copy['è¿”ç‚¹_æ•°å€¼'] = df_copy[rebate_field].astype(str)
                df_copy['è¿”ç‚¹_æ•°å€¼'] = df_copy['è¿”ç‚¹_æ•°å€¼'].str.replace(',', '').str.strip()
                df_copy['è¿”ç‚¹_æ•°å€¼'] = pd.to_numeric(df_copy['è¿”ç‚¹_æ•°å€¼'], errors='coerce').fillna(0.0)
                rebate_total = df_copy['è¿”ç‚¹_æ•°å€¼'].sum()
            except:
                rebate_total = 0

        # æ£€æŸ¥æŠ¥ä»·å­—æ®µ
        quote_field = None
        for field in ['æŠ¥ä»·', 'cooperation_quote']:
            if field in df_copy.columns:
                quote_field = field
                break

        if quote_field:
            try:
                # è½¬æ¢æŠ¥ä»·å­—æ®µ
                df_copy['æŠ¥ä»·_æ•°å€¼'] = df_copy[quote_field].astype(str)
                df_copy['æŠ¥ä»·_æ•°å€¼'] = df_copy['æŠ¥ä»·_æ•°å€¼'].str.replace(',', '').str.strip()
                df_copy['æŠ¥ä»·_æ•°å€¼'] = pd.to_numeric(df_copy['æŠ¥ä»·_æ•°å€¼'], errors='coerce').fillna(0.0)
                quote_total = df_copy['æŠ¥ä»·_æ•°å€¼'].sum()
            except:
                quote_total = 0

        if quote_total > 0:
            rebate_ratio = (rebate_total / quote_total * 100)

        # ========== æŒ‰åª’ä»‹ç»Ÿè®¡ ==========
        media_detail = []
        if 'å®šæ¡£åª’ä»‹' in df_copy.columns and len(df_copy) > 0:
            # ç¡®ä¿æœ‰æœ‰æ•ˆçš„å®šæ¡£åª’ä»‹å­—æ®µ
            df_copy['å®šæ¡£åª’ä»‹_æ¸…æ´—'] = df_copy['å®šæ¡£åª’ä»‹'].fillna('æœªçŸ¥åª’ä»‹').astype(str)

            # æŒ‰åª’ä»‹åˆ†ç»„ç»Ÿè®¡
            media_stats = df_copy.groupby('å®šæ¡£åª’ä»‹_æ¸…æ´—').agg({
                'æˆæœ¬_æ•°å€¼': 'sum',
                'è¾¾äººæ˜µç§°': 'count'
            }).reset_index()

            media_stats.columns = ['å®šæ¡£åª’ä»‹', 'æ€»æˆæœ¬', 'å¤„ç†è¾¾äººæ•°é‡']

            # è®¡ç®—å¹³å‡æˆæœ¬ï¼ˆé¿å…é™¤ä»¥0ï¼‰
            media_stats['å¹³å‡æˆæœ¬'] = media_stats.apply(
                lambda row: row['æ€»æˆæœ¬'] / row['å¤„ç†è¾¾äººæ•°é‡'] if row['å¤„ç†è¾¾äººæ•°é‡'] > 0 else 0,
                axis=1
            )

            # å¦‚æœæœ‰è¿”ç‚¹ï¼Œæ·»åŠ è¿”ç‚¹ä¿¡æ¯
            if 'è¿”ç‚¹_æ•°å€¼' in df_copy.columns:
                media_rebate = df_copy.groupby('å®šæ¡£åª’ä»‹_æ¸…æ´—')['è¿”ç‚¹_æ•°å€¼'].sum().reset_index()
                media_rebate.columns = ['å®šæ¡£åª’ä»‹', 'æ€»è¿”ç‚¹']
                media_stats = pd.merge(media_stats, media_rebate, on='å®šæ¡£åª’ä»‹', how='left')

            media_detail = media_stats.to_dict('records')
            logger.info(f"åª’ä»‹ç»Ÿè®¡å®Œæˆï¼Œæ¶‰åŠåª’ä»‹æ•°: {len(media_detail)}")

        # ========== æ„å»ºç»“æœ ==========
        summary = {
            'æ€»æ•°æ®æ¡æ•°': len(df_copy),
            'æ€»æˆæœ¬': total_cost,
            'æ•´ä½“å¹³å‡æˆæœ¬': avg_cost,
            'æ•´ä½“è¿”ç‚¹å æŠ¥ä»·æ¯”ä¾‹(%)': f"{rebate_ratio:.2f}%" if rebate_ratio > 0 else "0%",
            'æœ‰æ•ˆæ•°æ®æ¡æ•°': len(df_copy),
            'æ— æ•ˆæ•°æ®æ¡æ•°': 0,
            'å¤‡æ³¨': 'ç®€åŒ–æˆæœ¬åˆ†æï¼ˆä¿®å¤ç‰ˆï¼‰'
        }

        invalid_stats = {
            'æ€»æ•°æ®æ¡æ•°': len(df_copy),
            'æœ‰æ•ˆæ•°æ®æ¡æ•°': len(df_copy),
            'æ— æ•ˆæ•°æ®æ¡æ•°': 0,
            'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)': '100%',
            'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)': '0%',
            'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ': {},
            'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)': 0
        }

        # æˆæœ¬æ•ˆç‡æ’åï¼ˆæŒ‰å¹³å‡æˆæœ¬å‡åºï¼‰
        cost_efficiency_ranking = []
        if media_detail:
            ranking_df = pd.DataFrame(media_detail)
            if 'å¹³å‡æˆæœ¬' in ranking_df.columns:
                ranking_df = ranking_df[ranking_df['å¤„ç†è¾¾äººæ•°é‡'] > 0]  # åªå–æœ‰æ•°æ®çš„åª’ä»‹
                ranking_df = ranking_df.sort_values('å¹³å‡æˆæœ¬', ascending=True).head(10)
                cost_efficiency_ranking = ranking_df.to_dict('records')

        # è¯¦ç»†æ•°æ®ï¼ˆå–å‰100æ¡ï¼‰
        detailed_data = []
        if len(df_copy) > 0:
            # é€‰æ‹©å…³é”®å­—æ®µ
            key_columns = []
            for col in ['è¾¾äººæ˜µç§°', 'é¡¹ç›®åç§°', 'å®šæ¡£åª’ä»‹', 'æˆæœ¬_æ•°å€¼', 'æŠ¥ä»·_æ•°å€¼', 'è¿”ç‚¹_æ•°å€¼']:
                if col in df_copy.columns:
                    key_columns.append(col)

            if key_columns:
                detailed_df = df_copy[key_columns].head(100)
                # é‡å‘½ååˆ—
                column_mapping = {
                    'æˆæœ¬_æ•°å€¼': 'æˆæœ¬',
                    'æŠ¥ä»·_æ•°å€¼': 'æŠ¥ä»·',
                    'è¿”ç‚¹_æ•°å€¼': 'è¿”ç‚¹'
                }
                detailed_df = detailed_df.rename(columns=column_mapping)
                detailed_data = detailed_df.to_dict('records')

        result = {
            "result": media_detail,
            "summary": summary,
            "overall_summary": summary,
            "invalid_data_stats": invalid_stats,
            "media_detail": media_detail,
            "group_summary": [],
            "filtered_summary": {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0},
            "cost_efficiency_ranking": cost_efficiency_ranking,
            "detailed_data": detailed_data,
            "media_group_workload": [],
            "fixed_media_workload": media_detail,
            "fixed_media_cost": media_detail,
            "fixed_media_rebate": [],
            "fixed_media_performance": [],
            "fixed_media_level": [],
            "fixed_media_comprehensive": []
        }

        logger.info(f"ç®€åŒ–æˆæœ¬åˆ†æå®Œæˆï¼Œæ€»æˆæœ¬: {total_cost}")

    except Exception as e:
        logger.error(f"ç®€åŒ–æˆæœ¬åˆ†æå¤±è´¥: {e}", exc_info=True)
        # è¿”å›æœ€åŸºæœ¬çš„ç»“æœ
        result = {
            "result": [],
            "summary": {"æ€»æ•°æ®æ¡æ•°": len(df) if not df.empty else 0, "å¤‡æ³¨": f"åˆ†æå¤±è´¥: {str(e)[:100]}"},
            "overall_summary": {},
            "invalid_data_stats": {},
            "media_detail": [],
            "group_summary": [],
            "filtered_summary": {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0},
            "cost_efficiency_ranking": [],
            "detailed_data": []
        }

    return result


def convert_list_to_dataframe(data_list, default_columns=None):
    """å°†åˆ—è¡¨è½¬æ¢ä¸ºDataFrameï¼Œç¡®ä¿ReportGeneratorèƒ½æ­£ç¡®å¤„ç†"""
    if not data_list:
        return pd.DataFrame()

    try:
        if isinstance(data_list, list):
            if len(data_list) == 0:
                return pd.DataFrame()

            # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦ä¸ºå­—å…¸
            if isinstance(data_list[0], dict):
                return pd.DataFrame(data_list)
            else:
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œåˆ›å»ºç®€å•DataFrame
                if default_columns:
                    return pd.DataFrame(data_list, columns=default_columns)
                else:
                    return pd.DataFrame({'æ•°æ®': data_list})
        else:
            # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•ç›´æ¥è½¬æ¢
            return pd.DataFrame(data_list)
    except Exception as e:
        logger.warning(f"è½¬æ¢åˆ—è¡¨åˆ°DataFrameå¤±è´¥: {e}")
        return pd.DataFrame()

@app.context_processor
def inject_common_variables():
    """æ³¨å…¥å…¨å±€é€šç”¨å˜é‡åˆ°æ‰€æœ‰æ¨¡æ¿ï¼Œæ— éœ€æ‰‹åŠ¨ä¼ å‚"""
    now = datetime.now()
    return {
        'current_year': now.year,
        'current_date': now.strftime('%Y-%m-%d'),
        'current_datetime': now.strftime('%Y-%m-%d %H:%M:%S'),
        'app': app,
        'has_endpoint': has_endpoint,
        'view_functions': app.view_functions
    }

# ========== æ ¸å¿ƒä¿®å¤ï¼šä¼˜åŒ– load_analysis_result å‡½æ•°ï¼Œç¡®ä¿æ•°æ®æ­£ç¡®åŠ è½½ ==========
def load_analysis_result(analysis_id):
    """
    æ ¸å¿ƒä¼˜åŒ–ï¼šä»å†…å­˜/æœ¬åœ°æ–‡ä»¶åŠ è½½åˆ†æç»“æœï¼Œè‡ªåŠ¨å®Œæˆç±»å‹è½¬æ¢+æ•°æ®å…œåº•
    æ‰€æœ‰è¿”å›æ•°æ®å‡ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œæ¨¡æ¿æ¸²æŸ“ç»å¯¹æ— æŠ¥é”™
    """

    # ä¼˜å…ˆä»å†…å­˜è¯»å–ï¼ˆé€Ÿåº¦å¿«ï¼‰
    if analysis_id in analysis_results:
        analysis_data = analysis_results[analysis_id].copy()

        # âœ… æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿full_resultå­˜åœ¨ä¸”åŒ…å«å„æ¨¡å—æ•°æ®
        if 'full_result' not in analysis_data:
            # å¦‚æœfull_resultä¸å­˜åœ¨ï¼Œä»åŸå§‹æ•°æ®æ„å»º
            analysis_data['full_result'] = {
                'workload': analysis_data.get('workload', {}),
                'quality': analysis_data.get('quality', {}),
                'cost': analysis_data.get('cost', {})
            }

        full_result = analysis_data.get('full_result', {})

        # âœ… æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿æ•°æ®ç»“æ„æ­£ç¡®
        if not isinstance(full_result, dict):
            full_result = {}

        # è½¬æ¢æ•°æ®ç±»å‹
        full_result = convert_pandas_types_to_python(full_result)

        # ========================== æ ¸å¿ƒä¿®å¤ï¼šç»Ÿä¸€æ•°æ®é”®å ==========================
        # å°† workload ä¸­çš„ 'detail' è½¬æ¢ä¸º 'result'
        if 'workload' in full_result:
            workload_data = full_result['workload']
            if isinstance(workload_data, dict):
                # ç¡®ä¿æœ‰resultå­—æ®µ
                if 'detail' in workload_data and 'result' not in workload_data:
                    workload_data['result'] = workload_data.pop('detail', [])
                elif 'detail_df' in workload_data and 'result' not in workload_data:
                    workload_data['result'] = workload_data.pop('detail_df', [])
                elif 'result' not in workload_data:
                    workload_data['result'] = []

                # ç¡®ä¿å…¶ä»–å­—æ®µå­˜åœ¨
                workload_data['summary'] = workload_data.get('summary', {})
                workload_data['group_summary'] = workload_data.get('group_summary', [])
                workload_data['top_media_ranking'] = workload_data.get('top_media_ranking', [])

        # å°† quality ä¸­çš„ 'detail' è½¬æ¢ä¸º 'result'
        if 'quality' in full_result:
            quality_data = full_result['quality']
            if isinstance(quality_data, dict):
                if 'detail' in quality_data and 'result' not in quality_data:
                    quality_data['result'] = quality_data.pop('detail', [])
                elif 'detail_df' in quality_data and 'result' not in quality_data:
                    quality_data['result'] = quality_data.pop('detail_df', [])
                elif 'result' not in quality_data:
                    quality_data['result'] = []

                # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿åˆ†ç±»æ•°æ®å­—æ®µå­˜åœ¨ä¸”æ˜¯åˆ—è¡¨
                quality_data['premium_detail'] = quality_data.get('premium_detail', [])
                quality_data['high_read_detail'] = quality_data.get('high_read_detail', [])

                # ç¡®ä¿å…¶ä»–å­—æ®µå­˜åœ¨
                quality_data['summary'] = quality_data.get('summary', {})
                quality_data['group_summary'] = quality_data.get('group_summary', [])
                quality_data['quality_distribution'] = quality_data.get('quality_distribution', [])

                # âœ… ä¿®å¤ï¼šç¡®ä¿group_summaryæ˜¯åˆ—è¡¨
                if not isinstance(quality_data['group_summary'], list):
                    quality_data['group_summary'] = []
                if not isinstance(quality_data['quality_distribution'], list):
                    quality_data['quality_distribution'] = []
                if not isinstance(quality_data['premium_detail'], list):
                    quality_data['premium_detail'] = []
                if not isinstance(quality_data['high_read_detail'], list):
                    quality_data['high_read_detail'] = []

        # ========================== âœ… æ–°å¢æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿ cost æ•°æ®åŒ…å«æ— æ•ˆæ•°æ®ç»Ÿè®¡ ==========================
        if 'cost' in full_result:
            cost_data = full_result['cost']
            if not isinstance(cost_data, dict):
                cost_data = {}
                full_result['cost'] = cost_data

            # âœ… æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿ invalid_data_detail å’Œ invalid_data_stats å­—æ®µå­˜åœ¨
            if 'invalid_data_detail' not in cost_data:
                cost_data['invalid_data_detail'] = []

            # âœ… ä» overall_summary æå–æ— æ•ˆæ•°æ®ç»Ÿè®¡
            overall_summary = cost_data.get('overall_summary', {})
            if not isinstance(overall_summary, dict):
                overall_summary = {}
                cost_data['overall_summary'] = overall_summary

            # âœ… ç¡®ä¿ overall_summary åŒ…å«æ— æ•ˆæ•°æ®ç»Ÿè®¡å­—æ®µ
            if 'æ€»æ•°æ®æ¡æ•°' not in overall_summary:
                overall_summary['æ€»æ•°æ®æ¡æ•°'] = 0
            if 'æœ‰æ•ˆæ•°æ®æ¡æ•°' not in overall_summary:
                overall_summary['æœ‰æ•ˆæ•°æ®æ¡æ•°'] = 0
            if 'æ— æ•ˆæ•°æ®æ¡æ•°' not in overall_summary:
                overall_summary['æ— æ•ˆæ•°æ®æ¡æ•°'] = 0
            if 'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                overall_summary['æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = '0%'
            if 'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                overall_summary['æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = '0%'
            if 'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ' not in overall_summary:
                overall_summary['æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ'] = {}
            if 'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)' not in overall_summary:
                overall_summary['æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)'] = 0

            # âœ… åˆ›å»ºç‹¬ç«‹çš„ invalid_data_stats å­—æ®µ
            invalid_data_stats = {
                'æ€»æ•°æ®æ¡æ•°': overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0),
                'æœ‰æ•ˆæ•°æ®æ¡æ•°': overall_summary.get('æœ‰æ•ˆæ•°æ®æ¡æ•°', 0),
                'æ— æ•ˆæ•°æ®æ¡æ•°': overall_summary.get('æ— æ•ˆæ•°æ®æ¡æ•°', 0),
                'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)': overall_summary.get('æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%'),
                'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)': overall_summary.get('æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%'),
                'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ': overall_summary.get('æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ', {}),
                'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)': overall_summary.get('æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)', 0)
            }

            cost_data['invalid_data_stats'] = invalid_data_stats
            # ========================== âœ… æ–°å¢æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿å¼‚å¸¸æ•°æ®ç›¸å…³å­—æ®µå­˜åœ¨ ==========================
            # åœ¨ load_analysis_result å‡½æ•°ä¸­æŸ¥æ‰¾è¿™éƒ¨åˆ†ä»£ç 
            if 'cost' in full_result:
                cost_data = full_result['cost']

                # âœ… ç¡®ä¿å¼‚å¸¸æ•°æ®è¯¦æƒ…å­—æ®µå­˜åœ¨
                if 'abnormal_data_detail' not in cost_data:
                    cost_data['abnormal_data_detail'] = []

                # âœ… ç¡®ä¿å¼‚å¸¸æ•°æ®ç»Ÿè®¡å­—æ®µå­˜åœ¨
                if 'overall_summary' in cost_data:
                    overall_summary = cost_data['overall_summary']

                    # ç¡®ä¿ overall_summary åŒ…å«å¼‚å¸¸æ•°æ®ç»Ÿè®¡å­—æ®µ
                    if 'å¼‚å¸¸æ•°æ®æ¡æ•°' not in overall_summary:
                        overall_summary['å¼‚å¸¸æ•°æ®æ¡æ•°'] = 0
                    if 'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                        overall_summary['å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)'] = '0%'
                    if 'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ' not in overall_summary:
                        overall_summary['å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ'] = {}
                    if 'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)' not in overall_summary:
                        overall_summary['å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)'] = 0
                    if 'å‚ä¸åˆ†ææ•°æ®æ¡æ•°' not in overall_summary:
                        overall_summary['å‚ä¸åˆ†ææ•°æ®æ¡æ•°'] = overall_summary.get('æ€»æ•°æ®æ¡æ•°',
                                                                                  0) - overall_summary.get(
                            'æ— æ•ˆæ•°æ®æ¡æ•°', 0)
                    if 'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                        overall_summary['å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)'] = '100%'

                    # âœ… åˆ›å»ºç‹¬ç«‹çš„ abnormal_data_stats å­—æ®µ
                    abnormal_data_stats = {
                        'å¼‚å¸¸æ•°æ®æ¡æ•°': overall_summary.get('å¼‚å¸¸æ•°æ®æ¡æ•°', 0),
                        'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)': overall_summary.get('å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)', '0%'),
                        'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ': overall_summary.get('å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ', {}),
                        'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)': overall_summary.get('å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)', 0),
                        'å‚ä¸åˆ†ææ•°æ®æ¡æ•°': overall_summary.get('å‚ä¸åˆ†ææ•°æ®æ¡æ•°', 0),
                        'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)': overall_summary.get('å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)', '100%')
                    }

                    cost_data['abnormal_data_stats'] = abnormal_data_stats

            if 'cost' in full_result:
                cost_data = full_result['cost']

                # âœ… ç¡®ä¿å¼‚å¸¸æ•°æ®è¯¦æƒ…å­—æ®µå­˜åœ¨
                if 'abnormal_data_detail' not in cost_data:
                    cost_data['abnormal_data_detail'] = []

                # âœ… ä» overall_summary æå–å¼‚å¸¸æ•°æ®ç»Ÿè®¡
                if 'overall_summary' in cost_data:
                    overall_summary = cost_data['overall_summary']

                    # ç¡®ä¿ overall_summary åŒ…å«å¼‚å¸¸æ•°æ®ç»Ÿè®¡å­—æ®µ
                    if 'å¼‚å¸¸æ•°æ®æ¡æ•°' not in overall_summary:
                        overall_summary['å¼‚å¸¸æ•°æ®æ¡æ•°'] = 0
                    if 'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                        overall_summary['å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)'] = '0%'
                    if 'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ' not in overall_summary:
                        overall_summary['å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ'] = {}
                    if 'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)' not in overall_summary:
                        overall_summary['å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)'] = 0
                    if 'å‚ä¸åˆ†ææ•°æ®æ¡æ•°' not in overall_summary:
                        overall_summary['å‚ä¸åˆ†ææ•°æ®æ¡æ•°'] = overall_summary.get('æ€»æ•°æ®æ¡æ•°',
                                                                                  0) - overall_summary.get(
                            'æ— æ•ˆæ•°æ®æ¡æ•°', 0)
                    if 'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                        overall_summary['å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)'] = '100%'

                    # âœ… åˆ›å»ºç‹¬ç«‹çš„ abnormal_data_stats å­—æ®µ
                    abnormal_data_stats = {
                        'å¼‚å¸¸æ•°æ®æ¡æ•°': overall_summary.get('å¼‚å¸¸æ•°æ®æ¡æ•°', 0),
                        'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)': overall_summary.get('å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)', '0%'),
                        'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ': overall_summary.get('å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ', {}),
                        'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)': overall_summary.get('å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)', 0),
                        'å‚ä¸åˆ†ææ•°æ®æ¡æ•°': overall_summary.get('å‚ä¸åˆ†ææ•°æ®æ¡æ•°', 0),
                        'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)': overall_summary.get('å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)', '100%')
                    }

                    cost_data['abnormal_data_stats'] = abnormal_data_stats

            # âœ… ç¡®ä¿å…¶ä»–æˆæœ¬å­—æ®µå­˜åœ¨
            cost_data['result'] = cost_data.get('result', [])
            cost_data['cleaned_data'] = cost_data.get('cleaned_data', [])
            cost_data['filtered_data'] = cost_data.get('filtered_data', [])
            cost_data['summary'] = overall_summary
            cost_data['overall_summary'] = overall_summary
            cost_data['detail_data'] = cost_data.get('detail_data', [])
            cost_data['media_detail'] = cost_data.get('media_detail', [])
            cost_data['group_summary'] = cost_data.get('group_summary', [])
            cost_data['filtered_summary'] = cost_data.get('filtered_summary', {})
            cost_data['cost_efficiency_ranking'] = cost_data.get('cost_efficiency_ranking', [])

            # âœ… ç¡®ä¿æˆæœ¬å‘æŒ¥åˆ†æçš„æ‰€æœ‰å·¥ä½œè¡¨å­—æ®µå­˜åœ¨
            cost_data['media_group_workload'] = cost_data.get('media_group_workload', [])
            cost_data['fixed_media_workload'] = cost_data.get('fixed_media_workload', [])
            cost_data['fixed_media_cost'] = cost_data.get('fixed_media_cost', [])
            cost_data['fixed_media_rebate'] = cost_data.get('fixed_media_rebate', [])
            cost_data['fixed_media_performance'] = cost_data.get('fixed_media_performance', [])
            cost_data['fixed_media_level'] = cost_data.get('fixed_media_level', [])
            cost_data['fixed_media_comprehensive'] = cost_data.get('fixed_media_comprehensive', [])
            cost_data['detailed_data'] = cost_data.get('detailed_data', [])

        # ========================== æ ¸å¿ƒä¿®å¤ 2/5ï¼šè¡¥å…¨å°ç»„æ•°æ®+è´¨é‡åˆ†å¸ƒæ•°æ® å¹¶åšç©ºå€¼å…œåº• ==========================
        # å·¥ä½œé‡å°ç»„æ•°æ®å…œåº• + âœ… å…³é”®ä¿®å¤ï¼šè¡¥å…¨æ€»å®šæ¡£æ•°/æ€»ææŠ¥æ•°å­—æ®µ
        workload_group = full_result.get('workload', {}).get('group_summary', [])
        if isinstance(workload_group, list):
            workload_group = fill_group_data_fields(workload_group)
            full_result['workload']['group_summary'] = workload_group
        else:
            full_result['workload']['group_summary'] = []

        # è´¨é‡åˆ†å¸ƒæ•°æ®å…œåº•
        quality_dist = full_result.get('quality', {}).get('quality_distribution', [])
        if not isinstance(quality_dist, list):
            quality_dist = []
        full_result['quality']['quality_distribution'] = quality_dist

        # è´¨é‡å°ç»„æ•°æ®å…œåº•
        quality_group = full_result.get('quality', {}).get('group_summary', [])
        if isinstance(quality_group, list):
            quality_group = fill_group_data_fields(quality_group)
            full_result['quality']['group_summary'] = quality_group
        else:
            full_result['quality']['group_summary'] = []

        # âœ… å…³é”®ä¿®å¤ï¼šè¡¥å…¨åˆ†ç±»æ•°æ®
        premium_detail = full_result.get('quality', {}).get('premium_detail', [])
        if not isinstance(premium_detail, list):
            premium_detail = []
        full_result['quality']['premium_detail'] = premium_detail

        high_read_detail = full_result.get('quality', {}).get('high_read_detail', [])
        if not isinstance(high_read_detail, list):
            high_read_detail = []
        full_result['quality']['high_read_detail'] = high_read_detail

        # æ•°æ®å…œåº•ï¼šç¡®ä¿æ‰€æœ‰æ ¸å¿ƒå­—æ®µæ˜¯åˆ—è¡¨/å­—å…¸ï¼Œé¿å…æ¨¡æ¿éå†æŠ¥é”™
        workload_top = full_result.get('workload', {}).get('result', [])
        if not isinstance(workload_top, list):
            workload_top = []
        full_result['workload']['result'] = workload_top

        quality_top = full_result.get('quality', {}).get('result', [])
        if not isinstance(quality_top, list):
            quality_top = []
        full_result['quality']['result'] = quality_top

        cost_efficiency_ranking = full_result.get('cost', {}).get('cost_efficiency_ranking', [])
        cost_result = full_result.get('cost', {}).get('result', [])
        if not isinstance(cost_result, list):
            cost_result = []
        full_result['cost']['result'] = cost_efficiency_ranking if cost_efficiency_ranking else cost_result

        # âœ… æ ¸å¿ƒä¿®å¤ï¼šæˆæœ¬æ•°æ® åŒå±‚å…œåº• - è§£å†³overall_summaryæœªå®šä¹‰æŠ¥é”™
        cost_overall = full_result.get('cost', {}).get('overall_summary', {}) or {}
        cost_summary = full_result.get('cost', {}).get('summary', {}) or {}
        # åˆå¹¶ä¸¤ä¸ªæ±‡æ€»ï¼Œä¼˜å…ˆoverall_summaryï¼Œå½»åº•è§£å†³æ¨¡æ¿{{ overall_summary.get('æ•´ä½“å¹³å‡æˆæœ¬',0) }}æŠ¥é”™
        full_cost_summary = {**cost_summary, **cost_overall}
        full_result['cost']['overall_summary'] = full_cost_summary
        full_result['cost']['summary'] = full_cost_summary

        # ========================== æ ¸å¿ƒä¿®å¤ 3/5ï¼šæˆæœ¬æ•°æ®å­—æ®µè¡¥å…¨ ==========================
        # è¡¥å…¨åª’ä»‹æ˜ç»†/è¿‡æ»¤æ•°æ®/æ’åæ•°æ®çš„æ‰€æœ‰ç¼ºå¤±å­—æ®µï¼Œè§£å†³ç­›é™¤æ€»æˆæœ¬æŠ¥é”™
        full_result['cost']['media_detail'] = fill_cost_data_fields(full_result.get('cost', {}).get('media_detail', []))
        full_result['cost']['filtered_data'] = fill_cost_data_fields(
            full_result.get('cost', {}).get('filtered_data', []))
        full_result['cost']['cost_efficiency_ranking'] = fill_cost_data_fields(
            full_result.get('cost', {}).get('cost_efficiency_ranking', []))
        full_result['cost']['result'] = fill_cost_data_fields(full_result.get('cost', {}).get('result', []))

        # âœ… æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿detail_dataå­—æ®µå­˜åœ¨
        full_result['cost']['detail_data'] = full_result['cost']['media_detail']

        # è¿‡æ»¤æ±‡æ€»å­—æ®µå…œåº•
        if not full_result['cost']['filtered_summary'] or not isinstance(full_result['cost']['filtered_summary'], dict):
            full_result['cost']['filtered_summary'] = {
                'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0, 'ç­›é™¤è¾¾äººæ•°é‡': 0, 'ç­›é™¤å‘å¸ƒæ•°é‡': 0
            }
        else:
            # ä¸ºè¿‡æ»¤æ±‡æ€»è¡¥å…¨ç¼ºå¤±å­—æ®µ
            fs = full_result['cost']['filtered_summary']
            fs['ç­›é™¤æ€»æˆæœ¬'] = fs.get('ç­›é™¤æ€»æˆæœ¬', 0) or 0
            fs['ç­›é™¤æˆæœ¬å æ¯”'] = fs.get('ç­›é™¤æˆæœ¬å æ¯”', 0) or 0
            fs['ç­›é™¤è¾¾äººæ•°é‡'] = fs.get('ç­›é™¤è¾¾äººæ•°é‡', 0) or 0
            fs['ç­›é™¤å‘å¸ƒæ•°é‡'] = fs.get('ç­›é™¤å‘å¸ƒæ•°é‡', 0) or 0

        # ========================== âœ… è‡´å‘½ä¿®å¤ æ ¸å¿ƒæ ¹å› ï¼šæ–°å¢ detail_data å…œåº•èµ‹å€¼ ==========================
        # è§£å†³ cost_analysis.html ç¬¬768è¡Œ const detailData = {{ detail_data|tojson|safe }}; æŠ¥Undefinedåºåˆ—åŒ–é”™è¯¯
        full_result['cost']['detail_data'] = full_result['cost']['media_detail']

        # é¢„å¤„ç†æˆæœ¬ç™¾åˆ†æ•°å­—æ®µ
        if isinstance(full_cost_summary, dict):
            rebate_key = 'æ•´ä½“è¿”ç‚¹å æŠ¥ä»·æ¯”ä¾‹(%)'
            full_cost_summary[rebate_key + '_num'] = preprocess_percent_str_to_float(
                full_cost_summary.get(rebate_key, '0%'))
            cost_keys = list(full_cost_summary.keys())
            for key in cost_keys:
                if '%' in str(key) and f'{key}_num' not in full_cost_summary:
                    full_cost_summary[f'{key}_num'] = preprocess_percent_str_to_float(full_cost_summary[key])

        full_result['cost']['summary'] = full_cost_summary
        analysis_data['full_result'] = full_result

        # âœ… ç¡®ä¿analysis_dataåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
        analysis_data['category'] = analysis_data.get('category', 'æœªçŸ¥ç±»ç›®')
        analysis_data['timestamp'] = analysis_data.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        analysis_data['selected_groups'] = analysis_data.get('selected_groups', [])
        analysis_data['reports'] = analysis_data.get('reports', {
            "workload": {"excel": ""},
            "quality": {"excel": ""},
            "cost": {"excel": ""},
            "full": {"full_excel": ""}
        })

        return analysis_data

    # å†…å­˜æ— æ•°æ®ï¼Œä»æœ¬åœ°JSONè¯»å–
    result_file = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results', f'{analysis_id}.json')
    if os.path.exists(result_file):
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # ========================== æ ¸å¿ƒä¿®å¤ï¼šè½¬æ¢æœ¬åœ°å­˜å‚¨çš„é”®å ==========================
            # è½¬æ¢ workload æ•°æ®
            workload_data = data.get('workload', {})
            if 'detail' in workload_data and 'result' not in workload_data:
                workload_data['result'] = workload_data.pop('detail', [])

            # è½¬æ¢ quality æ•°æ®
            quality_data = data.get('quality', {})
            if 'detail' in quality_data and 'result' not in quality_data:
                quality_data['result'] = quality_data.pop('detail', [])

            # âœ… å…³é”®ä¿®å¤ï¼šè¯»å–åˆ†ç±»æ•°æ®å¹¶ç¡®ä¿æ˜¯åˆ—è¡¨
            premium_detail = quality_data.get('premium_detail', [])
            if not isinstance(premium_detail, list):
                premium_detail = []

            high_read_detail = quality_data.get('high_read_detail', [])
            if not isinstance(high_read_detail, list):
                high_read_detail = []

            quality_distribution = quality_data.get('quality_distribution', [])
            if not isinstance(quality_distribution, list):
                quality_distribution = []

            group_summary = quality_data.get('group_summary', [])
            if not isinstance(group_summary, list):
                group_summary = []

            # âœ… æ ¸å¿ƒä¿®å¤ï¼šè¯»å– cost æ•°æ®ï¼Œç¡®ä¿åŒ…å«æ— æ•ˆæ•°æ®ç»Ÿè®¡
            cost_data = data.get('cost', {})
            if not isinstance(cost_data, dict):
                cost_data = {}

            # âœ… ç¡®ä¿ overall_summary å­˜åœ¨å¹¶åŒ…å«æ— æ•ˆæ•°æ®ç»Ÿè®¡
            overall_summary = cost_data.get('overall_summary', {})
            if not isinstance(overall_summary, dict):
                overall_summary = {}

            # âœ… ç¡®ä¿ overall_summary åŒ…å«æ— æ•ˆæ•°æ®ç»Ÿè®¡å­—æ®µ
            overall_summary['æ€»æ•°æ®æ¡æ•°'] = overall_summary.get('æ€»æ•°æ®æ¡æ•°', data.get('æ€»è®°å½•æ•°', 0))
            overall_summary['æœ‰æ•ˆæ•°æ®æ¡æ•°'] = overall_summary.get('æœ‰æ•ˆæ•°æ®æ¡æ•°',
                                                                  overall_summary.get('æœ‰æ•ˆæ•°æ®æ¡æ•°', 0))
            overall_summary['æ— æ•ˆæ•°æ®æ¡æ•°'] = overall_summary.get('æ— æ•ˆæ•°æ®æ¡æ•°',
                                                                  overall_summary.get('æ— æ•ˆæ•°æ®æ¡æ•°', 0))

            if overall_summary['æ€»æ•°æ®æ¡æ•°'] > 0:
                overall_summary['æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = overall_summary.get('æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)',
                                                                         f"{(overall_summary['æœ‰æ•ˆæ•°æ®æ¡æ•°'] / overall_summary['æ€»æ•°æ®æ¡æ•°'] * 100):.2f}%")
                overall_summary['æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = overall_summary.get('æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)',
                                                                         f"{(overall_summary['æ— æ•ˆæ•°æ®æ¡æ•°'] / overall_summary['æ€»æ•°æ®æ¡æ•°'] * 100):.2f}%")
            else:
                overall_summary['æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = '0%'
                overall_summary['æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = '0%'

            overall_summary['æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ'] = overall_summary.get('æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ', {})
            overall_summary['æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)'] = overall_summary.get('æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)', 0)

            # âœ… åˆ›å»º invalid_data_stats
            invalid_data_stats = {
                'æ€»æ•°æ®æ¡æ•°': overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0),
                'æœ‰æ•ˆæ•°æ®æ¡æ•°': overall_summary.get('æœ‰æ•ˆæ•°æ®æ¡æ•°', 0),
                'æ— æ•ˆæ•°æ®æ¡æ•°': overall_summary.get('æ— æ•ˆæ•°æ®æ¡æ•°', 0),
                'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)': overall_summary.get('æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%'),
                'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)': overall_summary.get('æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%'),
                'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ': overall_summary.get('æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ', {}),
                'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)': overall_summary.get('æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)', 0)
            }

            cost_summary = data.get('cost_summary', {})
            full_cost_summary = {**cost_summary, **overall_summary}

            if isinstance(full_cost_summary, dict):
                rebate_key = 'æ•´ä½“è¿”ç‚¹å æŠ¥ä»·æ¯”ä¾‹(%)'
                full_cost_summary[rebate_key + '_num'] = preprocess_percent_str_to_float(
                    full_cost_summary.get(rebate_key, '0%'))
                cost_keys = list(full_cost_summary.keys())
                for key in cost_keys:
                    if '%' in str(key) and f'{key}_num' not in full_cost_summary:
                        full_cost_summary[f'{key}_num'] = preprocess_percent_str_to_float(full_cost_summary[key])

            # è¯»å–æ—¶ä¹Ÿè¡¥å…¨å°ç»„å­—æ®µ+æˆæœ¬å­—æ®µ
            workload_group = fill_group_data_fields(workload_data.get('group_summary', []))
            cost_media_detail = fill_cost_data_fields(cost_data.get('media_detail', []))

            # âœ… è¯»å– invalid_data_detail
            invalid_data_detail = cost_data.get('invalid_data_detail', [])
            if not isinstance(invalid_data_detail, list):
                invalid_data_detail = []

            # âœ… ç¡®ä¿ cost_data åŒ…å«æ‰€æœ‰å·¥ä½œè¡¨
            media_group_workload = cost_data.get('media_group_workload', [])
            fixed_media_workload = cost_data.get('fixed_media_workload', [])
            fixed_media_cost = cost_data.get('fixed_media_cost', [])
            fixed_media_rebate = cost_data.get('fixed_media_rebate', [])
            fixed_media_performance = cost_data.get('fixed_media_performance', [])
            fixed_media_level = cost_data.get('fixed_media_level', [])
            fixed_media_comprehensive = cost_data.get('fixed_media_comprehensive', [])

            result = {
                'analysis_id': analysis_id,
                'timestamp': data.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                'category': data.get('category', 'æœªçŸ¥ç±»ç›®'),
                'selected_groups': data.get('selected_groups', []),
                'full_result': {
                    'workload': {
                        'result': workload_data.get('result', []),
                        'summary': workload_data.get('summary', {}),
                        'group_summary': workload_group
                    },
                    'quality': {
                        'result': quality_data.get('result', []),
                        'summary': quality_data.get('summary', {}),
                        'group_summary': group_summary,
                        'quality_distribution': quality_distribution,
                        'premium_detail': premium_detail,  # âœ… æ–°å¢ï¼šä¼˜è´¨è¾¾äººæ•°æ®
                        'high_read_detail': high_read_detail  # âœ… æ–°å¢ï¼šé«˜é˜…è¯»è¾¾äººæ•°æ®
                    },
                    'cost': {
                        'result': cost_media_detail,
                        'cleaned_data': cost_data.get('cleaned_data', []),
                        'filtered_data': cost_data.get('filtered_data', []),
                        'summary': full_cost_summary,
                        'overall_summary': full_cost_summary,
                        'detail_data': cost_media_detail,
                        'media_detail': cost_media_detail,
                        'group_summary': cost_data.get('group_summary', []),
                        'filtered_summary': cost_data.get('filtered_summary', {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0}),
                        'cost_efficiency_ranking': cost_data.get('cost_efficiency_ranking', []),
                        # âœ… æ–°å¢ï¼šæ— æ•ˆæ•°æ®ç›¸å…³å­—æ®µ
                        'invalid_data_detail': invalid_data_detail,
                        'invalid_data_stats': invalid_data_stats,
                        # âœ… æ–°å¢ï¼šæˆæœ¬å‘æŒ¥åˆ†ææ‰€æœ‰å·¥ä½œè¡¨
                        'media_group_workload': media_group_workload,
                        'fixed_media_workload': fixed_media_workload,
                        'fixed_media_cost': fixed_media_cost,
                        'fixed_media_rebate': fixed_media_rebate,
                        'fixed_media_performance': fixed_media_performance,
                        'fixed_media_level': fixed_media_level,
                        'fixed_media_comprehensive': fixed_media_comprehensive,
                        'detailed_data': cost_data.get('detailed_data', [])
                    }
                },
                'reports': {
                    'workload': {'excel': data.get('report_files', {}).get('workload', '')},
                    'quality': {'excel': data.get('report_files', {}).get('quality', '')},
                    'cost': {'excel': data.get('report_files', {}).get('cost', '')},
                    'full': {'full_excel': data.get('report_files', {}).get('full', '')}
                }
            }

            return result

        except Exception as e:
            logger.error(f"âŒ è¯»å–æœ¬åœ°åˆ†æç»“æœå¤±è´¥ï¼š{result_file}ï¼Œé”™è¯¯ï¼š{e}")
            return None
    else:
        logger.warning(f"âš ï¸ åˆ†æç»“æœ {analysis_id} ä¸å­˜åœ¨")
        return None

# ========== æ ¸å¿ƒä¿®å¤ï¼šä¼˜åŒ– dashboard è·¯ç”± ==========
@app.route('/dashboard/<analysis_id>')
@login_required  # æ–°å¢ï¼šç™»å½•éªŒè¯
def dashboard(analysis_id=None):
    """ä»ªè¡¨ç›˜ï¼šå±•ç¤ºä¸‰å¤§åˆ†æç»“æœçš„æ¦‚è§ˆå’Œè¯¦æƒ…ï¼Œå…¼å®¹æ— å‚æ•°è®¿é—®"""
    analysis_id = analysis_id or request.args.get('analysis_id', 'latest')
    upload_success = request.args.get('upload_success', '0')
    analysis_data = None

    if analysis_id == 'latest':
        if analysis_results:
            latest_id = sorted(analysis_results.keys())[-1]
            analysis_data = load_analysis_result(latest_id)
            analysis_id = latest_id
        else:
            flash('âš ï¸ æš‚æ— åˆ†æç»“æœï¼Œè¯·å…ˆä¸Šä¼ æ–‡ä»¶è¿›è¡Œåˆ†æ', 'info')
            return redirect(url_for('index'))
    else:
        analysis_data = load_analysis_result(analysis_id)
        if not analysis_data:
            flash(f"âŒ åˆ†æç»“æœ {analysis_id} ä¸å­˜åœ¨", 'error')
            return redirect(url_for('index'))

    all_groups = sorted(list(set([v for v in NAME_TO_GROUP_MAPPING.values() if v != 'otherç»„' and v is not None])))
    return render_template('dashboard.html',
                          analysis_id=analysis_id,
                          analysis_data=analysis_data,
                          upload_success=upload_success,
                          all_groups=all_groups)

# ------------------------------ æ ¸å¿ƒè·¯ç”± ------------------------------
@app.route('/file-upload', methods=['GET', 'POST'])
@login_required  # æ–°å¢ï¼šç™»å½•éªŒè¯
def index():
    # æ–°å¢ï¼šç™»å½•éªŒè¯
    if not session.get('user_id'):
        return redirect(url_for('auth.login', next=url_for('index')))

    """é¦–é¡µï¼šæ–‡ä»¶ä¸Šä¼  + åˆ†æå‚æ•°é…ç½®ï¼Œæ ¸å¿ƒé€»è¾‘æ— æ”¹åŠ¨ï¼Œä»…ä¼˜åŒ–å¼‚å¸¸å¤„ç†"""
    if request.method == 'POST':
        try:
            g.uploaded_files = set()
            # è·å–è¡¨å•å‚æ•°
            category = request.form.get('category', 'é»˜è®¤ç±»ç›®').strip()
            selected_groups = request.form.getlist('selected_groups[]')
            use_original_state = request.form.get('use_original_state', 'false') == 'true'
            cpm_good = float(request.form.get('cpm_good', 50.0))
            cpm_medium = float(request.form.get('cpm_medium', 100.0))
            cpe_good = float(request.form.get('cpe_good', 5.0))
            cpe_medium = float(request.form.get('cpe_medium', 10.0))

            # è·å–ä¸Šä¼ æ–‡ä»¶
            workload_files = request.files.getlist('workload_files[]')
            quality_files = request.files.getlist('quality_files[]')
            cost_files = request.files.getlist('cost_files[]')

            # éªŒè¯æ˜¯å¦æœ‰æœ‰æ•ˆæ–‡ä»¶
            has_valid_file = any([file and file.filename.strip() for file_list in [workload_files, quality_files, cost_files] for file in file_list])
            if not has_valid_file:
                flash('âš ï¸ è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªéç©ºçš„Excel/CSVæ–‡ä»¶', 'warning')
                return redirect(url_for('index'))

            # ä¿å­˜æ–‡ä»¶
            workload_file_paths = [save_file_with_duplicate_check(f, True) for f in workload_files if f and f.filename.strip()]
            workload_file_paths = [p for p in workload_file_paths if p]

            quality_file_paths = [save_file_with_duplicate_check(f, True) for f in quality_files if f and f.filename.strip()]
            quality_file_paths = [p for p in quality_file_paths if p]

            cost_file_paths = [save_file_with_duplicate_check(f, True) for f in cost_files if f and f.filename.strip()]
            cost_file_paths = [p for p in cost_file_paths if p]

            # ========== å·¥ä½œé‡åˆ†æ - å…¼å®¹çœŸå®DataProcessorçš„ä¸¤ç§è¿”å›æ ¼å¼ âœ…æ ¸å¿ƒä¿®å¤ ==========
            workload_df = pd.DataFrame()
            workload_result = {"result": pd.DataFrame(), "summary": {}, "group_summary": pd.DataFrame(), "top_media_ranking": pd.DataFrame()}
            if workload_file_paths:
                process_result = data_processor.process_for_media_analysis(workload_file_paths, category)
                # å…¼å®¹ï¼šçœŸå®æ¨¡å—è¿”å›å­—å…¸/ç›´æ¥è¿”å›df ä¸¤ç§æ ¼å¼
                if isinstance(process_result, dict):
                    workload_df = process_result.get('processed_data', pd.DataFrame())
                elif isinstance(process_result, pd.DataFrame):
                    workload_df = process_result
                else:
                    workload_df = pd.DataFrame()

                if not workload_df.empty:
                    workload_analyzer = WorkloadAnalyzer(
                        df=workload_df,
                        known_id_name_mapping=ID_TO_NAME_MAPPING,
                        config={"FLOWER_TO_NAME_MAPPING": {}}
                    )
                    workload_analysis = workload_analyzer.analyze(top_n=10)

                    # âœ… æ ¸å¿ƒä¿®å¤ï¼šç»Ÿä¸€é”®åï¼Œç¡®ä¿å‰ç«¯æ¨¡æ¿ä½¿ç”¨'result'è€Œä¸æ˜¯'detail'
                    workload_result = {
                        "result": workload_analysis.get('detail', pd.DataFrame()),
                        "summary": workload_analysis.get('summary', {}),
                        "group_summary": workload_analysis.get('group_summary', pd.DataFrame()),
                        "top_media_ranking": workload_analysis.get('top_media_ranking', pd.DataFrame())
                    }

                    logger.info(f"ğŸ“Š å·¥ä½œé‡åˆ†æå®Œæˆï¼Œæ˜ç»†æ•°æ®è¡Œæ•°: {len(workload_result['result'])}")

            # ========== å·¥ä½œè´¨é‡åˆ†æ âœ… æ ¸å¿ƒBUGä¿®å¤+å…¼å®¹çœŸå®æ¨¡å—è¿”å›æ ¼å¼ ==========
            # ========== å·¥ä½œè´¨é‡åˆ†æ âœ… æ ¸å¿ƒBUGä¿®å¤+å…¼å®¹çœŸå®æ¨¡å—è¿”å›æ ¼å¼ ==========
            quality_df = pd.DataFrame()
            quality_result = {"result": pd.DataFrame(), "summary": {}, "group_summary": pd.DataFrame(),
                              "quality_distribution": pd.DataFrame(),
                              "premium_detail": pd.DataFrame(),  # âœ… æ–°å¢ï¼šä¼˜è´¨è¾¾äººæ•°æ®
                              "high_read_detail": pd.DataFrame()}  # âœ… æ–°å¢ï¼šé«˜é˜…è¯»è¾¾äººæ•°æ®
            if quality_file_paths:
                process_result = data_processor.process_for_media_analysis(quality_file_paths, category)
                # å…¼å®¹ï¼šçœŸå®æ¨¡å—è¿”å›å­—å…¸/ç›´æ¥è¿”å›df ä¸¤ç§æ ¼å¼
                if isinstance(process_result, dict):
                    quality_df = process_result.get('processed_data', pd.DataFrame())
                elif isinstance(process_result, pd.DataFrame):
                    quality_df = process_result
                else:
                    quality_df = pd.DataFrame()

                if not quality_df.empty:
                    quality_analyzer = QualityAnalyzer(
                        df=quality_df,
                        known_id_name_mapping=ID_TO_NAME_MAPPING,
                        config={"FLOWER_TO_NAME_MAPPING": {}}
                    )
                    # âœ… ä¿®æ”¹ï¼šç§»é™¤sort_by_qualityå‚æ•°ï¼Œå§‹ç»ˆæŒ‰å°ç»„æ’åº
                    quality_analysis = quality_analyzer.analyze(use_original_state=use_original_state)

                    # âœ… æ ¸å¿ƒä¿®å¤ï¼šç»Ÿä¸€é”®åï¼Œç¡®ä¿å‰ç«¯æ¨¡æ¿ä½¿ç”¨'result'è€Œä¸æ˜¯'detail'ï¼Œå¹¶åŒ…å«åˆ†ç±»æ•°æ®
                    quality_result = {
                        "result": quality_analysis.get('detail', pd.DataFrame()),
                        "summary": quality_analysis.get('summary', {}),
                        "group_summary": quality_analysis.get('group_summary', pd.DataFrame()),
                        "quality_distribution": quality_analysis.get('quality_distribution', pd.DataFrame()),
                        "premium_detail": quality_analysis.get('premium_detail', pd.DataFrame()),  # âœ… æ–°å¢
                        "high_read_detail": quality_analysis.get('high_read_detail', pd.DataFrame())  # âœ… æ–°å¢
                    }

                    logger.info(f"ğŸ“Š è´¨é‡åˆ†æå®Œæˆï¼Œæ˜ç»†æ•°æ®è¡Œæ•°: {len(quality_result['result'])}")

            # ========== æˆæœ¬åˆ†æ âœ… å…¼å®¹çœŸå®æ¨¡å—è¿”å›æ ¼å¼+ä½¿ç”¨æ‰€æœ‰æ•°æ® ==========
            cost_result = {
                "result": pd.DataFrame(), "cleaned_data": pd.DataFrame(), "filtered_data": pd.DataFrame(),
                "summary": {},
                "overall_summary": {}, "detail_data": pd.DataFrame(),
                "media_detail": pd.DataFrame(), "group_summary": pd.DataFrame(),
                "filtered_summary": {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0},
                "cost_efficiency_ranking": pd.DataFrame(),
                # æ–°å¢ï¼šæ— æ•ˆæ•°æ®ç›¸å…³å­—æ®µ
                "invalid_data_detail": [],
                "invalid_data_stats": {}
            }
            if cost_file_paths:
                process_result = data_processor.process_for_cost_analysis(cost_file_paths, category)
                # å…¼å®¹ï¼šçœŸå®æ¨¡å—è¿”å›å­—å…¸/ç›´æ¥è¿”å›df ä¸¤ç§æ ¼å¼
                if isinstance(process_result, dict):
                    cost_raw_df = process_result.get('processed_data', pd.DataFrame())
                    cost_filtered_df = process_result.get('filtered_data', pd.DataFrame())
                elif isinstance(process_result, pd.DataFrame):
                    cost_raw_df = process_result
                    cost_filtered_df = pd.DataFrame()  # ä¸è¿”å›è¢«ç­›é™¤æ•°æ®
                else:
                    cost_raw_df = pd.DataFrame()
                    cost_filtered_df = pd.DataFrame()

                if not cost_raw_df.empty:
                    cost_analyzer = CostAnalyzer(cost_raw_df, cost_filtered_df)
                    try:
                        cost_analysis = cost_analyzer.analyze(top_n=10)
                    except AttributeError as e:
                        logger.warning(f"æˆæœ¬åˆ†æå‡ºç°å±æ€§é”™è¯¯ï¼Œè¿›è¡Œä¿®å¤: {e}")
                        # åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„æˆæœ¬åˆ†æç»“æœ
                        cost_analysis = {
                            'overall_summary': {'æ€»æ•°æ®æ¡æ•°': len(cost_raw_df)},
                            'media_detail': cost_raw_df,
                            'group_summary': pd.DataFrame(),
                            'filtered_summary': {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0},
                            'cost_efficiency_ranking': pd.DataFrame(),
                            'invalid_data_detail': [],
                            'media_group_workload': pd.DataFrame(),
                            'fixed_media_workload': pd.DataFrame(),
                            'fixed_media_cost': pd.DataFrame(),
                            'fixed_media_rebate': pd.DataFrame(),
                            'fixed_media_performance': pd.DataFrame(),
                            'fixed_media_level': pd.DataFrame(),
                            'fixed_media_comprehensive': pd.DataFrame(),
                            'detailed_data': cost_raw_df
                        }

                    # âœ… æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼ŒåŒ…æ‹¬æ— æ•ˆæ•°æ®
                    cost_summary = cost_analysis.get('overall_summary', cost_analysis.get('summary', {}))
                    cost_media_detail = cost_analysis.get('media_detail',
                                                          cost_analysis.get('detail_df', pd.DataFrame()))

                    # è·å–æ— æ•ˆæ•°æ®è¯¦æƒ…
                    invalid_data_detail = cost_analysis.get('invalid_data_detail', [])
                    invalid_data_stats = {
                        'æ€»æ•°æ®æ¡æ•°': len(cost_raw_df),
                        'æœ‰æ•ˆæ•°æ®æ¡æ•°': len(cost_raw_df) - (
                            cost_raw_df['æˆæœ¬æ— æ•ˆ'].sum() if 'æˆæœ¬æ— æ•ˆ' in cost_raw_df.columns else 0),
                        'æ— æ•ˆæ•°æ®æ¡æ•°': cost_raw_df['æˆæœ¬æ— æ•ˆ'].sum() if 'æˆæœ¬æ— æ•ˆ' in cost_raw_df.columns else 0,
                        'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)': f"{(len(cost_raw_df) - (cost_raw_df['æˆæœ¬æ— æ•ˆ'].sum() if 'æˆæœ¬æ— æ•ˆ' in cost_raw_df.columns else 0)) / len(cost_raw_df) * 100:.2f}%" if len(
                            cost_raw_df) > 0 else '0%',
                        'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)': f"{(cost_raw_df['æˆæœ¬æ— æ•ˆ'].sum() if 'æˆæœ¬æ— æ•ˆ' in cost_raw_df.columns else 0) / len(cost_raw_df) * 100:.2f}%" if len(
                            cost_raw_df) > 0 else '0%',
                        'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ': {},
                        'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)': cost_raw_df.loc[
                            cost_raw_df['æˆæœ¬æ— æ•ˆ'] == True, 'æˆæœ¬'].sum() if 'æˆæœ¬æ— æ•ˆ' in cost_raw_df.columns else 0
                    }

                    cost_result = {
                        "result": cost_media_detail,
                        "cleaned_data": cost_raw_df,
                        "filtered_data": cost_filtered_df,
                        "summary": cost_summary,
                        "overall_summary": cost_summary,
                        "detail_data": cost_media_detail,
                        "media_detail": cost_media_detail,
                        "group_summary": cost_analysis.get('group_summary', pd.DataFrame()),
                        "filtered_summary": cost_analysis.get('filtered_summary', {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0}),
                        "cost_efficiency_ranking": cost_analysis.get('cost_efficiency_ranking', pd.DataFrame()),
                        # âœ… æ–°å¢ï¼šæ— æ•ˆæ•°æ®ç›¸å…³
                        "invalid_data_detail": invalid_data_detail,
                        "invalid_data_stats": invalid_data_stats,
                        # âœ… æ–°å¢ï¼šæˆæœ¬å‘æŒ¥åˆ†æçš„æ‰€æœ‰å·¥ä½œè¡¨æ•°æ®
                        "media_group_workload": cost_analysis.get('media_group_workload', pd.DataFrame()),
                        "fixed_media_workload": cost_analysis.get('fixed_media_workload', pd.DataFrame()),
                        "fixed_media_cost": cost_analysis.get('fixed_media_cost', pd.DataFrame()),
                        "fixed_media_rebate": cost_analysis.get('fixed_media_rebate', pd.DataFrame()),
                        "fixed_media_performance": cost_analysis.get('fixed_media_performance', pd.DataFrame()),
                        "fixed_media_level": cost_analysis.get('fixed_media_level', pd.DataFrame()),
                        "fixed_media_comprehensive": cost_analysis.get('fixed_media_comprehensive', pd.DataFrame()),
                        "detailed_data": cost_analysis.get('detailed_data', pd.DataFrame())
                    }

                    logger.info(
                        f"ğŸ“Š æˆæœ¬åˆ†æå®Œæˆï¼Œæ€»æ•°æ®: {len(cost_raw_df)} æ¡, æ— æ•ˆæ•°æ®: {invalid_data_stats['æ— æ•ˆæ•°æ®æ¡æ•°']} æ¡")

            # ========== åœ¨ç”ŸæˆæŠ¥å‘Šä¹‹å‰å…ˆåˆ›å»ºanalysis_id ==========
            analysis_id = datetime.now().strftime('%Y%m%d%H%M%S')
            # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨å®ä¾‹ - ä¼ å…¥çœŸå®åˆ†æç»“æœ
            report_generator = ReportGenerator(
                analysis_results={
                    'workload': workload_result,
                    'quality': quality_result,
                    'cost': cost_result
                },
                output_dir=app.config['OUTPUT_DIR']
            )

            # ç”ŸæˆæŠ¥å‘Š
            reports = {
                "workload": {"excel": ""},
                "quality": {"excel": ""},
                "cost": {"excel": ""},
                "full": {"full_excel": ""}
            }

            # ç”ŸæˆExcelæŠ¥å‘Šï¼ˆæ ¹æ®åˆ†ææ¨¡å¼ï¼‰
            analysis_mode = 'full'
            # âœ… ä¿®å¤ï¼šç§»é™¤analysis_idå‚æ•°
            excel_report_path = report_generator.generate_excel_report(analysis_mode)
            if excel_report_path:
                reports["full"]["full_excel"] = excel_report_path

            # âœ… æ ¸å¿ƒä¿®å¤ï¼šè½¬æ¢æ•°æ®æ ¼å¼å¹¶ç»Ÿä¸€é”®å
            workload_for_storage = {
                "result": convert_pandas_types_to_python(workload_result.get("result", [])),
                "summary": convert_pandas_types_to_python(workload_result.get("summary", {})),
                "group_summary": convert_pandas_types_to_python(workload_result.get("group_summary", [])),
                "top_media_ranking": convert_pandas_types_to_python(workload_result.get("top_media_ranking", []))
            }

            quality_for_storage = {
                "result": convert_pandas_types_to_python(quality_result.get("result", [])),
                "summary": convert_pandas_types_to_python(quality_result.get("summary", {})),
                "group_summary": convert_pandas_types_to_python(quality_result.get("group_summary", [])),
                "quality_distribution": convert_pandas_types_to_python(quality_result.get("quality_distribution", [])),
                "premium_detail": convert_pandas_types_to_python(quality_result.get("premium_detail", [])),
                "high_read_detail": convert_pandas_types_to_python(quality_result.get("high_read_detail", []))
            }

            # åœ¨ index å‡½æ•°ä¸­ï¼Œæ‰¾åˆ°å­˜å‚¨æˆæœ¬åˆ†æç»“æœçš„éƒ¨åˆ†ï¼Œä¿®æ”¹ä¸ºï¼š
            # åœ¨å­˜å‚¨æˆæœ¬åˆ†æç»“æœçš„åœ°æ–¹ï¼Œç¡®ä¿åŒ…å« invalid_data_detail
            cost_for_storage = {
                "result": convert_pandas_types_to_python(cost_result.get("result", [])),
                "summary": convert_pandas_types_to_python(cost_result.get("summary", {})),
                "overall_summary": convert_pandas_types_to_python(cost_result.get("overall_summary", {})),
                "media_detail": convert_pandas_types_to_python(cost_result.get("media_detail", [])),
                "group_summary": convert_pandas_types_to_python(cost_result.get("group_summary", [])),
                "filtered_summary": convert_pandas_types_to_python(cost_result.get("filtered_summary", {})),
                "cost_efficiency_ranking": convert_pandas_types_to_python(
                    cost_result.get("cost_efficiency_ranking", [])),
                # âœ… æ–°å¢ï¼šæ— æ•ˆæ•°æ®è¯¦æƒ…
                "invalid_data_detail": convert_pandas_types_to_python(cost_result.get("invalid_data_detail", [])),
                "invalid_data_stats": convert_pandas_types_to_python(cost_result.get("invalid_data_stats", {})),
                # âœ… æ–°å¢ï¼šæˆæœ¬å‘æŒ¥åˆ†ææ‰€æœ‰å·¥ä½œè¡¨
                "media_group_workload": convert_pandas_types_to_python(cost_result.get("media_group_workload", [])),
                "fixed_media_workload": convert_pandas_types_to_python(cost_result.get("fixed_media_workload", [])),
                "fixed_media_cost": convert_pandas_types_to_python(cost_result.get("fixed_media_cost", [])),
                "fixed_media_rebate": convert_pandas_types_to_python(cost_result.get("fixed_media_rebate", [])),
                "fixed_media_performance": convert_pandas_types_to_python(
                    cost_result.get("fixed_media_performance", [])),
                "fixed_media_level": convert_pandas_types_to_python(cost_result.get("fixed_media_level", [])),
                "fixed_media_comprehensive": convert_pandas_types_to_python(
                    cost_result.get("fixed_media_comprehensive", [])),
                "detailed_data": convert_pandas_types_to_python(cost_result.get("detailed_data", []))
            }

            analysis_data_full = {
                "analysis_id": analysis_id,
                "full_result": {
                    "workload": workload_for_storage,
                    "quality": quality_for_storage,
                    "cost": cost_for_storage
                },
                "reports": reports,
                "category": category,
                "selected_groups": selected_groups,
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            # âœ… æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿æ•°æ®å­˜å‚¨åˆ°å†…å­˜
            analysis_results[analysis_id] = analysis_data_full

            # æŒä¹…åŒ–åˆ°JSON
            analysis_data_serializable = convert_pandas_types_to_python({
                "analysis_id": analysis_id,
                "category": category,
                "selected_groups": selected_groups,
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "workload": workload_for_storage,
                "quality": quality_for_storage,
                "cost": {  # å…³é”®ä¿®å¤ï¼šæ”¹ä¸ºå®Œæ•´çš„ cost ç»“æ„
                    "result": cost_for_storage.get("result", []),
                    "summary": cost_for_storage.get("summary", {}),
                    "overall_summary": cost_for_storage.get("overall_summary", {}),
                    "media_detail": cost_for_storage.get("media_detail", []),
                    "group_summary": cost_for_storage.get("group_summary", []),
                    "filtered_summary": cost_for_storage.get("filtered_summary", {}),
                    "cost_efficiency_ranking": cost_for_storage.get("cost_efficiency_ranking", []),
                    # âœ… æ–°å¢ï¼šæˆæœ¬å‘æŒ¥åˆ†æçš„æ‰€æœ‰å·¥ä½œè¡¨æ•°æ®
                    "media_group_workload": convert_pandas_types_to_python(
                        cost_result.get("media_group_workload", pd.DataFrame())),
                    "fixed_media_workload": convert_pandas_types_to_python(
                        cost_result.get("fixed_media_workload", pd.DataFrame())),
                    "fixed_media_cost": convert_pandas_types_to_python(
                        cost_result.get("fixed_media_cost", pd.DataFrame())),
                    "fixed_media_rebate": convert_pandas_types_to_python(
                        cost_result.get("fixed_media_rebate", pd.DataFrame())),
                    "fixed_media_performance": convert_pandas_types_to_python(
                        cost_result.get("fixed_media_performance", pd.DataFrame())),
                    "fixed_media_level": convert_pandas_types_to_python(
                        cost_result.get("fixed_media_level", pd.DataFrame())),
                    "fixed_media_comprehensive": convert_pandas_types_to_python(
                        cost_result.get("fixed_media_comprehensive", pd.DataFrame())),
                    "detailed_data": convert_pandas_types_to_python(cost_result.get("detailed_data", pd.DataFrame()))
                },
                "report_files": {
                    "workload": reports["workload"]["excel"],
                    "quality": reports["quality"]["excel"],
                    "cost": reports["cost"]["excel"],
                    "full": reports["full"]["full_excel"]
                }
            })

            result_file_path = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results', f'{analysis_id}.json')
            os.makedirs(os.path.dirname(result_file_path), exist_ok=True)

            with open(result_file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_data_serializable, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… åˆ†æå®Œæˆï¼Œåˆ†æIDï¼š{analysis_id}")
            flash('âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œåˆ†æå·²å®Œæˆï¼', 'success')
            return redirect(url_for('dashboard', analysis_id=analysis_id, upload_success=1))

        except Exception as e:
            error_msg = f"âŒ åˆ†æå¤±è´¥ï¼š{str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            flash(error_msg, 'error')
            return redirect(url_for('index'))

    # GETè¯·æ±‚æ¸²æŸ“é¦–é¡µ
    all_groups = sorted(list(set([v for v in NAME_TO_GROUP_MAPPING.values() if v != 'otherç»„' and v is not None])))
    return render_template('index.html', all_groups=all_groups)

# ========== æ–°å¢æµ‹è¯•è·¯ç”± ==========
@app.route('/test_data/<analysis_id>')
@login_required  # æ–°å¢ï¼šç™»å½•éªŒè¯
def test_data(analysis_id):
    """æµ‹è¯•è·¯ç”±ï¼šæŸ¥çœ‹å†…å­˜ä¸­çš„æ•°æ®"""
    if analysis_id in analysis_results:
        data = analysis_results[analysis_id]
        # è¿”å›JSONæ ¼å¼çš„æ•°æ®ä»¥ä¾¿æ£€æŸ¥
        return jsonify({
            'success': True,
            'analysis_id': analysis_id,
            'keys': list(data.keys()),
            'full_result_keys': list(data.get('full_result', {}).keys()) if data.get('full_result') else [],
            'workload_summary': data.get('full_result', {}).get('workload', {}).get('summary', {}),
            'timestamp': data.get('timestamp', ''),
            'category': data.get('category', '')
        })
    else:
        # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
        result_file = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results', f'{analysis_id}.json')
        if os.path.exists(result_file):
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return jsonify({
                'success': True,
                'source': 'file',
                'data_keys': list(data.keys())
            })
        return jsonify({'success': False, 'error': 'æ•°æ®ä¸å­˜åœ¨'})

# ------------------------------ å„åˆ†ææŠ¥å‘Šè¯¦æƒ…é¡µ ------------------------------
@app.route('/report/workload/<analysis_id>')
@login_required  # æ–°å¢ï¼šç™»å½•éªŒè¯
def workload_report(analysis_id):
    """å·¥ä½œé‡åˆ†ææŠ¥å‘Šè¯¦æƒ…é¡µï¼Œä¿®å¤å˜é‡ååŒ¹é…é—®é¢˜"""
    if analysis_id == 'latest':
        results_dir = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results')
        try:
            result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
            if not result_files:
                return render_template('workload_analysis.html',
                                       analysis_id='latest',
                                       analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                      "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                       detail_data=[],
                                       workload_summary={},
                                       group_summary=[],
                                       top_ranking=[],
                                       report={"excel": ""})
            result_files.sort(reverse=True)
            latest_file = result_files[0]
            analysis_id = latest_file.replace('.json', '')
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°åˆ†æç»“æœå¤±è´¥: {e}")
            return render_template('workload_analysis.html',
                                   analysis_id='latest',
                                   analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                  "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                   detail_data=[],
                                   workload_summary={},
                                   group_summary=[],
                                   top_ranking=[],
                                   report={"excel": ""})

    # åŠ è½½åˆ†æç»“æœ
    analysis_data = load_analysis_result(analysis_id)
    if not analysis_data:
        category = "æš‚æ— ç±»ç›®"
        detail_data = []
        workload_summary = {}
        group_summary = []
        top_ranking = []
        report = {"excel": ""}
        analysis_data_info = {"category": category,
                              "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    else:
        # ä»full_result.workloadä¸­è·å–æ•°æ®
        full_result = analysis_data.get("full_result", {})
        workload_data = full_result.get("workload", {})
        category = analysis_data.get("category", "æš‚æ— ç±»ç›®")

        # è·å–æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ¿æœŸæœ›çš„å˜é‡å
        detail_data = workload_data.get("result", [])
        workload_summary = workload_data.get("summary", {})
        group_summary = workload_data.get("group_summary", [])
        top_ranking = workload_data.get("top_media_ranking", [])

        # å¦‚æœresultä¸ºç©ºä½†æœ‰detailï¼Œå°è¯•ä»detailè·å–
        if not detail_data and "detail" in workload_data:
            detail_data = workload_data.get("detail", [])

        # ç¡®ä¿top_rankingæœ‰æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰å•ç‹¬çš„top_rankingï¼Œä½¿ç”¨detail_dataå‰10æ¡ï¼‰
        if not top_ranking and detail_data:
            # æŒ‰ç»¼åˆè¯„ä¼°æ’åºåå–å‰10
            try:
                if detail_data and len(detail_data) > 0:
                    # å°è¯•æŒ‰ç»¼åˆè¯„ä¼°æ’åº
                    sorted_data = sorted(detail_data, key=lambda x: x.get('ç»¼åˆè¯„ä¼°', '') if isinstance(x, dict) else '')
                    top_ranking = sorted_data[:10]
                else:
                    top_ranking = detail_data[:10] if len(detail_data) > 10 else detail_data
            except:
                top_ranking = detail_data[:10] if len(detail_data) > 10 else detail_data

        report = analysis_data.get("reports", {}).get("workload", {"excel": ""})
        analysis_data_info = {
            "category": category,
            "timestamp": analysis_data.get("timestamp", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        }

        # ä¿®å¤ï¼šç¡®ä¿workload_summaryåŒ…å«å¿…è¦çš„å­—æ®µ
        if not workload_summary:
            workload_summary = {
                'æ€»å®šæ¡£é‡': 0,
                'æ€»CHAIN_RETURNEDæ•°': 0,
                'æ•´ä½“å®šæ¡£ç‡': '0%',
                'æ€»å¤„ç†é‡': 0,
                'åª’ä»‹æ€»æ•°': len(detail_data) if detail_data else 0
            }
        else:
            # ç¡®ä¿æœ‰å¿…è¦çš„å­—æ®µ
            workload_summary['æ€»å®šæ¡£é‡'] = workload_summary.get('æ€»å®šæ¡£é‡', 0) or 0
            workload_summary['æ€»CHAIN_RETURNEDæ•°'] = workload_summary.get('æ€»CHAIN_RETURNEDæ•°', 0) or 0
            workload_summary['æ•´ä½“å®šæ¡£ç‡'] = workload_summary.get('æ•´ä½“å®šæ¡£ç‡', '0%') or '0%'
            workload_summary['æ€»å¤„ç†é‡'] = workload_summary.get('æ€»å¤„ç†é‡', 0) or 0
            workload_summary['åª’ä»‹æ€»æ•°'] = len(detail_data) if detail_data else 0

    # å…³é”®ä¿®å¤ï¼šä¼ é€’æ­£ç¡®çš„å˜é‡åç»™æ¨¡æ¿
    return render_template('workload_analysis.html',
                           analysis_id=analysis_id,
                           analysis_data=analysis_data_info,
                           detail_data=detail_data,
                           workload_summary=workload_summary,
                           group_summary=group_summary,
                           top_ranking=top_ranking,
                           report=report)


@app.route('/')
def root_redirect():
    """æ ¹è·¯å¾„é‡å®šå‘åˆ°æ•°æ®æ¥æºé€‰æ‹©é¡µ"""
    return redirect(url_for('data_source_selector'))

# app_auto.py æ–°å¢è·¯ç”±
@app.route('/data-source')
@login_required
def data_source_selector():
    """æ•°æ®æ¥æºé€‰æ‹©é¡µï¼ˆç™»å½•åé»˜è®¤é¦–é¡µï¼‰"""
    return render_template('data_source_selector.html')


@app.route('/db-analysis')
@login_required
def db_analysis_index():
    """æ•°æ®åº“åˆ†æé…ç½®é¡µ"""
    return render_template('db_analysis_index.html')

@app.route('/db-analysis/submit', methods=['POST'])
@login_required
def db_analysis_submit():
    """æ•°æ®åº“åˆ†ææäº¤å¤„ç†ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰"""
    try:
        # è·å–è¡¨å•å‚æ•°
        start_date = request.form.get('start_date').strip()
        end_date = request.form.get('end_date').strip()
        analysis_modules = request.form.getlist('analysis_modules')
        use_original_state = request.form.get('use_original_state', 'false') == 'true'
        cpm_good = float(request.form.get('cpm_good', 50.0))
        cpm_medium = float(request.form.get('cpm_medium', 100.0))
        cpe_good = float(request.form.get('cpe_good', 5.0))
        cpe_medium = float(request.form.get('cpe_medium', 10.0))
        category = f"æ•°æ®åº“åˆ†æ_{start_date}_{end_date}"

        # éªŒè¯å‚æ•°
        if not start_date or not end_date:
            flash('âš ï¸ è¯·é€‰æ‹©å®Œæ•´çš„æ—¶é—´æ®µ', 'warning')
            return redirect(url_for('db_analysis_index'))

        # ========== æ•°æ®åº“è¯»å–æ•°æ® ==========
        workload_df = pd.DataFrame()
        quality_df = pd.DataFrame()
        cost_df = pd.DataFrame()

        if 'workload' in analysis_modules:
            workload_df = query_workload_data(start_date, end_date)
            if not workload_df.empty:
                logger.info(f"ğŸ“Š æŸ¥è¯¢åˆ°å·¥ä½œé‡æ•°æ® {len(workload_df)} æ¡")

        if 'quality' in analysis_modules:
            quality_df = query_quality_data(start_date, end_date)
            if not quality_df.empty:
                logger.info(f"ğŸ“ˆ æŸ¥è¯¢åˆ°å·¥ä½œè´¨é‡æ•°æ® {len(quality_df)} æ¡")

        if 'cost' in analysis_modules:
            cost_df = query_cost_data(start_date, end_date)
            if not cost_df.empty:
                logger.info(f"ğŸ’° æŸ¥è¯¢åˆ°æˆæœ¬æ•°æ® {len(cost_df)} æ¡")
                # âœ… æ–°å¢ï¼šæ£€æŸ¥æˆæœ¬æ•°æ®å­—æ®µ
                logger.info(f"âœ… æˆæœ¬æ•°æ®åˆ—å: {list(cost_df.columns)}")
                logger.info(f"âœ… æˆæœ¬æ•°æ®æ ·æœ¬ï¼ˆå‰3è¡Œï¼‰:")
                for i in range(min(3, len(cost_df))):
                    logger.info(f"è¡Œ{i}: {cost_df.iloc[i].to_dict()}")

        # éªŒè¯æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        has_valid_data = not (workload_df.empty and quality_df.empty and cost_df.empty)
        if not has_valid_data:
            flash('âš ï¸ æ‰€é€‰æ—¶é—´æ®µå†…æ— æœ‰æ•ˆæ•°æ®ï¼Œè¯·è°ƒæ•´æ—¥æœŸèŒƒå›´', 'warning')
            return redirect(url_for('db_analysis_index'))

        # ========== æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç† ==========
        def clean_dataframe(df):
            """æ•°æ®æ¸…æ´—å‡½æ•°"""
            if df.empty:
                return df

            df_copy = df.copy()

            # 1. æ¸…ç†å­—ç¬¦ä¸²å­—æ®µç©ºæ ¼
            str_cols = df_copy.select_dtypes(include=['object']).columns
            for col in str_cols:
                df_copy[col] = df_copy[col].astype(str).str.strip()

            # 2. å¤„ç†å°ç»„åç§°ä¸­çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
            if 'æ‰€å±å°ç»„' in df_copy.columns:
                df_copy['æ‰€å±å°ç»„'] = df_copy['æ‰€å±å°ç»„'].str.replace(r'\s+', '', regex=True)
                # ç»Ÿä¸€å°ç»„åç§°
                group_mapping = {
                    'å®¶å±…åª’ä»‹ç»„': 'å®¶å±…åª’ä»‹ç»„',
                    ' å®¶å±…åª’ä»‹ç»„': 'å®¶å±…åª’ä»‹ç»„',
                    'å®¶å±…åª’ä»‹ç»„ ': 'å®¶å±…åª’ä»‹ç»„',
                    'æ•°ç åª’ä»‹ç»„': 'æ•°ç åª’ä»‹ç»„',
                    ' æ•°ç åª’ä»‹ç»„': 'æ•°ç åª’ä»‹ç»„',
                    'æ•°ç åª’ä»‹ç»„ ': 'æ•°ç åª’ä»‹ç»„',
                    'å¿«æ¶ˆåª’ä»‹ç»„': 'å¿«æ¶ˆåª’ä»‹ç»„',
                    ' å¿«æ¶ˆåª’ä»‹ç»„': 'å¿«æ¶ˆåª’ä»‹ç»„',
                    'å¿«æ¶ˆåª’ä»‹ç»„ ': 'å¿«æ¶ˆåª’ä»‹ç»„',
                    'otherç»„': 'otherç»„',
                    ' otherç»„': 'otherç»„',
                    'otherç»„ ': 'otherç»„',
                    'é»˜è®¤ç»„': 'otherç»„'
                }
                df_copy['æ‰€å±å°ç»„'] = df_copy['æ‰€å±å°ç»„'].replace(group_mapping)

            # 3. æ¸…ç†åª’ä»‹åç§°ç©ºæ ¼
            media_fields = ['å®šæ¡£åª’ä»‹', 'æäº¤åª’ä»‹', 'åª’ä»‹å§“å', 'å¯¹åº”çœŸå', 'åª’ä»‹çœŸå',
                            'schedule_user_name', 'submit_media_user_name']
            for field in media_fields:
                if field in df_copy.columns:
                    df_copy[field] = df_copy[field].astype(str).str.strip()

            # 4. ä¿®å¤Noneå€¼
            df_copy = df_copy.replace({'None': 'æœªçŸ¥', 'none': 'æœªçŸ¥', 'nan': 'æœªçŸ¥', 'NaN': 'æœªçŸ¥'})
            df_copy = df_copy.fillna({
                'å®šæ¡£åª’ä»‹': 'æœªçŸ¥åª’ä»‹',
                'æ‰€å±å°ç»„': 'otherç»„',
                'åª’ä»‹çœŸå': 'æœªçŸ¥',
                'schedule_user_name': 'æœªçŸ¥'
            })

            return df_copy

        # åº”ç”¨æ•°æ®æ¸…æ´—
        if not workload_df.empty:
            workload_df = clean_dataframe(workload_df)
            logger.info(f"å·¥ä½œé‡æ•°æ®æ¸…æ´—å®Œæˆï¼Œè¡Œæ•°: {len(workload_df)}")

        if not quality_df.empty:
            quality_df = clean_dataframe(quality_df)
            logger.info(f"å·¥ä½œè´¨é‡æ•°æ®æ¸…æ´—å®Œæˆï¼Œè¡Œæ•°: {len(quality_df)}")

        if not cost_df.empty:
            cost_df = clean_dataframe(cost_df)
            logger.info(f"æˆæœ¬æ•°æ®æ¸…æ´—å®Œæˆï¼Œè¡Œæ•°: {len(cost_df)}")

        # ========== åº”ç”¨å­—æ®µæ˜ å°„ ==========
        logger.info("å¼€å§‹åº”ç”¨æ•°æ®åº“å­—æ®µæ˜ å°„...")
        if not workload_df.empty:
            # å·¥ä½œé‡æ•°æ® - ç¡®ä¿æ˜¯å®šæ¡£æ•°æ®
            workload_df['æ•°æ®ç±»å‹'] = 'å®šæ¡£'
            logger.info(f"å·¥ä½œé‡æ•°æ®å‡†å¤‡å®Œæˆï¼Œè¡Œæ•°: {len(workload_df)}")
            logger.info(f"å­—æ®µç¤ºä¾‹: {list(workload_df.columns[:10])}")

        if not quality_df.empty:
            # å·¥ä½œè´¨é‡æ•°æ® - ç¡®ä¿æ˜¯ææŠ¥æ•°æ®
            quality_df['æ•°æ®ç±»å‹'] = 'ææŠ¥'
            logger.info(f"å·¥ä½œè´¨é‡æ•°æ®å‡†å¤‡å®Œæˆï¼Œè¡Œæ•°: {len(quality_df)}")

        if not cost_df.empty:
            # æˆæœ¬æ•°æ® - ç¡®ä¿æ˜¯å®šæ¡£æ•°æ®
            cost_df['æ•°æ®ç±»å‹'] = 'å®šæ¡£'
            logger.info(f"æˆæœ¬æ•°æ®å‡†å¤‡å®Œæˆï¼Œè¡Œæ•°: {len(cost_df)}")

            # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿æˆæœ¬æ•°æ®æœ‰æ‰€æœ‰å¿…è¦å­—æ®µ
            required_cost_fields = ['å®šæ¡£åª’ä»‹', 'æ‰€å±å°ç»„', 'å®šæ¡£åª’ä»‹å°ç»„', 'æˆæœ¬', 'æŠ¥ä»·', 'ä¸‹å•ä»·', 'è¿”ç‚¹']
            for field in required_cost_fields:
                if field not in cost_df.columns:
                    logger.warning(f"æˆæœ¬æ•°æ®ç¼ºå°‘å­—æ®µ: {field}ï¼Œæ­£åœ¨åˆ›å»ºé»˜è®¤å€¼")
                    if field == 'å®šæ¡£åª’ä»‹å°ç»„' and 'æ‰€å±å°ç»„' in cost_df.columns:
                        cost_df['å®šæ¡£åª’ä»‹å°ç»„'] = cost_df['æ‰€å±å°ç»„']
                    elif field == 'å®šæ¡£åª’ä»‹':
                        cost_df['å®šæ¡£åª’ä»‹'] = cost_df.get('schedule_user_name', 'æœªçŸ¥åª’ä»‹')
                    elif field in ['æˆæœ¬', 'æŠ¥ä»·', 'ä¸‹å•ä»·', 'è¿”ç‚¹']:
                        cost_df[field] = 0.0
                    else:
                        cost_df[field] = 'æœªçŸ¥'

            # âœ… æ–°å¢ï¼šæ£€æŸ¥å¼‚å¸¸æ•°æ®å­—æ®µ
            if 'æ•°æ®å¼‚å¸¸' not in cost_df.columns:
                logger.warning("æˆæœ¬æ•°æ®ç¼ºå°‘'æ•°æ®å¼‚å¸¸'å­—æ®µï¼Œæ­£åœ¨åˆ›å»º")
                cost_df['æ•°æ®å¼‚å¸¸'] = False

            if 'æ•°æ®å¼‚å¸¸åŸå› ' not in cost_df.columns:
                cost_df['æ•°æ®å¼‚å¸¸åŸå› '] = ''

            if 'ç­›é™¤åŸå› ' not in cost_df.columns:
                cost_df['ç­›é™¤åŸå› '] = ''

            # âœ… æ–°å¢ï¼šè¯†åˆ«å¼‚å¸¸æ•°æ®
            logger.info("å¼€å§‹è¯†åˆ«æˆæœ¬å¼‚å¸¸æ•°æ®...")

            # 1. æŠ¥ä»·å¼‚å¸¸ï¼ˆæŠ¥ä»· < ä¸‹å•ä»·ï¼‰
            if 'æŠ¥ä»·' in cost_df.columns and 'ä¸‹å•ä»·' in cost_df.columns:
                mask_price_abnormal = (cost_df['æŠ¥ä»·'].notna()) & (cost_df['ä¸‹å•ä»·'].notna()) & (
                            cost_df['æŠ¥ä»·'] < cost_df['ä¸‹å•ä»·'])
                if mask_price_abnormal.sum() > 0:
                    cost_df.loc[mask_price_abnormal, 'æ•°æ®å¼‚å¸¸'] = True
                    cost_df.loc[mask_price_abnormal, 'æ•°æ®å¼‚å¸¸åŸå› '] = 'æŠ¥ä»·æˆ–ä¸‹å•ä»·å¼‚å¸¸'
                    cost_df.loc[mask_price_abnormal, 'ç­›é™¤åŸå› '] = 'æŠ¥ä»·æˆ–ä¸‹å•ä»·å¼‚å¸¸'
                    logger.info(f"âœ… è¯†åˆ«åˆ°æŠ¥ä»·å¼‚å¸¸æ•°æ®: {mask_price_abnormal.sum()} æ¡")

            # 2. è¿”ç‚¹æ¯”ä¾‹å¼‚å¸¸
            if 'è¿”ç‚¹æ¯”ä¾‹' in cost_df.columns:
                mask_rebate_abnormal = (cost_df['è¿”ç‚¹æ¯”ä¾‹'].notna()) & (
                            (cost_df['è¿”ç‚¹æ¯”ä¾‹'] > 1.0) | (cost_df['è¿”ç‚¹æ¯”ä¾‹'] < -0.5))
                if mask_rebate_abnormal.sum() > 0:
                    cost_df.loc[mask_rebate_abnormal, 'æ•°æ®å¼‚å¸¸'] = True

                    def format_rebate_reason(ratio):
                        return f"è¿”ç‚¹æ¯”ä¾‹å¼‚å¸¸({ratio * 100:.1f}%)"

                    cost_df.loc[mask_rebate_abnormal, 'æ•°æ®å¼‚å¸¸åŸå› '] = cost_df.loc[
                        mask_rebate_abnormal, 'è¿”ç‚¹æ¯”ä¾‹'].apply(format_rebate_reason)
                    logger.info(f"âœ… è¯†åˆ«åˆ°è¿”ç‚¹æ¯”ä¾‹å¼‚å¸¸æ•°æ®: {mask_rebate_abnormal.sum()} æ¡")

            # 3. æˆæœ¬ä¸º0æˆ–ç¼ºå¤±
            if 'æˆæœ¬' in cost_df.columns:
                mask_zero_cost = (cost_df['æˆæœ¬'] == 0) | (cost_df['æˆæœ¬'].isna())
                if mask_zero_cost.sum() > 0:
                    cost_df.loc[mask_zero_cost, 'æ•°æ®å¼‚å¸¸'] = True
                    cost_df.loc[mask_zero_cost, 'æ•°æ®å¼‚å¸¸åŸå› '] = 'æˆæœ¬ä¸º0æˆ–ç¼ºå¤±'
                    logger.info(f"âœ… è¯†åˆ«åˆ°æˆæœ¬ä¸º0æˆ–ç¼ºå¤±æ•°æ®: {mask_zero_cost.sum()} æ¡")

            # 4. æ•°æ®å¼‚å¸¸æ ‡è®°
            if 'æ•°æ®å¼‚å¸¸' in cost_df.columns:
                abnormal_count = cost_df['æ•°æ®å¼‚å¸¸'].sum()
                logger.info(f"âœ… æ€»å¼‚å¸¸æ•°æ®ç»Ÿè®¡: {abnormal_count} æ¡")
                if abnormal_count > 0:
                    logger.info(
                        f"âœ… å¼‚å¸¸åŸå› åˆ†å¸ƒ: {cost_df[cost_df['æ•°æ®å¼‚å¸¸']]['æ•°æ®å¼‚å¸¸åŸå› '].value_counts().to_dict()}")

        # ========== å·¥ä½œé‡åˆ†æ - ç®€åŒ–ç‰ˆ ==========
        workload_result = {"result": [], "summary": {}, "group_summary": [], "top_media_ranking": []}
        if not workload_df.empty:
            try:
                logger.info(f"ğŸ“Š å¼€å§‹ç®€åŒ–å·¥ä½œé‡åˆ†æï¼Œå…± {len(workload_df)} æ¡æ•°æ®")

                # å°è¯•ä½¿ç”¨åŸæœ‰çš„åˆ†æå™¨
                try:
                    workload_analyzer = WorkloadAnalyzer(
                        df=workload_df,
                        known_id_name_mapping=ID_TO_NAME_MAPPING,
                        config={"FLOWER_TO_NAME_MAPPING": {}}
                    )
                    workload_analysis = workload_analyzer.analyze(top_n=10)

                    workload_result = {
                        "result": convert_pandas_types_to_python(workload_analysis.get('detail', pd.DataFrame())),
                        "summary": convert_pandas_types_to_python(workload_analysis.get('summary', {})),
                        "group_summary": convert_pandas_types_to_python(
                            workload_analysis.get('group_summary', pd.DataFrame())),
                        "top_media_ranking": convert_pandas_types_to_python(
                            workload_analysis.get('top_media_ranking', pd.DataFrame()))
                    }

                    logger.info(f"âœ… å·¥ä½œé‡åˆ†ææˆåŠŸï¼Œæ˜ç»†æ•°æ®è¡Œæ•°: {len(workload_result['result'])}")

                except Exception as e:
                    logger.warning(f"å·¥ä½œé‡æ ‡å‡†åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ: {e}")
                    # ä½¿ç”¨ç®€åŒ–åˆ†æ
                    workload_result = create_simple_workload_analysis(workload_df)

            except Exception as e:
                logger.error(f"å·¥ä½œé‡åˆ†æå¼‚å¸¸: {e}")
                # æœ€ç»ˆå…œåº•ï¼šåˆ›å»ºæœ€åŸºæœ¬çš„ç»“æœ
                workload_result = {
                    "result": [],
                    "summary": {"æ€»æ•°æ®æ¡æ•°": len(workload_df), "å¤‡æ³¨": "åˆ†æè¿‡ç¨‹å‡ºç°å¼‚å¸¸"},
                    "group_summary": [],
                    "top_media_ranking": []
                }

        # ========== å·¥ä½œè´¨é‡åˆ†æ - ç®€åŒ–ç‰ˆ ==========
        quality_result = {"result": [], "summary": {}, "group_summary": [],
                          "quality_distribution": [], "premium_detail": [], "high_read_detail": []}
        if not quality_df.empty:
            try:
                logger.info(f"ğŸ“ˆ å¼€å§‹ç®€åŒ–å·¥ä½œè´¨é‡åˆ†æï¼Œå…± {len(quality_df)} æ¡æ•°æ®")

                # å°è¯•ä½¿ç”¨åŸæœ‰çš„åˆ†æå™¨
                try:
                    quality_analyzer = QualityAnalyzer(
                        df=quality_df,
                        known_id_name_mapping=ID_TO_NAME_MAPPING,
                        config={"FLOWER_TO_NAME_MAPPING": {}}
                    )
                    quality_analysis = quality_analyzer.analyze(use_original_state=use_original_state)

                    quality_result = {
                        "result": convert_pandas_types_to_python(quality_analysis.get('detail', pd.DataFrame())),
                        "summary": convert_pandas_types_to_python(quality_analysis.get('summary', {})),
                        "group_summary": convert_pandas_types_to_python(
                            quality_analysis.get('group_summary', pd.DataFrame())),
                        "quality_distribution": convert_pandas_types_to_python(
                            quality_analysis.get('quality_distribution', pd.DataFrame())),
                        "premium_detail": convert_pandas_types_to_python(
                            quality_analysis.get('premium_detail', pd.DataFrame())),
                        "high_read_detail": convert_pandas_types_to_python(
                            quality_analysis.get('high_read_detail', pd.DataFrame()))
                    }

                    logger.info(f"âœ… å·¥ä½œè´¨é‡åˆ†ææˆåŠŸï¼Œæ˜ç»†æ•°æ®è¡Œæ•°: {len(quality_result['result'])}")

                except Exception as e:
                    logger.warning(f"å·¥ä½œè´¨é‡æ ‡å‡†åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ: {e}")
                    # ä½¿ç”¨ç®€åŒ–åˆ†æ
                    quality_result = create_simple_quality_analysis(quality_df)

            except Exception as e:
                logger.error(f"å·¥ä½œè´¨é‡åˆ†æå¼‚å¸¸: {e}")
                # æœ€ç»ˆå…œåº•
                quality_result = {
                    "result": [],
                    "summary": {"æ€»æ•°æ®æ¡æ•°": len(quality_df), "å¤‡æ³¨": "åˆ†æè¿‡ç¨‹å‡ºç°å¼‚å¸¸"},
                    "group_summary": [],
                    "quality_distribution": [],
                    "premium_detail": [],
                    "high_read_detail": []
                }

        # ========== æˆæœ¬åˆ†æ - å®Œæ•´ä¿®å¤ç‰ˆ ==========
        cost_result = {
            "result": [], "cleaned_data": [], "filtered_data": [],
            "summary": {},
            "overall_summary": {}, "detail_data": [],
            "media_detail": [], "group_summary": [],
            "filtered_summary": {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0},
            "cost_efficiency_ranking": [],
            "invalid_data_detail": [],
            "invalid_data_stats": {},
            "abnormal_data_detail": [],
            "abnormal_data_stats": {},
            "media_group_workload": [],
            "fixed_media_workload": [],
            "fixed_media_cost": [],
            "fixed_media_rebate": [],
            "fixed_media_performance": [],
            "fixed_media_level": [],
            "fixed_media_comprehensive": [],
            "detailed_data": []
        }

        if not cost_df.empty:
            try:
                logger.info(f"ğŸ’° å¼€å§‹å®Œæ•´æˆæœ¬åˆ†æï¼Œå…± {len(cost_df)} æ¡æ•°æ®")

                # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®æœ‰æ‰€æœ‰å¿…è¦å­—æ®µ
                cost_df_copy = cost_df.copy()

                # æ£€æŸ¥æˆæœ¬å­—æ®µæ˜¯å¦å­˜åœ¨
                cost_field_name = None
                for field in ['æˆæœ¬', 'cost_amount']:
                    if field in cost_df_copy.columns:
                        cost_field_name = field
                        break

                if cost_field_name:
                    # ç¡®ä¿æˆæœ¬å­—æ®µæ˜¯æ•°å€¼ç±»å‹
                    cost_df_copy['æˆæœ¬'] = pd.to_numeric(cost_df_copy[cost_field_name], errors='coerce').fillna(0.0)
                    logger.info(
                        f"âœ… æˆæœ¬å­—æ®µå¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæˆæœ¬æ•°æ®: {(cost_df_copy['æˆæœ¬'] > 0).sum()}/{len(cost_df_copy)}")
                else:
                    logger.warning("æœªæ‰¾åˆ°æˆæœ¬å­—æ®µï¼Œåˆ›å»ºé»˜è®¤æˆæœ¬å­—æ®µ")
                    cost_df_copy['æˆæœ¬'] = 0.0

                # æ£€æŸ¥æŠ¥ä»·å­—æ®µ
                quote_field = None
                for field in ['æŠ¥ä»·', 'cooperation_quote']:
                    if field in cost_df_copy.columns:
                        quote_field = field
                        break

                if quote_field:
                    cost_df_copy['æŠ¥ä»·'] = pd.to_numeric(cost_df_copy[quote_field], errors='coerce').fillna(0.0)
                else:
                    cost_df_copy['æŠ¥ä»·'] = 0.0

                # æ£€æŸ¥è¿”ç‚¹å­—æ®µ
                rebate_field = None
                for field in ['è¿”ç‚¹', 'rebate_amount']:
                    if field in cost_df_copy.columns:
                        rebate_field = field
                        break

                if rebate_field:
                    cost_df_copy['è¿”ç‚¹'] = pd.to_numeric(cost_df_copy[rebate_field], errors='coerce').fillna(0.0)
                else:
                    cost_df_copy['è¿”ç‚¹'] = 0.0

                # æ£€æŸ¥ä¸‹å•ä»·å­—æ®µ
                order_field = None
                for field in ['ä¸‹å•ä»·', 'order_amount']:
                    if field in cost_df_copy.columns:
                        order_field = field
                        break

                if order_field:
                    cost_df_copy['ä¸‹å•ä»·'] = pd.to_numeric(cost_df_copy[order_field], errors='coerce').fillna(0.0)
                else:
                    cost_df_copy['ä¸‹å•ä»·'] = 0.0

                # âœ… æ–°å¢ï¼šè®¡ç®—è¿”ç‚¹æ¯”ä¾‹
                if 'è¿”ç‚¹' in cost_df_copy.columns and 'æŠ¥ä»·' in cost_df_copy.columns:
                    cost_df_copy['è¿”ç‚¹æ¯”ä¾‹'] = cost_df_copy.apply(
                        lambda row: row['è¿”ç‚¹'] / row['æŠ¥ä»·'] if row['æŠ¥ä»·'] > 0 else 0.0,
                        axis=1
                    )
                else:
                    cost_df_copy['è¿”ç‚¹æ¯”ä¾‹'] = 0.0

                # âœ… å…³é”®ä¿®å¤ï¼šæ·»åŠ æˆæœ¬æ— æ•ˆæ ‡è®°
                cost_df_copy['æˆæœ¬æ— æ•ˆ'] = (cost_df_copy['æˆæœ¬'] == 0) | (cost_df_copy['æˆæœ¬'].isna())
                invalid_count = cost_df_copy['æˆæœ¬æ— æ•ˆ'].sum()
                logger.info(f"âœ… æˆæœ¬æ— æ•ˆæ•°æ®: {invalid_count} æ¡")

                # âœ… å…³é”®ä¿®å¤ï¼šæ·»åŠ æ•°æ®å¼‚å¸¸æ ‡è®°ï¼ˆå¦‚æœå°šæœªæ ‡è®°ï¼‰
                if 'æ•°æ®å¼‚å¸¸' not in cost_df_copy.columns:
                    cost_df_copy['æ•°æ®å¼‚å¸¸'] = False

                if 'æ•°æ®å¼‚å¸¸åŸå› ' not in cost_df_copy.columns:
                    cost_df_copy['æ•°æ®å¼‚å¸¸åŸå› '] = ''

                # å°è¯•ä½¿ç”¨åŸæœ‰çš„æˆæœ¬åˆ†æå™¨
                try:
                    cost_filtered_df = pd.DataFrame()
                    cost_analyzer = CostAnalyzer(cost_df_copy, cost_filtered_df)
                    cost_analysis = cost_analyzer.analyze(top_n=10)

                    # âœ… æ ¸å¿ƒä¿®å¤ï¼šä»æˆæœ¬åˆ†æç»“æœä¸­æå–æ‰€æœ‰å¿…è¦æ•°æ®
                    cost_summary = cost_analysis.get('overall_summary', cost_analysis.get('summary', {}))
                    cost_media_detail = cost_analysis.get('media_detail', pd.DataFrame())

                    # æå–æ— æ•ˆæ•°æ®è¯¦æƒ…
                    invalid_data_detail = cost_analysis.get('invalid_data_detail', [])

                    # æå–å¼‚å¸¸æ•°æ®è¯¦æƒ…
                    abnormal_data_detail = cost_analysis.get('abnormal_data_detail', [])

                    # æå–æ— æ•ˆæ•°æ®ç»Ÿè®¡
                    invalid_data_stats = cost_analysis.get('invalid_data_stats', {})

                    # æå–å¼‚å¸¸æ•°æ®ç»Ÿè®¡
                    abnormal_data_stats = cost_analysis.get('abnormal_data_stats', {})

                    # æ„å»ºå®Œæ•´çš„æˆæœ¬ç»“æœ
                    cost_result = {
                        "result": convert_pandas_types_to_python(cost_media_detail),
                        "cleaned_data": convert_pandas_types_to_python(cost_df_copy),
                        "filtered_data": convert_pandas_types_to_python(cost_filtered_df),
                        "summary": convert_pandas_types_to_python(cost_summary),
                        "overall_summary": convert_pandas_types_to_python(cost_summary),
                        "detail_data": convert_pandas_types_to_python(cost_media_detail),
                        "media_detail": convert_pandas_types_to_python(cost_media_detail),
                        "group_summary": convert_pandas_types_to_python(
                            cost_analysis.get('group_summary', pd.DataFrame())),
                        "filtered_summary": convert_pandas_types_to_python(
                            cost_analysis.get('filtered_summary', {'ç­›é™¤æ€»æˆæœ¬': 0, 'ç­›é™¤æˆæœ¬å æ¯”': 0})),
                        "cost_efficiency_ranking": convert_pandas_types_to_python(
                            cost_analysis.get('cost_efficiency_ranking', pd.DataFrame())),
                        # âœ… æ ¸å¿ƒä¿®å¤ï¼šæ— æ•ˆæ•°æ®ç›¸å…³
                        "invalid_data_detail": convert_pandas_types_to_python(invalid_data_detail),
                        "invalid_data_stats": convert_pandas_types_to_python(invalid_data_stats),
                        # âœ… æ ¸å¿ƒä¿®å¤ï¼šå¼‚å¸¸æ•°æ®ç›¸å…³
                        "abnormal_data_detail": convert_pandas_types_to_python(abnormal_data_detail),
                        "abnormal_data_stats": convert_pandas_types_to_python(abnormal_data_stats),
                        # âœ… æ ¸å¿ƒä¿®å¤ï¼šæˆæœ¬å‘æŒ¥åˆ†ææ‰€æœ‰å·¥ä½œè¡¨
                        "media_group_workload": convert_pandas_types_to_python(
                            cost_analysis.get('media_group_workload', pd.DataFrame())),
                        "fixed_media_workload": convert_pandas_types_to_python(
                            cost_analysis.get('fixed_media_workload', pd.DataFrame())),
                        "fixed_media_cost": convert_pandas_types_to_python(
                            cost_analysis.get('fixed_media_cost', pd.DataFrame())),
                        "fixed_media_rebate": convert_pandas_types_to_python(
                            cost_analysis.get('fixed_media_rebate', pd.DataFrame())),
                        "fixed_media_performance": convert_pandas_types_to_python(
                            cost_analysis.get('fixed_media_performance', pd.DataFrame())),
                        "fixed_media_level": convert_pandas_types_to_python(
                            cost_analysis.get('fixed_media_level', pd.DataFrame())),
                        "fixed_media_comprehensive": convert_pandas_types_to_python(
                            cost_analysis.get('fixed_media_comprehensive', pd.DataFrame())),
                        "detailed_data": convert_pandas_types_to_python(
                            cost_analysis.get('detailed_data', pd.DataFrame()))
                    }

                    logger.info(f"âœ… å®Œæ•´æˆæœ¬åˆ†ææˆåŠŸï¼Œæ˜ç»†æ•°æ®è¡Œæ•°: {len(cost_result['result'])}")
                    logger.info(f"âœ… æ— æ•ˆæ•°æ®è¯¦æƒ…: {len(invalid_data_detail)} æ¡")
                    logger.info(f"âœ… å¼‚å¸¸æ•°æ®è¯¦æƒ…: {len(abnormal_data_detail)} æ¡")

                except Exception as e:
                    logger.warning(f"æˆæœ¬æ ‡å‡†åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ: {e}")
                    # ä½¿ç”¨ç®€åŒ–åˆ†æ
                    cost_result = create_simple_cost_analysis(cost_df_copy)
                    logger.info("âœ… å·²ä½¿ç”¨ç®€åŒ–æˆæœ¬åˆ†æ")

            except Exception as e:
                logger.error(f"æˆæœ¬åˆ†æå¼‚å¸¸: {e}", exc_info=True)
                # æœ€ç»ˆå…œåº•
                cost_result = create_simple_cost_analysis(cost_df_copy if 'cost_df_copy' in locals() else cost_df)

        # ========== ç”ŸæˆæŠ¥å‘Š ==========
        analysis_id = datetime.now().strftime('%Y%m%d%H%M%S') + '_DB'
        report_generator = ReportGenerator(
            analysis_results={
                'workload': workload_result,
                'quality': quality_result,
                'cost': cost_result
            },
            output_dir=app.config['OUTPUT_DIR']
        )

        reports = {
            "workload": {"excel": ""},
            "quality": {"excel": ""},
            "cost": {"excel": ""},
            "full": {"full_excel": ""}
        }

        # âœ… ä¿®å¤ï¼šç§»é™¤analysis_idå‚æ•°
        excel_report_path = report_generator.generate_excel_report('full')
        if excel_report_path:
            reports["full"]["full_excel"] = excel_report_path

        # ========== æ•°æ®æ ¼å¼è½¬æ¢å’Œå­˜å‚¨ ==========
        workload_for_storage = {
            "result": workload_result.get("result", []),
            "summary": workload_result.get("summary", {}),
            "group_summary": workload_result.get("group_summary", []),
            "top_media_ranking": workload_result.get("top_media_ranking", [])
        }

        quality_for_storage = {
            "result": quality_result.get("result", []),
            "summary": quality_result.get("summary", {}),
            "group_summary": quality_result.get("group_summary", []),
            "quality_distribution": quality_result.get("quality_distribution", []),
            "premium_detail": quality_result.get("premium_detail", []),
            "high_read_detail": quality_result.get("high_read_detail", [])
        }

        cost_for_storage = {
            "result": cost_result.get("result", []),
            "summary": cost_result.get("summary", {}),
            "overall_summary": cost_result.get("overall_summary", {}),
            "media_detail": cost_result.get("media_detail", []),
            "group_summary": cost_result.get("group_summary", []),
            "filtered_summary": cost_result.get("filtered_summary", {}),
            "cost_efficiency_ranking": cost_result.get("cost_efficiency_ranking", []),
            "invalid_data_detail": cost_result.get("invalid_data_detail", []),
            "invalid_data_stats": cost_result.get("invalid_data_stats", {}),
            "abnormal_data_detail": cost_result.get("abnormal_data_detail", []),
            "abnormal_data_stats": cost_result.get("abnormal_data_stats", {}),
            "media_group_workload": cost_result.get("media_group_workload", []),
            "fixed_media_workload": cost_result.get("fixed_media_workload", []),
            "fixed_media_cost": cost_result.get("fixed_media_cost", []),
            "fixed_media_rebate": cost_result.get("fixed_media_rebate", []),
            "fixed_media_performance": cost_result.get("fixed_media_performance", []),
            "fixed_media_level": cost_result.get("fixed_media_level", []),
            "fixed_media_comprehensive": cost_result.get("fixed_media_comprehensive", []),
            "detailed_data": cost_result.get("detailed_data", [])
        }

        # å­˜å‚¨åˆ†æç»“æœ
        analysis_data_full = {
            "analysis_id": analysis_id,
            "full_result": {
                "workload": workload_for_storage,
                "quality": quality_for_storage,
                "cost": cost_for_storage
            },
            "reports": reports,
            "category": category,
            "selected_groups": [],
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "data_source": "database"
        }

        analysis_results[analysis_id] = analysis_data_full

        # æŒä¹…åŒ–åˆ°JSON
        analysis_data_serializable = {
            "analysis_id": analysis_id,
            "category": category,
            "selected_groups": [],
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "data_source": "database",
            "workload": workload_for_storage,
            "quality": quality_for_storage,
            "cost": cost_for_storage,
            "report_files": {
                "workload": reports["workload"]["excel"],
                "quality": reports["quality"]["excel"],
                "cost": reports["cost"]["excel"],
                "full": reports["full"]["full_excel"]
            }
        }

        result_file_path = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results', f'{analysis_id}.json')
        os.makedirs(os.path.dirname(result_file_path), exist_ok=True)
        with open(result_file_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_data_serializable, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… æ•°æ®åº“åˆ†æå®Œæˆï¼Œåˆ†æIDï¼š{analysis_id}")
        logger.info(f"âœ… å·¥ä½œé‡æ•°æ®ï¼š{len(workload_result.get('result', []))} æ¡")
        logger.info(f"âœ… å·¥ä½œè´¨é‡æ•°æ®ï¼š{len(quality_result.get('result', []))} æ¡")
        logger.info(f"âœ… æˆæœ¬æ•°æ®ï¼š{len(cost_result.get('result', []))} æ¡")
        logger.info(f"âœ… æ— æ•ˆæ•°æ®è¯¦æƒ…ï¼š{len(cost_result.get('invalid_data_detail', []))} æ¡")
        logger.info(f"âœ… å¼‚å¸¸æ•°æ®è¯¦æƒ…ï¼š{len(cost_result.get('abnormal_data_detail', []))} æ¡")

        flash('âœ… æ•°æ®åº“æ•°æ®è¯»å–æˆåŠŸï¼Œåˆ†æå·²å®Œæˆï¼', 'success')
        return redirect(url_for('dashboard', analysis_id=analysis_id, upload_success=1))

    except Exception as e:
        error_msg = f"âŒ æ•°æ®åº“åˆ†æå¤±è´¥ï¼š{str(e)}"
        logger.error(f"{error_msg}\n{traceback.format_exc()}")
        flash(error_msg, 'error')
        return redirect(url_for('db_analysis_index'))

@app.route('/report/quality/<analysis_id>')
@login_required  # æ–°å¢ï¼šç™»å½•éªŒè¯
def quality_report(analysis_id):
    """å·¥ä½œè´¨é‡åˆ†ææŠ¥å‘Šè¯¦æƒ…é¡µï¼Œä¿®å¤å˜é‡ååŒ¹é…é—®é¢˜"""
    if analysis_id == 'latest':
        results_dir = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results')
        try:
            result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
            if not result_files:
                # è¿”å›ç©ºæ•°æ®
                return render_template('quality_analysis.html',
                                       analysis_id='latest',
                                       analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                      "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                       detail_data=[],
                                       summary={},
                                       group_summary=[],
                                       quality_distribution=[],
                                       premium_detail=[],
                                       high_read_detail=[],
                                       report={"excel": ""})
            result_files.sort(reverse=True)
            latest_file = result_files[0]
            analysis_id = latest_file.replace('.json', '')
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°åˆ†æç»“æœå¤±è´¥: {e}")
            return render_template('quality_analysis.html',
                                   analysis_id='latest',
                                   analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                  "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                   detail_data=[],
                                   summary={},
                                   group_summary=[],
                                   quality_distribution=[],
                                   premium_detail=[],
                                   high_read_detail=[],
                                   report={"excel": ""})

    # åŠ è½½åˆ†æç»“æœ
    analysis_data = load_analysis_result(analysis_id)
    if not analysis_data:
        # è¿”å›ç©ºæ•°æ®
        return render_template('quality_analysis.html',
                               analysis_id=analysis_id,
                               analysis_data={"category": "æš‚æ— ç±»ç›®",
                                              "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                               detail_data=[],
                               summary={},
                               group_summary=[],
                               quality_distribution=[],
                               premium_detail=[],
                               high_read_detail=[],
                               report={"excel": ""})

    # ä»full_result.qualityä¸­è·å–æ•°æ®
    full_result = analysis_data.get("full_result", {})
    quality_data = full_result.get("quality", {})

    # âœ… å…³é”®ä¿®å¤ï¼šè·å–æ‰€æœ‰å¿…éœ€çš„æ•°æ®å­—æ®µ
    detail_data = quality_data.get("result", [])
    summary = quality_data.get("summary", {})
    group_summary = quality_data.get("group_summary", [])
    quality_distribution = quality_data.get("quality_distribution", [])
    premium_detail = quality_data.get("premium_detail", [])
    high_read_detail = quality_data.get("high_read_detail", [])

    # å¦‚æœresultä¸ºç©ºä½†æœ‰detailï¼Œå°è¯•ä»detailè·å–
    if not detail_data and "detail" in quality_data:
        detail_data = quality_data.get("detail", [])

    # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æ­£ç¡®çš„ç±»å‹
    if not isinstance(detail_data, list):
        detail_data = []
    if not isinstance(group_summary, list):
        group_summary = []
    if not isinstance(quality_distribution, list):
        quality_distribution = []
    if not isinstance(premium_detail, list):
        premium_detail = []
    if not isinstance(high_read_detail, list):
        high_read_detail = []

    report = analysis_data.get("reports", {}).get("quality", {"excel": ""})
    analysis_data_info = {
        "category": analysis_data.get("category", "æš‚æ— ç±»ç›®"),
        "timestamp": analysis_data.get("timestamp", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    }

    # âœ… å…³é”®ä¿®å¤ï¼šä¼ é€’æ­£ç¡®çš„å˜é‡åç»™æ¨¡æ¿
    return render_template('quality_analysis.html',
                           analysis_id=analysis_id,
                           analysis_data=analysis_data_info,
                           detail_data=detail_data,
                           summary=summary,
                           group_summary=group_summary,
                           quality_distribution=quality_distribution,
                           premium_detail=premium_detail,
                           high_read_detail=high_read_detail,
                           report=report)


@app.route('/report/cost/<analysis_id>')
@login_required  # æ–°å¢ï¼šç™»å½•éªŒè¯
def cost_report(analysis_id):
    """æˆæœ¬åˆ†ææŠ¥å‘Šè¯¦æƒ…é¡µï¼Œä¿®å¤å˜é‡ååŒ¹é…é—®é¢˜"""
    # å¤„ç† 'latest' æƒ…å†µ
    if analysis_id == 'latest':
        results_dir = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results')
        try:
            result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
            if not result_files:
                return render_template('cost_analysis.html',
                                       analysis_id='latest',
                                       analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                      "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                       overall_summary={},
                                       invalid_data_stats={
                                           'æ€»æ•°æ®æ¡æ•°': 0,
                                           'æœ‰æ•ˆæ•°æ®æ¡æ•°': 0,
                                           'æ— æ•ˆæ•°æ®æ¡æ•°': 0,
                                           'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)': '0%',
                                           'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)': '0%',
                                           'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ': {},
                                           'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)': 0
                                       },
                                       invalid_data_detail_count=0,
                                       abnormal_data_stats={
                                           'å¼‚å¸¸æ•°æ®æ¡æ•°': 0,
                                           'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)': '0%',
                                           'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ': {},
                                           'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)': 0,
                                           'å‚ä¸åˆ†ææ•°æ®æ¡æ•°': 0,
                                           'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)': '0%'
                                       },
                                       abnormal_data_detail_count=0,
                                       media_group_workload=[],
                                       fixed_media_workload=[],
                                       fixed_media_cost=[],
                                       fixed_media_rebate=[],
                                       fixed_media_performance=[],
                                       fixed_media_level=[],
                                       fixed_media_comprehensive=[],
                                       report={"excel": ""})
            result_files.sort(reverse=True)
            latest_file = result_files[0]
            analysis_id = latest_file.replace('.json', '')
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°åˆ†æç»“æœå¤±è´¥: {e}")
            return render_template('cost_analysis.html',
                                   analysis_id='latest',
                                   analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                  "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                   overall_summary={},
                                   invalid_data_stats={
                                       'æ€»æ•°æ®æ¡æ•°': 0,
                                       'æœ‰æ•ˆæ•°æ®æ¡æ•°': 0,
                                       'æ— æ•ˆæ•°æ®æ¡æ•°': 0,
                                       'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)': '0%',
                                       'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)': '0%',
                                       'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ': {},
                                       'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)': 0
                                   },
                                   invalid_data_detail_count=0,
                                   abnormal_data_stats={
                                       'å¼‚å¸¸æ•°æ®æ¡æ•°': 0,
                                       'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)': '0%',
                                       'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ': {},
                                       'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)': 0,
                                       'å‚ä¸åˆ†ææ•°æ®æ¡æ•°': 0,
                                       'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)': '0%'
                                   },
                                   abnormal_data_detail_count=0,
                                   media_group_workload=[],
                                   fixed_media_workload=[],
                                       fixed_media_cost=[],
                                       fixed_media_rebate=[],
                                       fixed_media_performance=[],
                                       fixed_media_level=[],
                                       fixed_media_comprehensive=[],
                                       report={"excel": ""})

    # åŠ è½½åˆ†æç»“æœ
    analysis_data = load_analysis_result(analysis_id)
    if not analysis_data:
        # è¿”å›ç©ºæ•°æ®ï¼ŒåŒ…å«æ— æ•ˆæ•°æ®ç»Ÿè®¡çš„é»˜è®¤å€¼
        overall_summary = {}
        invalid_data_stats = {
            'æ€»æ•°æ®æ¡æ•°': 0,
            'æœ‰æ•ˆæ•°æ®æ¡æ•°': 0,
            'æ— æ•ˆæ•°æ®æ¡æ•°': 0,
            'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)': '0%',
            'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)': '0%',
            'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ': {},
            'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)': 0
        }
        abnormal_data_stats = {
            'å¼‚å¸¸æ•°æ®æ¡æ•°': 0,
            'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)': '0%',
            'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ': {},
            'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)': 0,
            'å‚ä¸åˆ†ææ•°æ®æ¡æ•°': 0,
            'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)': '0%'
        }
        invalid_data_detail_count = 0
        abnormal_data_detail_count = 0
        media_group_workload = []
        fixed_media_workload = []
        fixed_media_cost = []
        fixed_media_rebate = []
        fixed_media_performance = []
        fixed_media_level = []
        fixed_media_comprehensive = []
        report = {"excel": ""}
        analysis_data_info = {"category": "æš‚æ— ç±»ç›®", "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    else:
        # ä»full_result.costä¸­è·å–æ•°æ®
        full_result = analysis_data.get("full_result", {})
        cost_data = full_result.get("cost", {})

        # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿è·å–æ‰€æœ‰æˆæœ¬åˆ†æå·¥ä½œè¡¨æ•°æ®
        overall_summary = cost_data.get("overall_summary", {})

        # âœ… æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿ overall_summary åŒ…å«æ— æ•ˆæ•°æ®ç»Ÿè®¡
        if not isinstance(overall_summary, dict):
            overall_summary = {}

        # âœ… ç¡®ä¿ overall_summary æœ‰åŸºæœ¬çš„æ— æ•ˆæ•°æ®ç»Ÿè®¡å­—æ®µ
        overall_summary['æ€»æ•°æ®æ¡æ•°'] = overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0)
        overall_summary['æœ‰æ•ˆæ•°æ®æ¡æ•°'] = overall_summary.get('æœ‰æ•ˆæ•°æ®æ¡æ•°', 0)
        overall_summary['æ— æ•ˆæ•°æ®æ¡æ•°'] = overall_summary.get('æ— æ•ˆæ•°æ®æ¡æ•°', 0)
        overall_summary['å¼‚å¸¸æ•°æ®æ¡æ•°'] = overall_summary.get('å¼‚å¸¸æ•°æ®æ¡æ•°', 0)
        overall_summary['å‚ä¸åˆ†ææ•°æ®æ¡æ•°'] = overall_summary.get('å‚ä¸åˆ†ææ•°æ®æ¡æ•°', 0)

        if overall_summary['æ€»æ•°æ®æ¡æ•°'] > 0:
            if 'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                overall_summary[
                    'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = f"{(overall_summary['æœ‰æ•ˆæ•°æ®æ¡æ•°'] / overall_summary['æ€»æ•°æ®æ¡æ•°'] * 100):.2f}%"
            if 'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                overall_summary[
                    'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = f"{(overall_summary['æ— æ•ˆæ•°æ®æ¡æ•°'] / overall_summary['æ€»æ•°æ®æ¡æ•°'] * 100):.2f}%"
            if 'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                overall_summary[
                    'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)'] = f"{(overall_summary['å¼‚å¸¸æ•°æ®æ¡æ•°'] / overall_summary['æ€»æ•°æ®æ¡æ•°'] * 100):.2f}%"
            if 'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)' not in overall_summary:
                overall_summary[
                    'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)'] = f"{(overall_summary['å‚ä¸åˆ†ææ•°æ®æ¡æ•°'] / overall_summary['æ€»æ•°æ®æ¡æ•°'] * 100):.2f}%"
        else:
            overall_summary['æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = overall_summary.get('æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%')
            overall_summary['æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = overall_summary.get('æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%')
            overall_summary['å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)'] = overall_summary.get('å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)', '0%')
            overall_summary['å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)'] = overall_summary.get('å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)', '0%')

        overall_summary['æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ'] = overall_summary.get('æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ', {})
        overall_summary['å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ'] = overall_summary.get('å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ', {})
        overall_summary['æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)'] = overall_summary.get('æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)', 0)
        overall_summary['å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)'] = overall_summary.get('å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)', 0)

        # âœ… è·å– invalid_data_statsï¼ˆä¼˜å…ˆä» cost_data è·å–ï¼Œå¦åˆ™ä» overall_summary ç”Ÿæˆï¼‰
        invalid_data_stats = cost_data.get("invalid_data_stats", {})
        if not invalid_data_stats:
            invalid_data_stats = {
                'æ€»æ•°æ®æ¡æ•°': overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0),
                'æœ‰æ•ˆæ•°æ®æ¡æ•°': overall_summary.get('æœ‰æ•ˆæ•°æ®æ¡æ•°', 0),
                'æ— æ•ˆæ•°æ®æ¡æ•°': overall_summary.get('æ— æ•ˆæ•°æ®æ¡æ•°', 0),
                'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)': overall_summary.get('æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%'),
                'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)': overall_summary.get('æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%'),
                'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ': overall_summary.get('æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ', {}),
                'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)': overall_summary.get('æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)', 0)
            }

        # âœ… è·å– abnormal_data_statsï¼ˆä¼˜å…ˆä» cost_data è·å–ï¼Œå¦åˆ™ä» overall_summary ç”Ÿæˆï¼‰
        abnormal_data_stats = cost_data.get("abnormal_data_stats", {})
        if not abnormal_data_stats:
            abnormal_data_stats = {
                'å¼‚å¸¸æ•°æ®æ¡æ•°': overall_summary.get('å¼‚å¸¸æ•°æ®æ¡æ•°', 0),
                'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)': overall_summary.get('å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)', '0%'),
                'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ': overall_summary.get('å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ', {}),
                'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)': overall_summary.get('å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)', 0),
                'å‚ä¸åˆ†ææ•°æ®æ¡æ•°': overall_summary.get('å‚ä¸åˆ†ææ•°æ®æ¡æ•°', 0),
                'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)': overall_summary.get('å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)', '0%')
            }

        # âœ… è·å– invalid_data_detail å’Œè®¡æ•°
        invalid_data_detail = cost_data.get("invalid_data_detail", [])
        invalid_data_detail_count = len(invalid_data_detail) if isinstance(invalid_data_detail, list) else 0

        # âœ… è·å– abnormal_data_detail å’Œè®¡æ•°
        abnormal_data_detail = cost_data.get("abnormal_data_detail", [])
        abnormal_data_detail_count = len(abnormal_data_detail) if isinstance(abnormal_data_detail, list) else 0

        # âœ… è·å–æ‰€æœ‰æˆæœ¬å·¥ä½œè¡¨æ•°æ®
        media_group_workload = cost_data.get("media_group_workload", [])
        if not isinstance(media_group_workload, list):
            media_group_workload = []

        fixed_media_workload = cost_data.get("fixed_media_workload", [])
        if not isinstance(fixed_media_workload, list):
            fixed_media_workload = []

        fixed_media_cost = cost_data.get("fixed_media_cost", [])
        if not isinstance(fixed_media_cost, list):
            fixed_media_cost = []

        fixed_media_rebate = cost_data.get("fixed_media_rebate", [])
        if not isinstance(fixed_media_rebate, list):
            fixed_media_rebate = []

        fixed_media_performance = cost_data.get("fixed_media_performance", [])
        if not isinstance(fixed_media_performance, list):
            fixed_media_performance = []

        fixed_media_level = cost_data.get("fixed_media_level", [])
        if not isinstance(fixed_media_level, list):
            fixed_media_level = []

        fixed_media_comprehensive = cost_data.get("fixed_media_comprehensive", [])
        if not isinstance(fixed_media_comprehensive, list):
            fixed_media_comprehensive = []

        report = analysis_data.get("reports", {}).get("cost", {"excel": ""})
        analysis_data_info = {
            "category": analysis_data.get("category", "æš‚æ— ç±»ç›®"),
            "timestamp": analysis_data.get("timestamp", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        }

    # âœ… å…³é”®ä¿®å¤ï¼šä¼ é€’æ­£ç¡®çš„å˜é‡åç»™æ¨¡æ¿ï¼ŒåŒ¹é… cost_analysis.html ä¸­çš„å˜é‡å
    return render_template('cost_analysis.html',
                           analysis_id=analysis_id,
                           analysis_data=analysis_data_info,
                           overall_summary=overall_summary,
                           invalid_data_stats=invalid_data_stats,  # âœ… æ–°å¢ï¼šæ— æ•ˆæ•°æ®ç»Ÿè®¡
                           invalid_data_detail_count=invalid_data_detail_count,  # âœ… æ–°å¢ï¼šæ— æ•ˆæ•°æ®è¯¦æƒ…æ•°é‡
                           abnormal_data_stats=abnormal_data_stats,  # âœ… æ–°å¢ï¼šå¼‚å¸¸æ•°æ®ç»Ÿè®¡
                           abnormal_data_detail_count=abnormal_data_detail_count,  # âœ… æ–°å¢ï¼šå¼‚å¸¸æ•°æ®è¯¦æƒ…æ•°é‡
                           media_group_workload=media_group_workload,
                           fixed_media_workload=fixed_media_workload,
                           fixed_media_cost=fixed_media_cost,
                           fixed_media_rebate=fixed_media_rebate,
                           fixed_media_performance=fixed_media_performance,
                           fixed_media_level=fixed_media_level,
                           fixed_media_comprehensive=fixed_media_comprehensive,
                           report=report)


@app.route('/report/cost/invalid_data/<analysis_id>')
def cost_invalid_data_report(analysis_id):
    """æˆæœ¬åˆ†ææ— æ•ˆæ•°æ®è¯¦æƒ…é¡µ - ä¿®å¤ç‰ˆæœ¬"""
    # å¤„ç† 'latest' æƒ…å†µ
    if analysis_id == 'latest':
        results_dir = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results')
        try:
            result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
            if not result_files:
                return render_template('cost_invalid_data.html',
                                       analysis_id='latest',
                                       analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                      "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                       invalid_data_detail=[],
                                       invalid_data_stats={})
            result_files.sort(reverse=True)
            latest_file = result_files[0]
            analysis_id = latest_file.replace('.json', '')
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°åˆ†æç»“æœå¤±è´¥: {e}")
            return render_template('cost_invalid_data.html',
                                   analysis_id='latest',
                                   analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                  "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                   invalid_data_detail=[],
                                   invalid_data_stats={})

    # åŠ è½½åˆ†æç»“æœ
    analysis_data = load_analysis_result(analysis_id)
    if not analysis_data:
        return render_template('cost_invalid_data.html',
                               analysis_id=analysis_id,
                               analysis_data={"category": "æš‚æ— ç±»ç›®",
                                              "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                               invalid_data_detail=[],
                               invalid_data_stats={})

    # è·å–æ— æ•ˆæ•°æ®è¯¦æƒ…å’Œç»Ÿè®¡
    full_result = analysis_data.get("full_result", {})
    cost_data = full_result.get("cost", {})

    # âœ… æ ¸å¿ƒä¿®å¤ï¼šç›´æ¥ä» cost_data è·å– invalid_data_detail
    invalid_data_detail = cost_data.get("invalid_data_detail", [])

    # âœ… å…³é”®ä¿®å¤ï¼šå¦‚æœ invalid_data_detail ä¸ºç©ºï¼Œä» detailed_data ä¸­ç­›é€‰
    if not invalid_data_detail or len(invalid_data_detail) == 0:
        detailed_data = cost_data.get("detailed_data", [])
        if detailed_data and isinstance(detailed_data, list) and len(detailed_data) > 0:
            logger.info(f"ä» detailed_data ä¸­ç­›é€‰æ— æ•ˆæ•°æ®ï¼Œæ€»æ•°æ®: {len(detailed_data)}")

            invalid_data_detail = []
            for item in detailed_data:
                if isinstance(item, dict):
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆæ•°æ®
                    cost_invalid = item.get('æˆæœ¬æ— æ•ˆ', False)

                    # å¦‚æœæ˜¯æ— æ•ˆæ•°æ®ï¼ˆæˆæœ¬=0æˆ–æˆæœ¬ç¼ºå¤±ï¼‰
                    if cost_invalid:
                        # æ„å»ºæ— æ•ˆæ•°æ®è¯¦æƒ…æ ¼å¼
                        detail = {
                            'è®°å½•åºå·': item.get('è®°å½•åºå·', 0),
                            'è¾¾äººæ˜µç§°': item.get('è¾¾äººæ˜µç§°', 'æœªçŸ¥'),
                            'é¡¹ç›®åç§°': item.get('é¡¹ç›®åç§°', 'æœªçŸ¥'),
                            'å®šæ¡£åª’ä»‹': item.get('å®šæ¡£åª’ä»‹', 'æœªçŸ¥'),
                            'æˆæœ¬': item.get('æˆæœ¬', 0),
                            'æŠ¥ä»·': item.get('æŠ¥ä»·', 0),
                            'ä¸‹å•ä»·': item.get('ä¸‹å•ä»·', 0),
                            'è¿”ç‚¹': item.get('è¿”ç‚¹', 0),
                            'è¿”ç‚¹æ¯”ä¾‹': item.get('è¿”ç‚¹æ¯”ä¾‹', 0) * 100 if item.get('è¿”ç‚¹æ¯”ä¾‹') else 0,
                            'ä¸å«æ‰‹ç»­è´¹çš„ä¸‹å•ä»·': item.get('ä¸å«æ‰‹ç»­è´¹çš„ä¸‹å•ä»·', ''),
                            'æˆæœ¬æ— æ•ˆåŸå› ': item.get('æˆæœ¬æ— æ•ˆåŸå› ', 'æˆæœ¬ä¸º0æˆ–ç¼ºå¤±'),
                            'æ˜¯å¦è¢«ç­›é™¤': True,  # æ— æ•ˆæ•°æ®é»˜è®¤è¢«ç­›é™¤
                            'ç­›é™¤åŸå› ': item.get('æˆæœ¬æ— æ•ˆåŸå› ', 'æˆæœ¬ä¸º0æˆ–ç¼ºå¤±'),
                            'æ— æ•ˆç±»å‹': 'æˆæœ¬ä¸º0æˆ–ç¼ºå¤±'
                        }

                        # åˆ¤æ–­æ— æ•ˆç±»å‹
                        if detail['æˆæœ¬'] == 0:
                            detail['æˆæœ¬æ— æ•ˆåŸå› '] = 'æˆæœ¬ä¸º0'
                        elif pd.isna(detail['æˆæœ¬']):
                            detail['æˆæœ¬æ— æ•ˆåŸå› '] = 'æˆæœ¬ç¼ºå¤±'
                        elif 'æˆæœ¬æ— æ•ˆ' in str(item):
                            detail['æˆæœ¬æ— æ•ˆåŸå› '] = 'æˆæœ¬æ— æ•ˆ'
                        else:
                            detail['æˆæœ¬æ— æ•ˆåŸå› '] = 'æœªçŸ¥åŸå› '

                        invalid_data_detail.append(detail)

            logger.info(f"ç­›é€‰åˆ°æ— æ•ˆæ•°æ®: {len(invalid_data_detail)} æ¡")

    # âœ… ç¡®ä¿ invalid_data_detail æ˜¯åˆ—è¡¨
    if not isinstance(invalid_data_detail, list):
        logger.warning(f"invalid_data_detail ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(invalid_data_detail)}")
        # å°è¯•è½¬æ¢
        if isinstance(invalid_data_detail, pd.DataFrame):
            invalid_data_detail = invalid_data_detail.to_dict('records')
        elif isinstance(invalid_data_detail, dict):
            invalid_data_detail = [invalid_data_detail]
        else:
            invalid_data_detail = []

    # âœ… è·å–æ— æ•ˆæ•°æ®ç»Ÿè®¡
    invalid_data_stats = cost_data.get("invalid_data_stats", {})

    # å¦‚æœç»Ÿè®¡ä¿¡æ¯ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ ¹æ®è¯¦æƒ…è®¡ç®—
    if not invalid_data_stats or not isinstance(invalid_data_stats, dict):
        invalid_data_stats = {}

    # ç¡®ä¿ç»Ÿè®¡ä¿¡æ¯åŒ…å«å¿…è¦å­—æ®µ
    if 'æ— æ•ˆæ•°æ®æ¡æ•°' not in invalid_data_stats:
        invalid_data_stats['æ— æ•ˆæ•°æ®æ¡æ•°'] = len(invalid_data_detail)

    if 'æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)' not in invalid_data_stats:
        overall_summary = cost_data.get("overall_summary", {})
        total_count = overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0)
        if total_count > 0:
            invalid_data_stats['æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = f"{(len(invalid_data_detail) / total_count * 100):.2f}%"
        else:
            invalid_data_stats['æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = '0%'

    if 'æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ' not in invalid_data_stats:
        # ç»Ÿè®¡æ— æ•ˆåŸå› åˆ†å¸ƒ
        reason_dist = {}
        total_cost = 0
        for detail in invalid_data_detail:
            if isinstance(detail, dict):
                reason = detail.get('æˆæœ¬æ— æ•ˆåŸå› ', 'æœªçŸ¥åŸå› ')
                reason_dist[reason] = reason_dist.get(reason, 0) + 1
                total_cost += detail.get('æˆæœ¬', 0)

        invalid_data_stats['æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ'] = reason_dist
        invalid_data_stats['æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)'] = total_cost

    # ç¡®ä¿å…¶ä»–ç»Ÿè®¡å­—æ®µå­˜åœ¨
    if 'æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)' not in invalid_data_stats:
        invalid_data_stats['æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)'] = 0

    if 'æœ‰æ•ˆæ•°æ®æ¡æ•°' not in invalid_data_stats:
        overall_summary = cost_data.get("overall_summary", {})
        total_count = overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0)
        invalid_data_stats['æœ‰æ•ˆæ•°æ®æ¡æ•°'] = total_count - len(invalid_data_detail) if total_count > 0 else 0

    if 'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)' not in invalid_data_stats:
        overall_summary = cost_data.get("overall_summary", {})
        total_count = overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0)
        if total_count > 0:
            invalid_data_stats[
                'æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = f"{((total_count - len(invalid_data_detail)) / total_count * 100):.2f}%"
        else:
            invalid_data_stats['æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)'] = '0%'

    if 'æ€»æ•°æ®æ¡æ•°' not in invalid_data_stats:
        overall_summary = cost_data.get("overall_summary", {})
        invalid_data_stats['æ€»æ•°æ®æ¡æ•°'] = overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0)

    logger.info(f"æ¸²æŸ“æ— æ•ˆæ•°æ®é¡µé¢: analysis_id={analysis_id}, æ— æ•ˆæ•°æ®æ¡æ•°={len(invalid_data_detail)}")
    logger.info(f"æ— æ•ˆæ•°æ®ç»Ÿè®¡: {invalid_data_stats}")

    analysis_data_info = {
        "category": analysis_data.get("category", "æš‚æ— ç±»ç›®"),
        "timestamp": analysis_data.get("timestamp", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    }

    return render_template('cost_invalid_data.html',
                           analysis_id=analysis_id,
                           analysis_data=analysis_data_info,
                           invalid_data_detail=invalid_data_detail,
                           invalid_data_stats=invalid_data_stats)

@app.route('/report/cost/abnormal_data/<analysis_id>')
@login_required
def cost_abnormal_data_report(analysis_id):
    """æˆæœ¬åˆ†æå¼‚å¸¸æ•°æ®è¯¦æƒ…é¡µ - å®Œæ•´ä¿®å¤ç‰ˆæœ¬"""
    # å¤„ç† 'latest' æƒ…å†µ
    if analysis_id == 'latest':
        results_dir = os.path.join(app.config['OUTPUT_DIR'], 'analysis_results')
        try:
            result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
            if not result_files:
                return render_template('cost_abnormal_data.html',
                                       analysis_id='latest',
                                       analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                      "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                       abnormal_data_detail=[],
                                       abnormal_data_stats={})
            result_files.sort(reverse=True)
            latest_file = result_files[0]
            analysis_id = latest_file.replace('.json', '')
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°åˆ†æç»“æœå¤±è´¥: {e}")
            return render_template('cost_abnormal_data.html',
                                   analysis_id='latest',
                                   analysis_data={"category": "æš‚æ— ç±»ç›®",
                                                  "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                                   abnormal_data_detail=[],
                                   abnormal_data_stats={})

    # åŠ è½½åˆ†æç»“æœ
    analysis_data = load_analysis_result(analysis_id)
    if not analysis_data:
        return render_template('cost_abnormal_data.html',
                               analysis_id=analysis_id,
                               analysis_data={"category": "æš‚æ— ç±»ç›®",
                                              "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                               abnormal_data_detail=[],
                               abnormal_data_stats={})

    # è·å–å¼‚å¸¸æ•°æ®è¯¦æƒ…å’Œç»Ÿè®¡
    full_result = analysis_data.get("full_result", {})
    cost_data = full_result.get("cost", {})

    # âœ… æ ¸å¿ƒä¿®å¤ï¼šç›´æ¥ä» cost_data è·å– abnormal_data_detail
    abnormal_data_detail = cost_data.get("abnormal_data_detail", [])

    # âœ… å…³é”®ä¿®å¤ï¼šå¦‚æœ abnormal_data_detail ä¸ºç©ºï¼Œä» detailed_data ä¸­ç­›é€‰
    if not abnormal_data_detail or len(abnormal_data_detail) == 0:
        detailed_data = cost_data.get("detailed_data", [])
        if detailed_data and isinstance(detailed_data, list) and len(detailed_data) > 0:
            logger.info(f"ä» detailed_data ä¸­ç­›é€‰å¼‚å¸¸æ•°æ®ï¼Œæ€»æ•°æ®: {len(detailed_data)}")

            abnormal_data_detail = []
            for item in detailed_data:
                if isinstance(item, dict):
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚å¸¸æ•°æ®
                    data_abnormal = item.get('æ•°æ®å¼‚å¸¸', False)
                    cost_invalid = item.get('æˆæœ¬æ— æ•ˆ', False)

                    # å¦‚æœæ˜¯å¼‚å¸¸æ•°æ®ï¼ˆå‚ä¸åˆ†æä½†æ ‡è®°å¼‚å¸¸ï¼‰
                    if data_abnormal and not cost_invalid:
                        # æ„å»ºå¼‚å¸¸æ•°æ®è¯¦æƒ…æ ¼å¼
                        detail = {
                            'è®°å½•åºå·': item.get('è®°å½•åºå·', 0),
                            'è¾¾äººæ˜µç§°': item.get('è¾¾äººæ˜µç§°', 'æœªçŸ¥'),
                            'é¡¹ç›®åç§°': item.get('é¡¹ç›®åç§°', 'æœªçŸ¥'),
                            'å®šæ¡£åª’ä»‹': item.get('å®šæ¡£åª’ä»‹', 'æœªçŸ¥'),
                            'æˆæœ¬': item.get('æˆæœ¬', 0),
                            'æŠ¥ä»·': item.get('æŠ¥ä»·', 0),
                            'ä¸‹å•ä»·': item.get('ä¸‹å•ä»·', 0),
                            'è¿”ç‚¹': item.get('è¿”ç‚¹', 0),
                            'è¿”ç‚¹æ¯”ä¾‹': item.get('è¿”ç‚¹æ¯”ä¾‹', 0) * 100 if item.get('è¿”ç‚¹æ¯”ä¾‹') else 0,
                            'ä¸å«æ‰‹ç»­è´¹çš„ä¸‹å•ä»·': item.get('ä¸å«æ‰‹ç»­è´¹çš„ä¸‹å•ä»·', ''),
                            'æ•°æ®å¼‚å¸¸åŸå› ': item.get('æ•°æ®å¼‚å¸¸åŸå› ', 'æœªçŸ¥å¼‚å¸¸'),
                            'å¼‚å¸¸ç±»å‹': 'æ•°æ®å¼‚å¸¸',
                            'æ˜¯å¦å‚ä¸åˆ†æ': True,
                            'å‚ä¸åˆ†ææ ‡è¯†': 'å¼‚å¸¸æ•°æ®'
                        }

                        # åˆ¤æ–­å¼‚å¸¸ç±»å‹
                        reason = detail['æ•°æ®å¼‚å¸¸åŸå› ']
                        if 'æŠ¥ä»·<' in reason:
                            detail['å¼‚å¸¸ç±»å‹'] = 'æŠ¥ä»·å¼‚å¸¸'
                        elif 'æ— æ³•åˆ¤æ–­' in reason:
                            detail['å¼‚å¸¸ç±»å‹'] = 'æ•°æ®å¼‚å¸¸'
                        elif 'è¿”ç‚¹æ¯”ä¾‹' in reason:
                            detail['å¼‚å¸¸ç±»å‹'] = 'è¿”ç‚¹å¼‚å¸¸'
                        elif 'ç­›é™¤' in reason or reason in ['æ•°æ®å¼‚å¸¸', 'æˆæœ¬ä¸º0', 'æˆæœ¬ç¼ºå¤±', 'æ•°æ®ä¸å…¨']:
                            detail['å¼‚å¸¸ç±»å‹'] = 'ç­›é™¤å¼‚å¸¸'

                        abnormal_data_detail.append(detail)

            logger.info(f"ç­›é€‰åˆ°å¼‚å¸¸æ•°æ®: {len(abnormal_data_detail)} æ¡")

    # âœ… ç¡®ä¿ abnormal_data_detail æ˜¯åˆ—è¡¨
    if not isinstance(abnormal_data_detail, list):
        logger.warning(f"abnormal_data_detail ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(abnormal_data_detail)}")
        # å°è¯•è½¬æ¢
        if isinstance(abnormal_data_detail, pd.DataFrame):
            abnormal_data_detail = abnormal_data_detail.to_dict('records')
        elif isinstance(abnormal_data_detail, dict):
            abnormal_data_detail = [abnormal_data_detail]
        else:
            abnormal_data_detail = []

    # âœ… è·å–å¼‚å¸¸æ•°æ®ç»Ÿè®¡
    abnormal_data_stats = cost_data.get("abnormal_data_stats", {})

    # å¦‚æœç»Ÿè®¡ä¿¡æ¯ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ ¹æ®è¯¦æƒ…è®¡ç®—
    if not abnormal_data_stats or not isinstance(abnormal_data_stats, dict):
        abnormal_data_stats = {}

    # ç¡®ä¿ç»Ÿè®¡ä¿¡æ¯åŒ…å«å¿…è¦å­—æ®µ
    if 'å¼‚å¸¸æ•°æ®æ¡æ•°' not in abnormal_data_stats:
        abnormal_data_stats['å¼‚å¸¸æ•°æ®æ¡æ•°'] = len(abnormal_data_detail)

    if 'å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)' not in abnormal_data_stats:
        overall_summary = cost_data.get("overall_summary", {})
        total_count = overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0)
        if total_count > 0:
            abnormal_data_stats['å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)'] = f"{(len(abnormal_data_detail) / total_count * 100):.2f}%"
        else:
            abnormal_data_stats['å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)'] = '0%'

    if 'å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ' not in abnormal_data_stats:
        # ç»Ÿè®¡å¼‚å¸¸åŸå› åˆ†å¸ƒ
        reason_dist = {}
        total_cost = 0
        for detail in abnormal_data_detail:
            if isinstance(detail, dict):
                reason = detail.get('æ•°æ®å¼‚å¸¸åŸå› ', 'æœªçŸ¥åŸå› ')
                reason_dist[reason] = reason_dist.get(reason, 0) + 1
                total_cost += detail.get('æˆæœ¬', 0)

        abnormal_data_stats['å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ'] = reason_dist
        abnormal_data_stats['å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)'] = total_cost

    # ç¡®ä¿å…¶ä»–ç»Ÿè®¡å­—æ®µå­˜åœ¨
    if 'å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)' not in abnormal_data_stats:
        abnormal_data_stats['å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)'] = 0

    if 'å‚ä¸åˆ†ææ•°æ®æ¡æ•°' not in abnormal_data_stats:
        overall_summary = cost_data.get("overall_summary", {})
        total_count = overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0)
        invalid_count = overall_summary.get('æ— æ•ˆæ•°æ®æ¡æ•°', 0)
        abnormal_data_stats['å‚ä¸åˆ†ææ•°æ®æ¡æ•°'] = total_count - invalid_count if total_count > 0 else 0

    if 'å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)' not in abnormal_data_stats:
        overall_summary = cost_data.get("overall_summary", {})
        total_count = overall_summary.get('æ€»æ•°æ®æ¡æ•°', 0)
        invalid_count = overall_summary.get('æ— æ•ˆæ•°æ®æ¡æ•°', 0)
        if total_count > 0:
            abnormal_data_stats['å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)'] = f"{((total_count - invalid_count) / total_count * 100):.2f}%"
        else:
            abnormal_data_stats['å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)'] = '0%'

    logger.info(f"æ¸²æŸ“å¼‚å¸¸æ•°æ®é¡µé¢: analysis_id={analysis_id}, å¼‚å¸¸æ•°æ®æ¡æ•°={len(abnormal_data_detail)}")

    analysis_data_info = {
        "category": analysis_data.get("category", "æš‚æ— ç±»ç›®"),
        "timestamp": analysis_data.get("timestamp", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    }

    return render_template('cost_abnormal_data.html',
                           analysis_id=analysis_id,
                           analysis_data=analysis_data_info,
                           abnormal_data_detail=abnormal_data_detail,
                           abnormal_data_stats=abnormal_data_stats)

# ========================== âœ… æ–°å¢ç¼ºå¤±çš„è·¯ç”±å®šä¹‰ ==========================
@app.route('/download/table/<string:table_type>/<string:analysis_id>')
def download_table(table_type, analysis_id):
    """ä¸‹è½½å•ä¸ªè¡¨æ ¼æ•°æ®"""
    try:
        # åŠ è½½åˆ†æç»“æœ
        analysis_data = load_analysis_result(analysis_id)
        if not analysis_data:
            return jsonify({"error": "åˆ†æç»“æœä¸å­˜åœ¨"}), 404

        # è·å–æ•°æ®
        full_result = analysis_data.get('full_result', {})
        workload_data = full_result.get('workload', {})

        # æ ¹æ®è¡¨æ ¼ç±»å‹è·å–æ•°æ®
        if table_type == 'workload_detail':
            data = workload_data.get('result', [])
            sheet_name = 'å·¥ä½œé‡æ˜ç»†'
        elif table_type == 'workload_group':
            data = workload_data.get('group_summary', [])
            sheet_name = 'å·¥ä½œé‡å°ç»„æ±‡æ€»'
        elif table_type == 'workload_top':
            data = workload_data.get('top_media_ranking', [])
            sheet_name = 'å·¥ä½œé‡TOPæ’å'
        else:
            return jsonify({"error": "ä¸æ”¯æŒçš„è¡¨æ ¼ç±»å‹"}), 400

        if not data:
            return jsonify({"error": "è¡¨æ ¼æ•°æ®ä¸ºç©º"}), 404

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(data)

        # åˆ›å»ºExcelæ–‡ä»¶
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)

        output.seek(0)

        # è¿”å›æ–‡ä»¶
        filename = f"{sheet_name}_{analysis_id}.xlsx"
        return send_file(
            output,
            download_name=filename,
            as_attachment=True,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        logger.error(f"ä¸‹è½½è¡¨æ ¼å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500


# âœ… æ­£ç¡®çš„è·¯ç”±ï¼ˆä¸ JavaScript åŒ¹é…ï¼‰
@app.route('/download_cost_sheet/<string:sheet_key>/<string:analysis_id>')
def download_cost_sheet(sheet_key, analysis_id):
    """
    ä¸‹è½½æˆæœ¬åˆ†æå•ä¸ªå·¥ä½œè¡¨
    """
    logger.info(f"ä¸‹è½½å·¥ä½œè¡¨: sheet_key={sheet_key}, analysis_id={analysis_id}")

    try:
        # å¦‚æœæ˜¯latestï¼Œè·å–æœ€æ–°çš„analysis_id
        if analysis_id == 'latest':
            # è·å–æœ€æ–°åˆ†æç»“æœ
            if not analysis_results:
                return "æ²¡æœ‰å¯ç”¨çš„åˆ†ææ•°æ®ï¼Œè¯·å…ˆè¿›è¡Œåˆ†æ", 404

            latest_id = sorted(analysis_results.keys())[-1]
            analysis_id = latest_id

        # åŠ è½½åˆ†æç»“æœ
        analysis_data = load_analysis_result(analysis_id)
        if not analysis_data:
            return "åˆ†æç»“æœä¸å­˜åœ¨", 404

        # ä»åˆ†æç»“æœä¸­è·å–æ•°æ®
        full_result = analysis_data.get('full_result', {})
        cost_data = full_result.get('cost', {})

        # å·¥ä½œè¡¨æ˜ å°„
        sheet_mapping = {
            'media_group_workload': 'åª’ä»‹å°ç»„å·¥ä½œé‡åˆ†æ',
            'fixed_media_workload': 'å®šæ¡£åª’ä»‹å·¥ä½œé‡åˆ†æ',
            'fixed_media_cost': 'å®šæ¡£åª’ä»‹æˆæœ¬åˆ†æ',
            'fixed_media_rebate': 'å®šæ¡£åª’ä»‹è¿”ç‚¹åˆ†æ',
            'fixed_media_performance': 'å®šæ¡£åª’ä»‹æ•ˆæœåˆ†æ',
            'fixed_media_level': 'å®šæ¡£åª’ä»‹è¾¾äººé‡çº§åˆ†æ',
            'fixed_media_comprehensive': 'å®šæ¡£åª’ä»‹ç»¼åˆåˆ†æ'
        }

        # æ ¹æ® sheet_key è·å–æ•°æ®
        data = None
        sheet_name = sheet_mapping.get(sheet_key, sheet_key)

        if sheet_key == 'media_group_workload':
            data = cost_data.get("media_group_workload", [])
        elif sheet_key == 'fixed_media_workload':
            data = cost_data.get("fixed_media_workload", [])
        elif sheet_key == 'fixed_media_cost':
            data = cost_data.get("fixed_media_cost", [])
        elif sheet_key == 'fixed_media_rebate':
            data = cost_data.get("fixed_media_rebate", [])
        elif sheet_key == 'fixed_media_performance':
            data = cost_data.get("fixed_media_performance", [])
        elif sheet_key == 'fixed_media_level':
            data = cost_data.get("fixed_media_level", [])
        elif sheet_key == 'fixed_media_comprehensive':
            data = cost_data.get("fixed_media_comprehensive", [])
        else:
            return f"ä¸æ”¯æŒçš„å·¥ä½œè¡¨ç±»å‹: {sheet_key}", 400

        if not data:
            return f"å·¥ä½œè¡¨æ•°æ®ä¸ºç©º: {sheet_key}", 404

        # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(data)

        # åˆ›å»º Excel æ–‡ä»¶
        output = BytesIO()

        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)

        output.seek(0)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d')
        filename = f"{sheet_name}_{analysis_id}_{timestamp}.xlsx"

        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=filename
        )

    except Exception as e:
        logger.error(f"ä¸‹è½½å¤±è´¥: {e}", exc_info=True)
        return f"ä¸‹è½½å¤±è´¥: {str(e)}", 500

# ========== ä¸‹è½½ExcelæŠ¥å‘Šè·¯ç”± ==========
@app.route('/download/excel/<analysis_id>')
def download_excel_report(analysis_id):
    """æ ¹æ®åˆ†æIDç²¾å‡†ä¸‹è½½ExcelæŠ¥å‘Š"""
    excel_dir = os.path.join(app.config['OUTPUT_DIR'], 'excel')

    # æŸ¥æ‰¾åŒ¹é…çš„Excelæ–‡ä»¶
    excel_filename = None
    for file in os.listdir(excel_dir):
        if file.endswith('.xlsx') and analysis_id in file:
            excel_filename = file
            break

    if not excel_filename:
        # å°è¯•æŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
        excel_files = [f for f in os.listdir(excel_dir) if f.endswith('.xlsx')]
        if excel_files:
            excel_files.sort(reverse=True)
            excel_filename = excel_files[0]

    if not excel_filename:
        flash('âŒ æœªæ‰¾åˆ°è¯¥åˆ†æIDå¯¹åº”çš„ExcelæŠ¥å‘Š', 'error')
        return redirect(url_for('dashboard', analysis_id=analysis_id))

    try:
        return send_from_directory(
            excel_dir,
            excel_filename,
            as_attachment=True,
            download_name=excel_filename
        )
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{str(e)}")
        flash(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{str(e)}", 'error')
        return redirect(url_for('dashboard', analysis_id=analysis_id))


@app.route('/download/<analysis_id>/<report_type>')
@app.route('/download/<analysis_id>/<report_type>')
def download_report(analysis_id, report_type):
    """ä¸‹è½½æŠ¥å‘Š - æ ¹æ®æŠ¥å‘Šç±»å‹å¯¼å‡ºå®Œæ•´æ•°æ®"""
    try:
        # åŠ è½½åˆ†æç»“æœ
        analysis_data = load_analysis_result(analysis_id)
        if not analysis_data:
            flash('âŒ åˆ†æç»“æœä¸å­˜åœ¨', 'error')
            return redirect(url_for('dashboard', analysis_id=analysis_id))

        # è·å–å®Œæ•´æ•°æ®
        full_result = analysis_data.get('full_result', {})

        # åˆ›å»ºExcelæ–‡ä»¶
        output = BytesIO()

        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            if report_type == 'workload':
                # å·¥ä½œé‡åˆ†æçš„æ‰€æœ‰è¡¨æ ¼
                workload_data = full_result.get('workload', {})

                # 1. å·¥ä½œé‡æ˜ç»†
                if workload_data.get('result'):
                    df_detail = pd.DataFrame(workload_data['result'])
                    df_detail.to_excel(writer, sheet_name='å·¥ä½œé‡æ˜ç»†', index=False)

                # 2. å°ç»„æ±‡æ€»
                if workload_data.get('group_summary'):
                    df_group = pd.DataFrame(workload_data['group_summary'])
                    df_group.to_excel(writer, sheet_name='å·¥ä½œé‡å°ç»„æ±‡æ€»', index=False)

                # 3. TOPæ’å
                if workload_data.get('top_media_ranking'):
                    df_top = pd.DataFrame(workload_data['top_media_ranking'])
                    df_top.to_excel(writer, sheet_name='å·¥ä½œé‡TOPæ’å', index=False)

                # 4. å·¥ä½œé‡æ±‡æ€»ç»Ÿè®¡
                if workload_data.get('summary'):
                    df_summary = pd.DataFrame([workload_data['summary']])
                    df_summary.to_excel(writer, sheet_name='å·¥ä½œé‡æ±‡æ€»', index=False)

            elif report_type == 'quality':
                # è´¨é‡åˆ†æçš„æ‰€æœ‰è¡¨æ ¼
                quality_data = full_result.get('quality', {})

                # 1. è´¨é‡æ˜ç»†
                if quality_data.get('result'):
                    df_detail = pd.DataFrame(quality_data['result'])
                    df_detail.to_excel(writer, sheet_name='è´¨é‡æ˜ç»†', index=False)

                # 2. å°ç»„æ±‡æ€»
                if quality_data.get('group_summary'):
                    df_group = pd.DataFrame(quality_data['group_summary'])
                    df_group.to_excel(writer, sheet_name='è´¨é‡å°ç»„æ±‡æ€»', index=False)

                # 3. è´¨é‡åˆ†å¸ƒ
                if quality_data.get('quality_distribution'):
                    df_dist = pd.DataFrame(quality_data['quality_distribution'])
                    df_dist.to_excel(writer, sheet_name='è´¨é‡åˆ†å¸ƒ', index=False)

                # 4. ä¼˜è´¨è¾¾äººæ˜ç»†
                if quality_data.get('premium_detail'):
                    df_premium = pd.DataFrame(quality_data['premium_detail'])
                    df_premium.to_excel(writer, sheet_name='ä¼˜è´¨è¾¾äººè´¨é‡æ˜ç»†', index=False)

                # 5. é«˜é˜…è¯»è¾¾äººæ˜ç»†
                if quality_data.get('high_read_detail'):
                    df_high_read = pd.DataFrame(quality_data['high_read_detail'])
                    df_high_read.to_excel(writer, sheet_name='é«˜é˜…è¯»è¾¾äººè´¨é‡æ˜ç»†', index=False)

            elif report_type == 'cost':
                # æˆæœ¬åˆ†æçš„æ‰€æœ‰è¡¨æ ¼
                cost_data = full_result.get('cost', {})

                # æ‰€æœ‰å·¥ä½œè¡¨
                sheets = [
                    ('media_group_workload', 'åª’ä»‹å°ç»„å·¥ä½œé‡åˆ†æ'),
                    ('fixed_media_workload', 'å®šæ¡£åª’ä»‹å·¥ä½œé‡åˆ†æ'),
                    ('fixed_media_cost', 'å®šæ¡£åª’ä»‹æˆæœ¬åˆ†æ'),
                    ('fixed_media_rebate', 'å®šæ¡£åª’ä»‹è¿”ç‚¹åˆ†æ'),
                    ('fixed_media_performance', 'å®šæ¡£åª’ä»‹æ•ˆæœåˆ†æ'),
                    ('fixed_media_level', 'å®šæ¡£åª’ä»‹è¾¾äººé‡çº§åˆ†æ'),
                    ('fixed_media_comprehensive', 'å®šæ¡£åª’ä»‹ç»¼åˆåˆ†æ'),
                    ('media_detail', 'è¯¦ç»†æ•°æ®'),
                    ('group_summary', 'å°ç»„æ±‡æ€»'),
                    ('cost_efficiency_ranking', 'æˆæœ¬æ•ˆç‡æ’å')
                ]

                for sheet_key, sheet_name in sheets:
                    if cost_data.get(sheet_key):
                        df_sheet = pd.DataFrame(cost_data[sheet_key])
                        df_sheet.to_excel(writer, sheet_name=sheet_name, index=False)

                        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªsheetä¸æ˜¯éšè—çš„
                        workbook = writer.book
                        if workbook:
                            sheet = workbook[sheet_name]
                            sheet.sheet_state = 'visible'

            else:
                # å®Œæ•´æŠ¥å‘Šï¼ˆæ‰€æœ‰åˆ†æï¼‰- ä¸è°ƒç”¨report_generatorï¼Œç›´æ¥åˆ›å»º
                workbook_data = full_result

                # ç¡®ä¿å†™å…¥è‡³å°‘ä¸€ä¸ªå¯è§çš„å·¥ä½œè¡¨
                write_at_least_one = False

                # å·¥ä½œé‡æ•°æ®
                if 'workload' in workbook_data and workbook_data['workload'].get('result'):
                    df_workload = pd.DataFrame(workload_data['workload']['result'])
                    df_workload.to_excel(writer, sheet_name='å·¥ä½œé‡åˆ†æ', index=False)
                    write_at_least_one = True

                # è´¨é‡æ•°æ®
                if 'quality' in workbook_data and workbook_data['quality'].get('result'):
                    df_quality = pd.DataFrame(workload_data['quality']['result'])
                    df_quality.to_excel(writer, sheet_name='è´¨é‡åˆ†æ', index=False)
                    write_at_least_one = True

                # æˆæœ¬æ•°æ®
                if 'cost' in workbook_data and workbook_data['cost'].get('media_detail'):
                    df_cost = pd.DataFrame(workload_data['cost']['media_detail'])
                    df_cost.to_excel(writer, sheet_name='æˆæœ¬åˆ†æ', index=False)
                    write_at_least_one = True

                if not write_at_least_one:
                    # å¦‚æœæ²¡æœ‰ä»»ä½•æ•°æ®ï¼Œè‡³å°‘åˆ›å»ºä¸€ä¸ªç©ºçš„å·¥ä½œè¡¨
                    pd.DataFrame({'æç¤º': ['æ— åˆ†ææ•°æ®']}).to_excel(writer, sheet_name='æŠ¥å‘Šæ±‡æ€»', index=False)

        output.seek(0)

        # è®¾ç½®æ–‡ä»¶å
        filename = f"åª’ä»‹åˆ†ææŠ¥å‘Š_{report_type}_{analysis_id}.xlsx"

        return send_file(
            output,
            download_name=filename,
            as_attachment=True,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        logger.error(f"å¯¼å‡ºæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
        flash(f'âŒ å¯¼å‡ºæŠ¥å‘Šå¤±è´¥: {str(e)}', 'error')
        return redirect(url_for('dashboard', analysis_id=analysis_id))


# åœ¨ download_report è·¯ç”±åæ·»åŠ ï¼š
@app.route('/export/invalid_data/<analysis_id>')
def export_invalid_data(analysis_id):
    """å¯¼å‡ºæ— æ•ˆæ•°æ®ä¸ºExcel"""
    try:
        # åŠ è½½åˆ†æç»“æœ
        analysis_data = load_analysis_result(analysis_id)
        if not analysis_data:
            flash('âŒ åˆ†æç»“æœä¸å­˜åœ¨', 'error')
            return redirect(url_for('cost_report', analysis_id=analysis_id))

        # è·å–æ— æ•ˆæ•°æ®è¯¦æƒ…
        full_result = analysis_data.get('full_result', {})
        cost_data = full_result.get('cost', {})
        invalid_data_detail = cost_data.get('invalid_data_detail', [])
        invalid_data_stats = cost_data.get('invalid_data_stats', {})

        if not invalid_data_detail:
            flash('âš ï¸ æ— æ— æ•ˆæ•°æ®å¯å¯¼å‡º', 'info')
            return redirect(url_for('cost_report', analysis_id=analysis_id))

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(invalid_data_detail)

        # åˆ›å»ºExcelæ–‡ä»¶
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # å†™å…¥æ— æ•ˆæ•°æ®æ˜ç»†
            df.to_excel(writer, sheet_name='æ— æ•ˆæ•°æ®æ˜ç»†', index=False)

            # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
            stats_data = []
            if invalid_data_stats:
                stats_data = [
                    ['æ€»æ•°æ®æ¡æ•°', invalid_data_stats.get('æ€»æ•°æ®æ¡æ•°', 0)],
                    ['æœ‰æ•ˆæ•°æ®æ¡æ•°', invalid_data_stats.get('æœ‰æ•ˆæ•°æ®æ¡æ•°', 0)],
                    ['æ— æ•ˆæ•°æ®æ¡æ•°', invalid_data_stats.get('æ— æ•ˆæ•°æ®æ¡æ•°', 0)],
                    ['æœ‰æ•ˆæ•°æ®æ¯”ä¾‹', invalid_data_stats.get('æœ‰æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%')],
                    ['æ— æ•ˆæ•°æ®æ¯”ä¾‹', invalid_data_stats.get('æ— æ•ˆæ•°æ®æ¯”ä¾‹(%)', '0%')],
                    ['æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)', invalid_data_stats.get('æ— æ•ˆæ•°æ®æ€»æˆæœ¬(å…ƒ)', 0)]
                ]

            # å†™å…¥æ— æ•ˆåŸå› åˆ†å¸ƒ
            reason_dist = invalid_data_stats.get('æ— æ•ˆæ•°æ®åŸå› åˆ†å¸ƒ', {})
            if reason_dist:
                stats_data.append(['', ''])
                stats_data.append(['æ— æ•ˆåŸå› åˆ†å¸ƒ', 'æ•°é‡'])
                for reason, count in reason_dist.items():
                    stats_data.append([reason, count])

            if stats_data:
                stats_df = pd.DataFrame(stats_data, columns=['é¡¹ç›®', 'æ•°å€¼'])
                stats_df.to_excel(writer, sheet_name='æ•°æ®ç»Ÿè®¡', index=False)

        output.seek(0)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"æ— æ•ˆæ•°æ®æ˜ç»†_{analysis_id}_{timestamp}.xlsx"

        # è¿”å›æ–‡ä»¶
        return send_file(
            output,
            download_name=filename,
            as_attachment=True,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºæ— æ•ˆæ•°æ®å¤±è´¥: {e}", exc_info=True)
        flash(f'âŒ å¯¼å‡ºå¤±è´¥: {str(e)}', 'error')
        return redirect(url_for('cost_report', analysis_id=analysis_id))


@app.route('/export/abnormal_data/<analysis_id>')
@login_required
def export_abnormal_data(analysis_id):
    """å¯¼å‡ºå¼‚å¸¸æ•°æ®ä¸ºExcel - å®Œæ•´ä¿®å¤ç‰ˆæœ¬"""
    try:
        # åŠ è½½åˆ†æç»“æœ
        analysis_data = load_analysis_result(analysis_id)
        if not analysis_data:
            flash('âŒ åˆ†æç»“æœä¸å­˜åœ¨', 'error')
            return redirect(url_for('cost_report', analysis_id=analysis_id))

        # è·å–å¼‚å¸¸æ•°æ®è¯¦æƒ…
        full_result = analysis_data.get('full_result', {})
        cost_data = full_result.get('cost', {})
        abnormal_data_detail = cost_data.get('abnormal_data_detail', [])

        # âœ… å…³é”®ä¿®å¤ï¼šå¦‚æœ abnormal_data_detail ä¸ºç©ºï¼Œå°è¯•ä» detailed_data ä¸­ç­›é€‰
        if not abnormal_data_detail:
            detailed_data = cost_data.get("detailed_data", [])
            if detailed_data and isinstance(detailed_data, list) and len(detailed_data) > 0:
                logger.info(f"å¯¼å‡ºæ—¶ä» detailed_data ä¸­ç­›é€‰å¼‚å¸¸æ•°æ®ï¼Œæ€»æ•°æ®: {len(detailed_data)}")

                abnormal_data_detail = []
                for item in detailed_data:
                    if isinstance(item, dict):
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚å¸¸æ•°æ®
                        data_abnormal = item.get('æ•°æ®å¼‚å¸¸', False)
                        cost_invalid = item.get('æˆæœ¬æ— æ•ˆ', False)

                        # å¦‚æœæ˜¯å¼‚å¸¸æ•°æ®ï¼ˆå‚ä¸åˆ†æä½†æ ‡è®°å¼‚å¸¸ï¼‰
                        if data_abnormal and not cost_invalid:
                            # æ„å»ºå¼‚å¸¸æ•°æ®è¯¦æƒ…æ ¼å¼
                            detail = {
                                'è®°å½•åºå·': item.get('è®°å½•åºå·', 0),
                                'è¾¾äººæ˜µç§°': item.get('è¾¾äººæ˜µç§°', 'æœªçŸ¥'),
                                'é¡¹ç›®åç§°': item.get('é¡¹ç›®åç§°', 'æœªçŸ¥'),
                                'å®šæ¡£åª’ä»‹': item.get('å®šæ¡£åª’ä»‹', 'æœªçŸ¥'),
                                'æˆæœ¬': item.get('æˆæœ¬', 0),
                                'æŠ¥ä»·': item.get('æŠ¥ä»·', 0),
                                'ä¸‹å•ä»·': item.get('ä¸‹å•ä»·', 0),
                                'è¿”ç‚¹': item.get('è¿”ç‚¹', 0),
                                'è¿”ç‚¹æ¯”ä¾‹': item.get('è¿”ç‚¹æ¯”ä¾‹', 0) * 100 if item.get('è¿”ç‚¹æ¯”ä¾‹') else 0,
                                'ä¸å«æ‰‹ç»­è´¹çš„ä¸‹å•ä»·': item.get('ä¸å«æ‰‹ç»­è´¹çš„ä¸‹å•ä»·', ''),
                                'æ•°æ®å¼‚å¸¸åŸå› ': item.get('æ•°æ®å¼‚å¸¸åŸå› ', 'æœªçŸ¥å¼‚å¸¸'),
                                'å¼‚å¸¸ç±»å‹': 'æ•°æ®å¼‚å¸¸',
                                'æ˜¯å¦å‚ä¸åˆ†æ': True
                            }

                            # åˆ¤æ–­å¼‚å¸¸ç±»å‹
                            reason = detail['æ•°æ®å¼‚å¸¸åŸå› ']
                            if 'æŠ¥ä»·<' in reason:
                                detail['å¼‚å¸¸ç±»å‹'] = 'æŠ¥ä»·å¼‚å¸¸'
                            elif 'æ— æ³•åˆ¤æ–­' in reason:
                                detail['å¼‚å¸¸ç±»å‹'] = 'æ•°æ®å¼‚å¸¸'
                            elif 'è¿”ç‚¹æ¯”ä¾‹' in reason:
                                detail['å¼‚å¸¸ç±»å‹'] = 'è¿”ç‚¹å¼‚å¸¸'
                            elif 'ç­›é™¤' in reason or reason in ['æ•°æ®å¼‚å¸¸', 'æˆæœ¬ä¸º0', 'æˆæœ¬ç¼ºå¤±', 'æ•°æ®ä¸å…¨']:
                                detail['å¼‚å¸¸ç±»å‹'] = 'ç­›é™¤å¼‚å¸¸'

                            abnormal_data_detail.append(detail)

                logger.info(f"å¯¼å‡ºæ—¶ç­›é€‰åˆ°å¼‚å¸¸æ•°æ®: {len(abnormal_data_detail)} æ¡")

        abnormal_data_stats = cost_data.get('abnormal_data_stats', {})

        if not abnormal_data_detail:
            flash('âš ï¸ æ— å¼‚å¸¸æ•°æ®å¯å¯¼å‡º', 'info')
            return redirect(url_for('cost_abnormal_data_report', analysis_id=analysis_id))

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(abnormal_data_detail)

        # æ·»åŠ å¿…è¦çš„ä¸­æ–‡åˆ—å
        column_mapping = {
            'è®°å½•åºå·': 'åºå·',
            'è¾¾äººæ˜µç§°': 'è¾¾äººæ˜µç§°',
            'é¡¹ç›®åç§°': 'é¡¹ç›®åç§°',
            'å®šæ¡£åª’ä»‹': 'å®šæ¡£åª’ä»‹',
            'æˆæœ¬': 'æˆæœ¬(å…ƒ)',
            'æŠ¥ä»·': 'æŠ¥ä»·(å…ƒ)',
            'ä¸‹å•ä»·': 'ä¸‹å•ä»·(å…ƒ)',
            'è¿”ç‚¹': 'è¿”ç‚¹(å…ƒ)',
            'è¿”ç‚¹æ¯”ä¾‹': 'è¿”ç‚¹æ¯”ä¾‹(%)',
            'ä¸å«æ‰‹ç»­è´¹çš„ä¸‹å•ä»·': 'ä¸å«æ‰‹ç»­è´¹ä¸‹å•ä»·',
            'æ•°æ®å¼‚å¸¸åŸå› ': 'å¼‚å¸¸åŸå› ',
            'å¼‚å¸¸ç±»å‹': 'å¼‚å¸¸ç±»å‹',
            'æ˜¯å¦å‚ä¸åˆ†æ': 'æ˜¯å¦å‚ä¸åˆ†æ'
        }

        # é‡å‘½ååˆ—
        df = df.rename(columns=column_mapping)

        # åˆ›å»ºExcelæ–‡ä»¶
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # å†™å…¥å¼‚å¸¸æ•°æ®æ˜ç»†
            df.to_excel(writer, sheet_name='å¼‚å¸¸æ•°æ®æ˜ç»†', index=False)

            # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
            stats_data = []
            if abnormal_data_stats:
                stats_data = [
                    ['å¼‚å¸¸æ•°æ®æ¡æ•°', abnormal_data_stats.get('å¼‚å¸¸æ•°æ®æ¡æ•°', len(abnormal_data_detail))],
                    ['å¼‚å¸¸æ•°æ®æ¯”ä¾‹', abnormal_data_stats.get('å¼‚å¸¸æ•°æ®æ¯”ä¾‹(%)',
                                                             f"{(len(abnormal_data_detail) / (abnormal_data_stats.get('æ€»æ•°æ®æ¡æ•°', 1)) * 100):.2f}%" if abnormal_data_stats.get(
                                                                 'æ€»æ•°æ®æ¡æ•°', 0) > 0 else '0%')],
                    ['å‚ä¸åˆ†ææ•°æ®æ¡æ•°', abnormal_data_stats.get('å‚ä¸åˆ†ææ•°æ®æ¡æ•°', 0)],
                    ['å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹', abnormal_data_stats.get('å‚ä¸åˆ†ææ•°æ®æ¯”ä¾‹(%)', '100%')],
                    ['å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)', abnormal_data_stats.get('å¼‚å¸¸æ•°æ®æ€»æˆæœ¬(å…ƒ)', 0)]
                ]

            # å†™å…¥å¼‚å¸¸åŸå› åˆ†å¸ƒ
            reason_dist = abnormal_data_stats.get('å¼‚å¸¸æ•°æ®åŸå› åˆ†å¸ƒ', {})
            if reason_dist:
                stats_data.append(['', ''])
                stats_data.append(['å¼‚å¸¸åŸå› åˆ†å¸ƒ', 'æ•°é‡'])
                for reason, count in reason_dist.items():
                    stats_data.append([reason, count])

            if stats_data:
                stats_df = pd.DataFrame(stats_data, columns=['é¡¹ç›®', 'æ•°å€¼'])
                stats_df.to_excel(writer, sheet_name='æ•°æ®ç»Ÿè®¡', index=False)

        output.seek(0)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"å¼‚å¸¸æ•°æ®æ˜ç»†_{analysis_id}_{timestamp}.xlsx"

        # è¿”å›æ–‡ä»¶
        return send_file(
            output,
            download_name=filename,
            as_attachment=True,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºå¼‚å¸¸æ•°æ®å¤±è´¥: {e}", exc_info=True)
        flash(f'âŒ å¯¼å‡ºå¤±è´¥: {str(e)}', 'error')
        return redirect(url_for('cost_abnormal_data_report', analysis_id=analysis_id))

@app.route('/debug/user')
@login_required
def debug_user():
    """è°ƒè¯•ç”¨æˆ·ä¿¡æ¯"""
    user = get_current_user()
    if user:
        return jsonify({
            'user_id': session.get('user_id'),
            'username': session.get('username'),
            'role': session.get('role'),
            'is_admin': user.is_admin() if user else False,
            'is_active': user.is_active() if user else False
        })
    else:
        return jsonify({'error': 'ç”¨æˆ·æœªç™»å½•'})

# ------------------------------ è¾…åŠ©è·¯ç”± ------------------------------
@app.route('/analyze', methods=['POST'])
def analyze():
    """å…¼å®¹æ—§æäº¤é€»è¾‘çš„é‡å®šå‘"""
    return redirect(url_for('index'), code=307)

@app.route('/clear_results')
def clear_results():
    """æ¸…é™¤æ‰€æœ‰å†…å­˜ä¸­çš„åˆ†æç»“æœ"""
    global analysis_results
    analysis_results.clear()
    flash('âœ… æ‰€æœ‰å†…å­˜ä¸­çš„åˆ†æç»“æœå·²æ¸…é™¤', 'success')
    return redirect(url_for('index'))

# ------------------------------ é”™è¯¯å¤„ç† ------------------------------
@app.errorhandler(404)
def page_not_found(e):
    """404é¡µé¢å¤„ç†"""
    return render_template('404.html'), 404

@app.errorhandler(500)
def internal_server_error(e):
    """500æœåŠ¡å™¨é”™è¯¯å¤„ç†ï¼Œè®°å½•å®Œæ•´å †æ ˆä¿¡æ¯"""
    error_msg = f"âŒ æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼š{str(e)}"
    logger.error(f"{error_msg}\n{traceback.format_exc()}")
    return render_template('500.html', error_message=error_msg), 500

@app.route('/favicon.ico')
def favicon():
    """å±è”½favicon.icoè¯·æ±‚é”™è¯¯"""
    return '', 204

# ------------------------------ åº”ç”¨å…¥å£ ------------------------------
if __name__ == '__main__':
    logger.info("="*50)
    logger.info("ğŸš€ åª’ä»‹è‡ªåŠ¨åŒ–å®¡è®¡åˆ†æç³»ç»Ÿ - çœŸå®æ•°æ®æ¨¡å¼ å¯åŠ¨æˆåŠŸ")
    logger.info(f"ğŸŒ æœåŠ¡è®¿é—®åœ°å€ï¼šhttp://0.0.0.0:5000")
    logger.info(f"ğŸ“‚ ä¸Šä¼ ç›®å½•ï¼š{app.config['UPLOAD_FOLDER']}")
    logger.info(f"ğŸ“¤ è¾“å‡ºç›®å½•ï¼š{app.config['OUTPUT_DIR']}")
    logger.info("="*50)
    # ä¼˜åŒ–å¯åŠ¨å‚æ•°ï¼Œé¿å…å¤šè¿›ç¨‹å†²çª+æ”¯æŒå¹¶å‘
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=app.config['DEBUG'],
        threaded=True,
        use_reloader=False
    )
